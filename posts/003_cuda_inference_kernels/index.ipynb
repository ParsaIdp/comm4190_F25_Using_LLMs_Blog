{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"CUDA Inference Kernels and Serving Stacks\"\n",
        "description: \"Key kernels, memory layouts, and serving choices (with vLLM notes)\"\n",
        "author: \"Parsa Idehpour\"\n",
        "date: \"2025-09-14\"\n",
        "categories:\n",
        "  - systems\n",
        "  - inference\n",
        "  - cuda\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "What makes LLM inference fast on GPUs? This post summarizes the core CUDA kernels, memory layouts, and scheduling tricks that dominate latency and throughput.\n",
        "\n",
        "- Attention with KV cache (paged KV, continuous batch, block-sparse)\n",
        "- GEMM-heavy layers (QKV projections, MLP) and tensor cores\n",
        "- Quantization (weight-only, KV cache) and dequant overhead\n",
        "- Serving stacks (vLLM, TensorRT-LLM, Triton) and scheduling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "```{=html}\n",
        "<div style=\"text-align:center;\">\n",
        "  <img src=\"vllm-logo-text-light.png\" alt=\"vLLM\" width=\"35%\"/>\n",
        "  <p><em>Figure 1. vLLM logo </em></p>\n",
        "</div>\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel hotspots\n",
        "\n",
        "- Attention: QK^T, softmax, and AV; fused kernels reduce memory traffic.\n",
        "- KV cache: paging and swizzling to keep contiguous access; block tables.\n",
        "- GEMM: QKV, output projection, MLP GEMMs dominate FLOPs; use tensor cores.\n",
        "- Quant: weight-only (W8A16) vs. activation quant; KV cache quant (e.g., 8-bit) saves VRAM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple throughput estimator\n",
        "from typing import Optional\n",
        "\n",
        "def estimate_throughput(tokens_per_request: int,\n",
        "                        requests_per_second: float,\n",
        "                        decode_tokens_per_s: float,\n",
        "                        prefill_tokens_per_s: Optional[float] = None,\n",
        "                        prefill_tokens: int = 0) -> float:\n",
        "    \"\"\"\n",
        "    Crude throughput estimate in tokens/sec.\n",
        "    - tokens_per_request: average generated tokens per request\n",
        "    - requests_per_second: steady-state request rate\n",
        "    - decode_tokens_per_s: steady-state decode speed (per token) across batch\n",
        "    - prefill_tokens_per_s: optional prefill speed; if None, ignored\n",
        "    - prefill_tokens: avg prompt tokens per request\n",
        "    \"\"\"\n",
        "    decode_toks = tokens_per_request * requests_per_second\n",
        "    total = decode_toks\n",
        "    if prefill_tokens_per_s is not None and prefill_tokens > 0:\n",
        "        total += (prefill_tokens * requests_per_second) * (prefill_tokens_per_s / decode_tokens_per_s)\n",
        "    return total\n",
        "\n",
        "print(estimate_throughput(tokens_per_request=200,\n",
        "                          requests_per_second=5.0,\n",
        "                          decode_tokens_per_s=200000.0,\n",
        "                          prefill_tokens_per_s=1200000.0,\n",
        "                          prefill_tokens=1000))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## vLLM vs TensorRT-LLM vs Triton (high level)\n",
        "\n",
        "- vLLM: Continuous batching, paged KV cache, flexible Python API; great for dynamic workloads; supports multi-model serving.\n",
        "- TensorRT-LLM: Highly optimized CUDA/TensorRT kernels, graph capture, INT8/FP8 pipelines; excels on NVIDIA stacks for max perf.\n",
        "- Triton (NVIDIA inference server): Orchestrates models/runtimes (can host vLLM or TensorRT-LLM backends), handles deployment/HTTP/gRPC/metrics.\n",
        "\n",
        "Rule of thumb: For maximum single-GPU perf and static graphs, TensorRT-LLM can edge out; for dynamic batching/multitenancy ease, vLLM is excellent; Triton adds production serving features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KV cache sizing and paging\n",
        "\n",
        "- KV size per token ≈ 2 * num_layers * num_heads * head_dim * dtype_bytes.\n",
        "- Multiply by max sequence length and batch to estimate worst-case VRAM.\n",
        "- Paged KV: allocate in fixed-size blocks (e.g., 16/32/64 tokens) and map logical positions → physical pages; reduces fragmentation and enables efficient eviction.\n",
        "- Swizzling/contiguous layout within blocks keeps memory coalesced for attention reads.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KV cache memory estimator (bytes)\n",
        "from typing import Literal\n",
        "\n",
        "dtype_sizes = {\n",
        "    \"fp16\": 2,\n",
        "    \"bf16\": 2,\n",
        "    \"fp32\": 4,\n",
        "    \"int8\": 1,\n",
        "}\n",
        "\n",
        "def kv_cache_bytes(num_layers: int,\n",
        "                   num_heads: int,\n",
        "                   head_dim: int,\n",
        "                   seq_len: int,\n",
        "                   batch_size: int,\n",
        "                   dtype: Literal[\"fp16\", \"bf16\", \"fp32\", \"int8\"] = \"fp16\") -> int:\n",
        "    per_token = 2 * num_layers * num_heads * head_dim * dtype_sizes[dtype]\n",
        "    total_tokens = seq_len * batch_size\n",
        "    return per_token * total_tokens\n",
        "\n",
        "# Example: 32 layers, 32 heads, 128 dim, seq 4k, batch 16, fp16\n",
        "print(kv_cache_bytes(32, 32, 128, 4096, 16, \"fp16\") / (1024**3), \"GiB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batching and scheduling\n",
        "\n",
        "- Continuous batching: backfill finished sequences to keep kernels busy; great for variable-length requests.\n",
        "- Short-first / length-aware: schedule shorter jobs to reduce tail latency; mix for throughput.\n",
        "- Spec decode: trade compute for fewer steps; helps throughput if memory is ample.\n",
        "- Pinned memory and overlap: overlap H2D/D2H copies with compute using streams.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel-level optimization details\n",
        "\n",
        "- Fused attention: fuse QK^T, scaling, softmax, and AV to reduce DRAM round-trips.\n",
        "- FlashAttention-style tiling: block SRAM tiles to keep on-chip; minimize global reads/writes.\n",
        "- Layouts: use row/col-major consistent with GEMM kernels; prefer tensor core-friendly shapes (multiple of 8/16).\n",
        "- Persistent kernels: reduce launch overhead; good for steady-state decode.\n",
        "- Quant & dequant fusion: fuse dequant→GEMM to avoid bandwidth blowups.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## System-level tips\n",
        "\n",
        "- CUDA graphs: capture steady decode loop to reduce CPU overhead.\n",
        "- Streams: separate prefill, decode, and IO with stream priorities.\n",
        "- NUMA & pinning: pin host buffers and align worker affinity to avoid cross-socket traffic.\n",
        "- Overlap: prefetch next batch to GPU while decoding current tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "- vLLM docs and papers (continuous batching, paged KV)\n",
        "- TensorRT-LLM samples and perf guides\n",
        "- FlashAttention papers and kernels\n",
        "- NVIDIA Triton Inference Server docs\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
