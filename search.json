[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html",
    "href": "posts/003_cuda_inference_kernels/index.html",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "",
    "text": "What makes LLM inference fast on GPUs? This post summarizes the core CUDA kernels, memory layouts, and scheduling tricks that dominate latency and throughput.\n\nAttention with KV cache (paged KV, continuous batch, block-sparse)\nGEMM-heavy layers (QKV projections, MLP) and tensor cores\nQuantization (weight-only, KV cache) and dequant overhead\nServing stacks (vLLM, TensorRT-LLM, Triton) and scheduling\n\n\n  \n  Figure 1. vLLM logo"
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#overview",
    "href": "posts/003_cuda_inference_kernels/index.html#overview",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "",
    "text": "What makes LLM inference fast on GPUs? This post summarizes the core CUDA kernels, memory layouts, and scheduling tricks that dominate latency and throughput.\n\nAttention with KV cache (paged KV, continuous batch, block-sparse)\nGEMM-heavy layers (QKV projections, MLP) and tensor cores\nQuantization (weight-only, KV cache) and dequant overhead\nServing stacks (vLLM, TensorRT-LLM, Triton) and scheduling\n\n\n  \n  Figure 1. vLLM logo"
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#kernel-hotspots",
    "href": "posts/003_cuda_inference_kernels/index.html#kernel-hotspots",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "Kernel hotspots",
    "text": "Kernel hotspots\n\nAttention: QK^T, softmax, and AV; fused kernels reduce memory traffic.\nKV cache: paging and swizzling to keep contiguous access; block tables.\nGEMM: QKV, output projection, MLP GEMMs dominate FLOPs; use tensor cores.\nQuant: weight-only (W8A16) vs.¬†activation quant; KV cache quant (e.g., 8-bit) saves VRAM.\n\n\n# Simple throughput estimator\nfrom typing import Optional\n\ndef estimate_throughput(tokens_per_request: int,\n                        requests_per_second: float,\n                        decode_tokens_per_s: float,\n                        prefill_tokens_per_s: Optional[float] = None,\n                        prefill_tokens: int = 0) -&gt; float:\n    \"\"\"\n    Crude throughput estimate in tokens/sec.\n    - tokens_per_request: average generated tokens per request\n    - requests_per_second: steady-state request rate\n    - decode_tokens_per_s: steady-state decode speed (per token) across batch\n    - prefill_tokens_per_s: optional prefill speed; if None, ignored\n    - prefill_tokens: avg prompt tokens per request\n    \"\"\"\n    decode_toks = tokens_per_request * requests_per_second\n    total = decode_toks\n    if prefill_tokens_per_s is not None and prefill_tokens &gt; 0:\n        total += (prefill_tokens * requests_per_second) * (prefill_tokens_per_s / decode_tokens_per_s)\n    return total\n\nprint(estimate_throughput(tokens_per_request=200,\n                          requests_per_second=5.0,\n                          decode_tokens_per_s=200000.0,\n                          prefill_tokens_per_s=1200000.0,\n                          prefill_tokens=1000))"
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#vllm-vs-tensorrt-llm-vs-triton-high-level",
    "href": "posts/003_cuda_inference_kernels/index.html#vllm-vs-tensorrt-llm-vs-triton-high-level",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "vLLM vs TensorRT-LLM vs Triton (high level)",
    "text": "vLLM vs TensorRT-LLM vs Triton (high level)\n\nvLLM: Continuous batching, paged KV cache, flexible Python API; great for dynamic workloads; supports multi-model serving.\nTensorRT-LLM: Highly optimized CUDA/TensorRT kernels, graph capture, INT8/FP8 pipelines; excels on NVIDIA stacks for max perf.\nTriton (NVIDIA inference server): Orchestrates models/runtimes (can host vLLM or TensorRT-LLM backends), handles deployment/HTTP/gRPC/metrics.\n\nRule of thumb: For maximum single-GPU perf and static graphs, TensorRT-LLM can edge out; for dynamic batching/multitenancy ease, vLLM is excellent; Triton adds production serving features."
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#kv-cache-sizing-and-paging",
    "href": "posts/003_cuda_inference_kernels/index.html#kv-cache-sizing-and-paging",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "KV cache sizing and paging",
    "text": "KV cache sizing and paging\n\nKV size per token ‚âà 2 * num_layers * num_heads * head_dim * dtype_bytes.\nMultiply by max sequence length and batch to estimate worst-case VRAM.\nPaged KV: allocate in fixed-size blocks (e.g., 16/32/64 tokens) and map logical positions ‚Üí physical pages; reduces fragmentation and enables efficient eviction.\nSwizzling/contiguous layout within blocks keeps memory coalesced for attention reads.\n\n\n# KV cache memory estimator (bytes)\nfrom typing import Literal\n\ndtype_sizes = {\n    \"fp16\": 2,\n    \"bf16\": 2,\n    \"fp32\": 4,\n    \"int8\": 1,\n}\n\ndef kv_cache_bytes(num_layers: int,\n                   num_heads: int,\n                   head_dim: int,\n                   seq_len: int,\n                   batch_size: int,\n                   dtype: Literal[\"fp16\", \"bf16\", \"fp32\", \"int8\"] = \"fp16\") -&gt; int:\n    per_token = 2 * num_layers * num_heads * head_dim * dtype_sizes[dtype]\n    total_tokens = seq_len * batch_size\n    return per_token * total_tokens\n\n# Example: 32 layers, 32 heads, 128 dim, seq 4k, batch 16, fp16\nprint(kv_cache_bytes(32, 32, 128, 4096, 16, \"fp16\") / (1024**3), \"GiB\")"
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#batching-and-scheduling",
    "href": "posts/003_cuda_inference_kernels/index.html#batching-and-scheduling",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "Batching and scheduling",
    "text": "Batching and scheduling\n\nContinuous batching: backfill finished sequences to keep kernels busy; great for variable-length requests.\nShort-first / length-aware: schedule shorter jobs to reduce tail latency; mix for throughput.\nSpec decode: trade compute for fewer steps; helps throughput if memory is ample.\nPinned memory and overlap: overlap H2D/D2H copies with compute using streams."
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#kernel-level-optimization-details",
    "href": "posts/003_cuda_inference_kernels/index.html#kernel-level-optimization-details",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "Kernel-level optimization details",
    "text": "Kernel-level optimization details\n\nFused attention: fuse QK^T, scaling, softmax, and AV to reduce DRAM round-trips.\nFlashAttention-style tiling: block SRAM tiles to keep on-chip; minimize global reads/writes.\nLayouts: use row/col-major consistent with GEMM kernels; prefer tensor core-friendly shapes (multiple of 8/16).\nPersistent kernels: reduce launch overhead; good for steady-state decode.\nQuant & dequant fusion: fuse dequant‚ÜíGEMM to avoid bandwidth blowups."
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#system-level-tips",
    "href": "posts/003_cuda_inference_kernels/index.html#system-level-tips",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "System-level tips",
    "text": "System-level tips\n\nCUDA graphs: capture steady decode loop to reduce CPU overhead.\nStreams: separate prefill, decode, and IO with stream priorities.\nNUMA & pinning: pin host buffers and align worker affinity to avoid cross-socket traffic.\nOverlap: prefetch next batch to GPU while decoding current tokens."
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#references",
    "href": "posts/003_cuda_inference_kernels/index.html#references",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "References",
    "text": "References\n\nvLLM docs and papers (continuous batching, paged KV)\nTensorRT-LLM samples and perf guides\nFlashAttention papers and kernels\nNVIDIA Triton Inference Server docs"
  },
  {
    "objectID": "posts/007_breakthrough/index.html",
    "href": "posts/007_breakthrough/index.html",
    "title": "Breakthrough #3 ‚Äî Simulating: Neocortex, World Models, and Learning by Imagining",
    "section": "",
    "text": "Simulating is the chapter where Bennett‚Äôs staircase starts to look like a blueprint for building stronger agents. Reinforcement learning alone can sculpt behavior, but it is expensive: the agent must experience consequences, and mistakes can kill you. Simulation changes the bargain. You can evaluate possible actions internally before you commit to them in the world.\nBennett‚Äôs biological anchor here is neocortex (and, in his telling, the rise of mammals as a lineage that could trade metabolic cost for richer internal computation). The key claim is functional: brains evolve a generative modeling substrate that can run offline, producing coherent ‚Äúwhat might happen‚Äù scenarios that guide choice.\nI find it helpful to distinguish three layers in Bennett‚Äôs simulating story:\n\nGenerative prediction. The brain can generate plausible next states, not just react to current stimuli.\nCounterfactual variation. The brain can generate alternatives: ‚Äúwhat if I did something else?‚Äù\nDecision influence. The brain uses those generated futures to bias present action selection.\n\nIt‚Äôs that third step that turns prediction into planning. Prediction alone can exist without agency. Planning is prediction deployed as a control tool.\nBennett is unusually explicit about the hard part: not modeling, but search. If the brain can imagine many futures, it must decide which ones to spend compute on. It must choose whether to go deep on one scenario or sample many shallow ones. It must decide when to stop thinking and act. This is not a poetic flourish; it‚Äôs the planning bottleneck that shows up in every serious AI planning system.\nThe value of Bennett‚Äôs framing is that it forces you to treat ‚Äúthinking‚Äù as compute allocation. An agent that thinks well is an agent that uses a limited budget of imagination in the places where it matters most. That is why the rise of frontal cortex-like structures is conceptually tied to simulation: you need a controller for the simulator itself.\nHis treatment of episodic memory fits nicely into this. If simulation is a generative process, then memory is not just storage; it is the ability to re-render a specific past. Replay then becomes a way of training the generative model and the policy using experiences that are rare or important. Bennett‚Äôs two-speed story‚Äîfast episodic capture plus slow structural learning‚Äîmaps onto a very common engineering problem: you want a system that can both remember exceptions and still generalize.\nCounterfactuals are the real accelerator. If you can replay an episode and evaluate unchosen alternatives, you squeeze more learning signal out of the same experience. In practical terms, you reduce the number of costly real-world trials required to become competent. This is why simulating feels like a qualitative jump: it‚Äôs not just that the agent is smarter; it‚Äôs that the agent can learn in a safer, cheaper regime.\nSimulation also introduces new failure modes, and Bennett‚Äôs staircase implicitly depends on recognizing them. A model-free learner is slow and rigid. A simulator can be fast and imaginative‚Äîand wrong. It can hallucinate. It can over-plan using inaccurate dynamics. It can get trapped in internally coherent but externally false stories.\nThat matters because it motivates why later breakthroughs focus on grounding: mentalizing introduces constraints from other minds (social correction), and language turns simulation into something that can be checked and aligned across people. In other words, the staircase is not ‚Äúmore compute forever.‚Äù It‚Äôs ‚Äúmore compute plus new ways to keep it pointed at reality.‚Äù\n\n  \n  Figure 1. Vicarious trial-and-error: pausing to evaluate options before acting"
  },
  {
    "objectID": "posts/008_breakthrough/index.html",
    "href": "posts/008_breakthrough/index.html",
    "title": "Breakthrough #4 ‚Äî Mentalizing: Second-Order Models, Theory of Mind, and Primate Intelligence",
    "section": "",
    "text": "Mentalizing is Bennett‚Äôs attempt to compress a lot of primate uniqueness into a single computational upgrade. In his telling, primates don‚Äôt just simulate the external world; they simulate minds. They infer hidden variables‚Äîbeliefs, knowledge, goals, intent‚Äîthat generate observable behavior.\nThat sounds like philosophy until you notice how algorithmic it is. If other agents exist, then the environment is no longer governed only by physics. It‚Äôs governed by policies. The biggest uncertainty in your world becomes ‚Äúwhat will they do?‚Äù And the best predictors of what they will do are not visible objects; they are invisible internal states.\nBennett‚Äôs strongest unification move is to connect theory of mind, imitation, and ‚Äúfuture needs‚Äù under one umbrella.\n\nTheory of mind: predicting others by inferring what they know and what they want.\nImitation: copying competence by inferring the underlying goal, not the surface motor pattern.\nFuture needs: planning for a future self whose drives differ from the present self.\n\nAll three require the same thing: a model in which behavior is explained by latent mental causes. Once you have that model for yourself, it becomes reusable for others. That reuse story is the kind of thing evolution loves: build one abstraction, get multiple capabilities for ‚Äúfree.‚Äù\nImitation is where this gets concrete. Behavioral cloning is brittle: copying the exact moves only works when bodies, constraints, and contexts match. Skilled imitation requires intent inference. You watch someone do a task, you infer what objective they are achieving and what constraints they respect, and then you re-plan under your own dynamics. That is mentalizing as an algorithm.\nThe ‚Äúfuture needs‚Äù angle is similarly concrete once you think about sparse environments. If you only seek water when thirsty, you die in deserts. To carry water now for thirst later, you must represent a future self whose preferences differ from your present self. Bennett is effectively saying that primates can treat their future selves like other agents: they can reason about a self they are not currently experiencing.\nHe also notes the characteristic bias this introduces: we project the present into the future and into other people. If mentalizing is inference with priors, projection is exactly what you should expect when your own state is a strong prior and compute is limited.\nFrom an ML perspective, mentalizing is adjacent to inverse planning and inverse reinforcement learning: infer what objective a policy is optimizing from trajectories. It‚Äôs also adjacent to opponent modeling: you build an internal model of the other agent‚Äôs policy class, then condition on inferred intent.\nThe interesting research question Bennett‚Äôs story raises is: what is the training signal? Inverse problems are ill-posed. How does a brain learn the latent variables that make behavior predictable? Social feedback is one candidate (you get punished when you mispredict people). Self-consistency is another (a self-model that doesn‚Äôt help you control yourself gets selected against). Culture and language likely turbocharge it later, but Bennett wants a primate-level version that predates full language.\nWhether or not you buy every anatomical mapping, the computational claim is strong: the world becomes much more predictable once you model agents as optimizing hidden objectives, and primates look like the lineage that doubled down on that move.\n\n  \n  Figure 1. Predicting behavior by inferring hidden goals, knowledge, and intent"
  },
  {
    "objectID": "posts/009_breakthrough/index.html",
    "href": "posts/009_breakthrough/index.html",
    "title": "Breakthrough #5 ‚Äî Speaking: Language as Curriculum, Joint Attention, and the Human Hive Mind",
    "section": "",
    "text": "Speaking is where Bennett stops treating brains as isolated optimizers and starts treating humans as participants in a collective learning system. Language is not just communication in his framing. It is the mechanism that aligns internal simulations across individuals, and the alignment enables cumulative culture.\nTwo ideas do the heavy lifting: (1) humans evolved a developmental pipeline that makes language learnable, and (2) language acts as a low-bandwidth interface to high-dimensional internal models.\nOn learnability, Bennett emphasizes proto-conversations and joint attention. Proto-conversations are the turn-taking rhythms that appear before full speech: the infant and caregiver already coordinate timing and expectation. Joint attention is the active work of ensuring shared reference. The child doesn‚Äôt just notice an object; the child notices that the adult notices it too. That ‚Äúwe are aligned‚Äù fact is what makes a label unambiguous.\nThis matters because raw scenes are wildly ambiguous. If you hear a word while looking at a room, what does the word refer to? The cup? The color? The action? The relation? Joint attention collapses the hypothesis space by constructing a teaching context. The child‚Äôs environment is not a passive dataset; it is an interactive training regime engineered by caregivers and enforced by the child‚Äôs own drive to be understood.\nAs an ML person, I read this as a biological argument for why data quality is not ‚Äújust more tokens.‚Äù The distribution is actively shaped for learnability. The learner actively shapes what data it receives. That is an underappreciated difference between human language acquisition and most current-scale language modeling.\nOn function, Bennett‚Äôs ‚Äútethering simulations‚Äù metaphor is right. A sentence is not a high-bandwidth dump of an internal movie. It‚Äôs a compressed control signal that causes the listener to reconstruct the relevant movie inside their own generative model. Words behave like handles into shared structure; grammar binds handles into relations.\nOnce this works, culture becomes an external memory system. Skills, tools, norms, and explanations survive beyond any one individual. Bennett calls this a ‚Äúhive mind‚Äù in spirit: not because humans lose individuality, but because the unit of intelligence becomes partially collective. The population becomes a distributed optimizer over ideas, with language as the protocol that moves partial solutions between brains.\nThis also creates a feedback loop that accelerates intelligence: better communication improves coordination; coordination enables more complex tools and institutions; those tools change the developmental environment; and the new environment selects for even better learning and communication. Bennett is telling an evolutionary scaling story: intelligence doesn‚Äôt only scale by adding neurons; it scales by adding transmission and accumulation.\nThe most interesting place to press Bennett here is how language relates to simulation and mentalizing. His scaffold suggests language is not the root capability; it is a coordination layer built on top of reinforcement, simulation, and mind-modeling. That gives you a clean way to compare humans and current AI systems:\n\nLLMs show language can carry immense structure and can induce impressive competence.\nBennett‚Äôs story suggests the strongest generality comes when language is coupled to objectives, feedback, world models, and multi-agent inference.\n\nSo the question for modern AI is not ‚Äúdoes language matter?‚Äù It obviously does. The question is whether language-only training reliably yields the deeper agentic stack, or whether you eventually need the other ingredients directly (embodied objectives, interactive feedback, explicit planning, social inference). Bennett‚Äôs book doesn‚Äôt answer that, but it gives you a coherent set of axes to discuss it without handwaving.\n\n  \n  Figure 1. Language as a low-bandwidth channel that aligns rich internal models across minds"
  },
  {
    "objectID": "posts/0010_breakthrough/index.html",
    "href": "posts/0010_breakthrough/index.html",
    "title": "The Sixth Breakthrough ‚Äî Artificial Superintelligence and the Medium Shift to Silicon",
    "section": "",
    "text": "Bennett ends with a move that is both obvious and surprisingly rare: he treats the ‚Äúsuperintelligence‚Äù question as the continuation of the same staircase. If intelligence evolved through breakthroughs, it‚Äôs natural to ask whether we‚Äôre near the next one. His candidate is a medium shift: intelligence moving from biology to digital systems.\nHe‚Äôs not trying to forecast dates. He‚Äôs trying to identify what becomes qualitatively different when the substrate changes: speed, scaling, copyability, and the dynamics of variation and selection.\nBiological minds are constrained in ways we normalize. Neurons are slow compared to silicon. Brains are expensive and thermally limited. They‚Äôre embedded in bodies that require maintenance, reproduction, and survival. And most importantly: brains are not easily copyable. A human mind cannot fork itself into ten variants and see which one learns faster.\nDigital minds relax these constraints. Copying is cheap. Iteration is fast. Architectures and training regimes can be modified. That changes what ‚Äúevolution‚Äù looks like. Selection pressure can operate over designs and training methods, and improvements can propagate instantly by copying rather than slowly by reproduction.\nBennett‚Äôs point about copyability sharpens the intuition: if an intelligence can fork, explore development paths in parallel, keep what works, and discard what fails, the optimization dynamics accelerate. The system becomes ‚Äúfaster at becoming better,‚Äù not just ‚Äúbetter.‚Äù That is the core discontinuity he wants you to feel.\nHe also makes a pragmatic claim about near-term shape. Even if a digital superintelligence is not brain-like internally, it will likely inherit many human-like structures early on because it is built by humans and deployed among humans. Interface constraints matter. If you want an AI to operate in human institutions, it will need to model human goals, communicate in human languages, and navigate human multi-agent dynamics. That pressures it toward mentalizing-like and language-like capabilities whether or not it uses our exact anatomy.\nThen Bennett lands the plane on the only honest ending: values. Biology provided the objective for most of evolutionary history. In engineered systems, we provide the objective. Optimization power without stable objective specification is not neutral; it produces highly competent pursuit of whatever you accidentally or carelessly set as the target. Bennett‚Äôs staircase therefore ends as a demand for seriousness: if we are building the next step, the central question becomes not only what is possible, but what should be optimized and under what constraints.\nIf you want a crisp way to say this in a professor conversation: as systems get more capable, the hardest problem shifts from ‚Äúcan we make it learn?‚Äù to ‚Äúcan we keep its learned behavior aligned with what we actually want over long horizons and under distribution shift?‚Äù Bennett doesn‚Äôt solve alignment, but he frames why it becomes the dominant question in a world where intelligence is scalable and copyable.\n\n  \n  Figure 1. A medium shift changes constraints: speed, energy, copyability, and iteration"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "The Sixth Breakthrough ‚Äî Artificial Superintelligence and the Medium Shift to Silicon\n\n\n\nAI\n\nsuperintelligence\n\nalignment\n\nphilosophy\n\n\n\nMy AI-focused notes on Bennett‚Äôs epilogue: why the next breakthrough is likely ASI, how copyable minds change evolution, and why values become the core question.\n\n\n\n\n\nDec 16, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nBreakthrough #5 ‚Äî Speaking: Language as Curriculum, Joint Attention, and the Human Hive Mind\n\n\n\nneuroscience\n\nlanguage\n\nculture\n\nlearning\n\n\n\nMy detailed AI-focused notes on Bennett‚Äôs ‚Äòspeaking‚Äô breakthrough: proto-conversations, joint attention, naming/grammar as tethers, and language as a cultural replication system.\n\n\n\n\n\nDec 16, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nBreakthrough #4 ‚Äî Mentalizing: Second-Order Models, Theory of Mind, and Primate Intelligence\n\n\n\nneuroscience\n\ntheory-of-mind\n\nmulti-agent\n\ncognition\n\n\n\nMy detailed AI-focused notes on Bennett‚Äôs ‚Äòmentalizing‚Äô breakthrough: how primates model their own minds and reuse it to infer others‚Äô intent, enabling ToM and imitation.\n\n\n\n\n\nDec 16, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nBreakthrough #1 ‚Äî Steering: How the First Brains Solved Navigation\n\n\n\nneuroscience\n\nevolution\n\nreinforcement-learning\n\ncontrol\n\n\n\nMy AI-focused notes on Bennett‚Äôs ‚Äòsteering‚Äô breakthrough: valence, action selection, and why bilateral bodies made brains worth it.\n\n\n\n\n\nDec 16, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nBreakthrough #3 ‚Äî Simulating: Neocortex, World Models, and Learning by Imagining\n\n\n\nneuroscience\n\nworld-models\n\nplanning\n\nmodel-based-RL\n\n\n\nMy detailed AI-focused notes on Bennett‚Äôs ‚Äòsimulating‚Äô breakthrough: model-based RL, the search problem, episodic replay, and counterfactual credit assignment.\n\n\n\n\n\nDec 16, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nBreakthrough #2 ‚Äî Reinforcing: Dopamine, Basal Ganglia, and the Birth of RL\n\n\n\nneuroscience\n\nreinforcement-learning\n\ndopamine\n\nevolution\n\n\n\nMy AI-focused notes on Bennett‚Äôs ‚Äòreinforcing‚Äô breakthrough: temporal-difference learning as a biological design pattern.\n\n\n\n\n\nDec 16, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nTokenization Exploration\n\n\n\nprompting\n\nevaluation\n\nLLMs\n\n\n\nI explored tokenization with LLMs and evaluated prompting effectiveness\n\n\n\n\n\nNov 3, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nTokenization Laws and DNA Models\n\n\n\nLLMs\n\nbiology\n\ntokenization\n\n\n\nUnderstanding k-mer tokenization and EVO-2 style nucleotide modeling\n\n\n\n\n\nSep 14, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nInformation-Theoretic Bounds and Training Dynamics of Transformers\n\n\n\ntheory\n\ntransformers\n\ninformation-theory\n\n\n\nExplaining transformer training through entropy, cross-entropy, and bits-per-token\n\n\n\n\n\nSep 14, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nCUDA Inference Kernels and Serving Stacks\n\n\n\nsystems\n\ninference\n\ncuda\n\n\n\nKey kernels, memory layouts, and serving choices (with vLLM notes)\n\n\n\n\n\nSep 14, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/004_tokenizers/index.html",
    "href": "posts/004_tokenizers/index.html",
    "title": "Tokenization Exploration",
    "section": "",
    "text": "I explored tokenization with two models (GPT‚Äë5 and Sonnet 4.5) and reflected on what made the prompts effective. The discussion moved from DFT basics to whether a Fourier transform could serve as a tokenizer, then broadened into ways to improve on BPE and how to tailor a tokenizer to a specific corpus. A direct URL to my feedback will be added when it‚Äôs posted on the issues page."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#context",
    "href": "posts/004_tokenizers/index.html#context",
    "title": "Tokenization Exploration",
    "section": "",
    "text": "I explored tokenization with two models (GPT‚Äë5 and Sonnet 4.5) and reflected on what made the prompts effective. The discussion moved from DFT basics to whether a Fourier transform could serve as a tokenizer, then broadened into ways to improve on BPE and how to tailor a tokenizer to a specific corpus. A direct URL to my feedback will be added when it‚Äôs posted on the issues page."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#what-the-interactions-showed",
    "href": "posts/004_tokenizers/index.html#what-the-interactions-showed",
    "title": "Tokenization Exploration",
    "section": "What the interactions showed",
    "text": "What the interactions showed\nBoth models handled the DFT prompt comfortably, giving standard forward/inverse formulas and practical notes. When I asked if a DFT could be used as a tokenizer, the answers pushed back: a plain Fourier representation is lossy, fixed‚Äëlength, and not aligned to discrete linguistic structure; it‚Äôs better used inside the model or as auxiliary features. On ‚Äúbetter than BPE,‚Äù the conversation shifted to learned tokenizers (e.g., VQ with byte fallback), morphology‚Äëaware segmentation, adaptive/MDL‚Äëmotivated tokenization, and semantic or graph‚Äëbased grouping‚Äîapproaches that target predictability rather than just compression."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#models-and-outputs",
    "href": "posts/004_tokenizers/index.html#models-and-outputs",
    "title": "Tokenization Exploration",
    "section": "Models and outputs",
    "text": "Models and outputs\nI used two different model families (GPT‚Äë5 and Sonnet 4.5) and iterated across related prompts rather than stopping at one answer. That back‚Äëand‚Äëforth surfaced consensus on basics, clearer reasoning about feasibility, and a pathway from critique to concrete designs."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#conversation-excerpts",
    "href": "posts/004_tokenizers/index.html#conversation-excerpts",
    "title": "Tokenization Exploration",
    "section": "Conversation excerpts",
    "text": "Conversation excerpts\nI began with a simple check: ‚Äúdo you know discrete Fourier transform?‚Äù The reply included the standard definition ‚Äî ‚ÄúX[k] = Œ£ x[n] e^(‚àíj2œÄkn/N), with an inverse x[n] = (1/N) Œ£ X[k] e^(+j2œÄkn/N)‚Äù ‚Äî and reminders about linearity, modulation, and the FFT. From there I asked: ‚Äúimagine we want to use this as tokenizer to LLMs‚Ä¶ is that possible?‚Äù The answer argued a plain DFT is a poor tokenizer because it‚Äôs lossy, tied to fixed lengths, and not aligned to discrete symbols; it suggested using Fourier features inside models instead. Finally, I asked for alternatives ‚Äúbetter than BPE‚Äù and a tokenizer that can be tailored to data; the response proposed learned VQ schemes with a byte‚Äëfallback for exact reversibility, morphology‚Äëaware segmentation, adaptive tokenization optimized for predictability, and semantic/graph‚Äëbased grouping."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#what-worked-in-the-prompting",
    "href": "posts/004_tokenizers/index.html#what-worked-in-the-prompting",
    "title": "Tokenization Exploration",
    "section": "What worked in the prompting",
    "text": "What worked in the prompting\nUsing two different models and pushing beyond the first answer helped. Starting with definitions created a shared foundation; then asking ‚Äúis this actually usable as a tokenizer?‚Äù forced a feasibility check and concrete trade‚Äëoffs. Requesting ‚Äúnovel ideas‚Äù and a ‚Äútailorable tokenizer‚Äù nudged the models to move from critique to constructive designs (e.g., VQ with byte‚Äëfallback) rather than staying abstract. Framing the problem in terms of reversibility and predictability kept the discussion grounded in properties that matter for real systems."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#what-i-expected-vs-what-surprised-me",
    "href": "posts/004_tokenizers/index.html#what-i-expected-vs-what-surprised-me",
    "title": "Tokenization Exploration",
    "section": "What I expected vs what surprised me",
    "text": "What I expected vs what surprised me\nI expected the definitions to be solid and the DFT idea to be challenged on reversibility and fit to discrete text. What surprised me was how quickly the conversation moved from critique to a concrete, workable direction: a learned vector‚Äëquantized tokenizer with exact byte fallback, paired with a predictability‚Äëoriented objective. That made the idea immediately testable rather than purely conceptual."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#how-id-prompt-next-time",
    "href": "posts/004_tokenizers/index.html#how-id-prompt-next-time",
    "title": "Tokenization Exploration",
    "section": "How I‚Äôd prompt next time",
    "text": "How I‚Äôd prompt next time\nNext time I‚Äôd ask for two contrasting designs and when each is preferable, require a couple of citations, and force the model to spell out likely failure modes with mitigations. I‚Äôd also request a tiny code sketch and a toy evaluation plan so the proposal is actionable. Finally, I‚Äôd have one model critique the other‚Äôs answer and estimate simple trade‚Äëoffs like token savings versus predicted perplexity change."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#concrete-outputs-sampled-snippets",
    "href": "posts/004_tokenizers/index.html#concrete-outputs-sampled-snippets",
    "title": "Tokenization Exploration",
    "section": "Concrete outputs sampled (snippets)",
    "text": "Concrete outputs sampled (snippets)\n\nDFT feasibility as tokenizer: ‚ÄúUsing a plain DFT as the tokenizer for text LLMs isn‚Äôt practical‚Ä¶ it‚Äôs lossy, length‚Äërigid, and not semantically aligned. Use Fourier ideas inside the model or as features instead.‚Äù\n\n\nNovel tokenization ideas: ‚ÄúLearned VQ tokenizers, morphology‚Äëaware hybrids, adaptive/MDL tokenization, semantic/graph clustering, multi‚Äëresolution tokens.‚Äù\n\n\nData‚Äëtailorable tokenizer: ‚ÄúVQ‚ÄëText with byte fallback: learned codes for common spans; exact byte runs for everything else; train with reconstruction + commitment + predictability (MDL‚Äëstyle).‚Äù"
  },
  {
    "objectID": "posts/001_tokenization_dna_models/index.html",
    "href": "posts/001_tokenization_dna_models/index.html",
    "title": "Tokenization Laws and DNA Models",
    "section": "",
    "text": "This post explores how tokenization choices shape model capacity and performance, and how those ideas translate to DNA models. We‚Äôll use k-mer tokenization on nucleotide sequences and discuss how EVO-2-style models work with nucleotide tokens.\n\nWhat are tokenization laws?\nHow do k-mer vocabularies trade off context length vs.¬†vocabulary size?\nHow might EVO-2-like models represent nucleotides and long-range dependencies?\n\n\n  \n  Figure 1. EVO-2 model."
  },
  {
    "objectID": "posts/001_tokenization_dna_models/index.html#overview",
    "href": "posts/001_tokenization_dna_models/index.html#overview",
    "title": "Tokenization Laws and DNA Models",
    "section": "",
    "text": "This post explores how tokenization choices shape model capacity and performance, and how those ideas translate to DNA models. We‚Äôll use k-mer tokenization on nucleotide sequences and discuss how EVO-2-style models work with nucleotide tokens.\n\nWhat are tokenization laws?\nHow do k-mer vocabularies trade off context length vs.¬†vocabulary size?\nHow might EVO-2-like models represent nucleotides and long-range dependencies?\n\n\n  \n  Figure 1. EVO-2 model."
  },
  {
    "objectID": "posts/001_tokenization_dna_models/index.html#tokenization-laws-in-brief",
    "href": "posts/001_tokenization_dna_models/index.html#tokenization-laws-in-brief",
    "title": "Tokenization Laws and DNA Models",
    "section": "Tokenization laws in brief",
    "text": "Tokenization laws in brief\nTokenization laws describe empirical tradeoffs between model size, context length, and tokenizer vocabulary. For fixed compute, larger vocabularies shrink sequence length but increase embedding/softmax cost; smaller vocabularies do the opposite. The optimal point depends on data distribution and task (e.g., code vs.¬†natural language vs.¬†DNA)."
  },
  {
    "objectID": "posts/001_tokenization_dna_models/index.html#dna-modeling-with-k-mers",
    "href": "posts/001_tokenization_dna_models/index.html#dna-modeling-with-k-mers",
    "title": "Tokenization Laws and DNA Models",
    "section": "DNA modeling with k-mers",
    "text": "DNA modeling with k-mers\nFor DNA, a common tokenizer uses k-mers over the alphabet {A,C,G,T}. The vocabulary size is 4^k, and the stride determines overlap. Larger k compresses sequences but grows the vocab; smaller k expands sequences but keeps vocab small.\n\nExample: k=3 (3-mers) ‚áí vocab size = 64\nExample: k=6 (6-mers) ‚áí vocab size = 4096\n\nWe can quickly demonstrate 3-mer tokenization with stride 1.\n\n# Simple 3-mer tokenizer demo\nfrom collections import Counter\nfrom typing import List\n\n\ndef kmers(sequence: str, k: int = 3, stride: int = 1) -&gt; List[str]:\n    sequence = sequence.upper().replace(\"U\", \"T\")\n    tokens = []\n    for i in range(0, len(sequence) - k + 1, stride):\n        kmer = sequence[i:i+k]\n        if set(kmer) &lt;= {\"A\", \"C\", \"G\", \"T\"}:\n            tokens.append(kmer)\n    return tokens\n\n\nseq = \"ACGTACGTGACCT\"\nks = kmers(seq, k=3, stride=1)\nprint(\"Sequence:\", seq)\nprint(\"3-mers:\", ks)\nprint(\"Unique 3-mers:\", sorted(set(ks)))\nprint(\"Counts:\", Counter(ks))"
  },
  {
    "objectID": "posts/001_tokenization_dna_models/index.html#how-evo-2-style-models-use-nucleotide-tokens",
    "href": "posts/001_tokenization_dna_models/index.html#how-evo-2-style-models-use-nucleotide-tokens",
    "title": "Tokenization Laws and DNA Models",
    "section": "How EVO-2-style models use nucleotide tokens",
    "text": "How EVO-2-style models use nucleotide tokens\nHigh-level idea: - Use a tokenizer over nucleotides (e.g., 3‚Äì6-mer tokens) to convert sequences into discrete tokens. - Train an autoregressive transformer over these tokens to model genomic sequences. - Incorporate long-range context (e.g., thousands to millions of bases) using efficient attention or memory mechanisms. - Optionally multitask with masked objectives or structure-aware heads.\nThis lets the model learn motifs, regulatory patterns, and long-range interactions directly from token sequences."
  },
  {
    "objectID": "posts/005_breakthrough1/index.html",
    "href": "posts/005_breakthrough1/index.html",
    "title": "Breakthrough #1 ‚Äî Steering: How the First Brains Solved Navigation",
    "section": "",
    "text": "Bennett‚Äôs first ‚Äúbreakthrough‚Äù is intentionally humble. He isn‚Äôt trying to start the history of intelligence with something that resembles human thought. He starts with the first situation where having a nervous system pays for itself: once an organism can move, it must solve a control problem in real time.\nThat control problem has a very specific flavor. The organism is bombarded by noisy sensory streams. The world is patchy: food is not everywhere, danger is not everywhere. The organism has limited energy and limited time. So it needs a way to pick the next motor command that tends, on average, to move it toward beneficial states and away from harmful ones. That‚Äôs steering.\nWhat‚Äôs easy to miss is that ‚Äústeering‚Äù is already more than a reflex. Reflexes are local and brittle: they fire when a trigger appears, they don‚Äôt reconcile conflicts, and they don‚Äôt stabilize behavior over time. Steering, as Bennett uses the term, is the first place you see two properties that later become hallmarks of intelligence:\n\nIntegration. Multiple cues compete. Odor says ‚Äúgo,‚Äù pain says ‚Äústop,‚Äù heat says ‚Äúleave,‚Äù social signal says ‚Äúapproach.‚Äù Steering is the machinery that merges these into one coherent directional push.\nCommitment. If you change direction on every small fluctuation, you don‚Äôt get anywhere. Early nervous systems therefore need internal state‚Äîslow variables that persist and bias behavior for seconds to minutes. That‚Äôs how you get stable modes like exploration versus exploitation, approach versus avoidance, or ‚Äústay and forage‚Äù versus ‚Äúget out now.‚Äù\n\nBennett‚Äôs core claim is that the earliest brains build a tiny but powerful abstraction: valence. The world is carved into ‚Äúgood-for-me‚Äù and ‚Äúbad-for-me‚Äù signals. That carving can be wired at the start (some stimuli are intrinsically appetitive or aversive), but it becomes conditional when internal state gets involved. The same cue can flip meaning depending on hunger, thirst, fatigue, or stress.\nThis is the evolutionary origin of the objective function. Before you get learning, you get a stable notion of what counts as better versus worse. It sounds obvious, but it‚Äôs foundational: any agent-like system becomes interpretable only when you know what it‚Äôs trying to optimize. Bennett is saying: biology solved objective specification first, then built increasingly powerful machinery for optimizing it.\nA good way to see how strong steering can be is to picture an organism in a gradient (odor, temperature, salinity). If the signal gets better as it moves, keep going. If it gets worse, reorient. That kind of policy can look surprisingly purposeful because the environment itself is providing dense guidance. You don‚Äôt need a map if the world gives you ‚Äúhotter/colder‚Äù feedback every second.\nThere‚Äôs a deeper lesson here that generalizes beyond biology: a lot of competence comes from exploiting environmental structure, not from building elaborate internal representations. If the world offers smooth gradients, a small controller goes a long way. And if the world stops offering those gradients‚Äîif rewards become delayed, sparse, or deceptive‚Äîthen the same controller hits a wall. Bennett‚Äôs whole staircase can be read as a sequence of upgrades that appear when the world becomes too hard for the previous controller class.\nFrom a math/ML viewpoint, I‚Äôd describe steering as a small, stable policy class coupled to a hard-coded reward prior. Early intelligence is not ‚Äúgeneral reasoning.‚Äù It‚Äôs the ability to turn sensory correlations into motion that improves survival. That‚Äôs why steering is such a good starting point: it forces you to take intelligence seriously as embodied control, not as disembodied problem-solving.\nIf you want a pointed discussion question: what would count as the minimal internal state required to make steering robust? You can‚Äôt just say ‚Äúapproach good / avoid bad.‚Äù You need persistence, conflict resolution, and the ability to change what is ‚Äúgood‚Äù when the body‚Äôs needs change. That‚Äôs already the seed of later reinforcement learning.\n\n  \n  Figure 1. Gradient-following behavior as a minimal navigation strategy"
  },
  {
    "objectID": "posts/002_information_theory_transformers/index.html",
    "href": "posts/002_information_theory_transformers/index.html",
    "title": "Information-Theoretic Bounds and Training Dynamics of Transformers",
    "section": "",
    "text": "This post sketches how training a transformer can be framed in information-theoretic terms:\n\nTraining minimizes empirical cross-entropy, i.e., expected negative log-likelihood.\nThe optimal achievable loss is the entropy rate of the data (given the model class).\nBits-per-token (bpt) is simply loss in base-2 units.\nGeneralization can be discussed via compression and mutual information.\n\n\n  \n  Figure 1. Transformers"
  },
  {
    "objectID": "posts/002_information_theory_transformers/index.html#overview",
    "href": "posts/002_information_theory_transformers/index.html#overview",
    "title": "Information-Theoretic Bounds and Training Dynamics of Transformers",
    "section": "",
    "text": "This post sketches how training a transformer can be framed in information-theoretic terms:\n\nTraining minimizes empirical cross-entropy, i.e., expected negative log-likelihood.\nThe optimal achievable loss is the entropy rate of the data (given the model class).\nBits-per-token (bpt) is simply loss in base-2 units.\nGeneralization can be discussed via compression and mutual information.\n\n\n  \n  Figure 1. Transformers"
  },
  {
    "objectID": "posts/002_information_theory_transformers/index.html#cross-entropy-entropy-rate-and-bits-per-token",
    "href": "posts/002_information_theory_transformers/index.html#cross-entropy-entropy-rate-and-bits-per-token",
    "title": "Information-Theoretic Bounds and Training Dynamics of Transformers",
    "section": "Cross-entropy, entropy rate, and bits-per-token",
    "text": "Cross-entropy, entropy rate, and bits-per-token\n\nCross-entropy (training loss) estimates how many nats per token the model spends to encode the data.\nBits-per-token (bpt) is just loss in base-2: bpt = loss_nat / ln 2.\nThe theoretical lower bound is the entropy rate H of the data-generating process; if the model class is misspecified, the optimum is H + D(P||Q*).\n\n\n# Convert loss (nats/token) to bits-per-token and perplexity\nimport math\nfrom typing import Iterable, Tuple\n\n\ndef loss_to_metrics(loss_nats: float) -&gt; Tuple[float, float]:\n    \"\"\"Return (bits_per_token, perplexity) from loss in nats/token.\"\"\"\n    bits_per_token = loss_nats / math.log(2)\n    perplexity = math.exp(loss_nats)\n    return bits_per_token, perplexity\n\n\nexample_losses = [3.5, 2.8, 2.2, 1.9]\nfor step, loss in enumerate(example_losses, start=1):\n    bpt, ppl = loss_to_metrics(loss)\n    print(f\"step={step:02d} loss={loss:.3f} nats -&gt; bpt={bpt:.3f}, ppl={ppl:.2f}\")"
  },
  {
    "objectID": "posts/002_information_theory_transformers/index.html#training-dynamics-as-compression",
    "href": "posts/002_information_theory_transformers/index.html#training-dynamics-as-compression",
    "title": "Information-Theoretic Bounds and Training Dynamics of Transformers",
    "section": "Training dynamics as compression",
    "text": "Training dynamics as compression\n\nMinimizing cross-entropy is equivalent to minimizing expected code length.\nEarly training reduces redundant predictability (frequent patterns); later, model learns rarer structure.\nCapacity vs.¬†data curve: more parameters lower achievable cross-entropy until compute/data bottlenecks.\nGeneralization: MDL view‚Äîgood models compress both train and test data with similar code lengths."
  },
  {
    "objectID": "posts/006_breakthrough/index.html",
    "href": "posts/006_breakthrough/index.html",
    "title": "Breakthrough #2 ‚Äî Reinforcing: Dopamine, Basal Ganglia, and the Birth of RL",
    "section": "",
    "text": "Bennett‚Äôs second breakthrough‚Äîreinforcing‚Äîis where the story becomes recognizably ‚Äúlearning from reward.‚Äù Steering can produce competent behavior, but it cannot systematically improve itself when the environment changes. Reinforcing adds a mechanism that tunes action tendencies based on outcomes, and Bennett anchors this in vertebrate circuitry: basal ganglia plus dopamine.\nThe part that matters is not the anatomical labels; it‚Äôs the design pattern. You have an action-selection system that decides what gets executed, and you have a scalar teaching signal that nudges that system so it repeats actions with better long-run consequences.\nBennett emphasizes that the teaching signal is not simply ‚Äúreward happened.‚Äù It‚Äôs closer to ‚Äúthat was better or worse than expected.‚Äù That small conceptual twist is what makes reinforcement scale to real life, where outcomes are delayed and ambiguous.\nIf your learning signal only arrives at the endpoint, you can‚Äôt assign credit to the earlier choices that made the endpoint possible. A prediction-based teaching signal solves this by moving learning earlier in time: cues become meaningful because they predict outcomes, and the learning signal lights up at those cues rather than only at the reward itself. When an expected reward fails to arrive, the teaching signal goes negative. This gives you a bidirectional correction mechanism that shapes behavior efficiently.\nThis is also why Bennett pushes back on the ‚Äúdopamine = pleasure‚Äù caricature. Pleasure is an experience; learning is an algorithm. A teaching signal must carry information about mismatch between expectation and reality. When you interpret the dopamine story through that lens, a lot of behavioral phenomena stop being mysterious.\nAnticipation is what it looks like when predictors of reward become valuable. Disappointment is what it looks like when expectation is violated. Motivation becomes the behavioral manifestation of predicted value biasing action selection before the reward exists. Bennett is implicitly saying: many affective states are not add-ons; they are control-relevant summaries of prediction and uncertainty.\nA concept I think Bennett gets right is the separation between what counts as reward and how behavior is shaped by reward. Biological reward is anchored to survival: hunger, thirst, pain, thermoregulation, social safety. Reinforcement is the learning mechanism that figures out what to do in order to satisfy those needs in a particular environment.\nThat separation is important for two reasons. First, it explains flexibility: the organism can learn novel strategies without evolution rewriting its physiology every generation. Second, it highlights the danger of misalignment in any system with powerful optimization: if the reward signal is corrupted, the learning machinery will still optimize it, often in pathological ways. Bennett‚Äôs later discussion of superintelligence rhymes with this: the bigger the optimizer, the more central the objective becomes.\nReinforcement creates habits. Habits are a feature, not a bug: they‚Äôre fast, robust, and cheap. But Bennett treats habit learning as having an inherent ceiling. It‚Äôs slow to acquire when outcomes are rare. It becomes rigid in distribution shift. It struggles when you need to evaluate novel actions you‚Äôve never tried.\nThat limitation sets up the next breakthrough. Once acting becomes expensive, evolution has an incentive to build a system that can learn without acting‚Äîby imagining. Reinforcing makes the agent trainable; simulating makes it sample efficient.\n\n\n\n  \n  Figure 1. Action gating trained by scalar feedback (actor‚Äìcritic motif as an intuition)"
  }
]