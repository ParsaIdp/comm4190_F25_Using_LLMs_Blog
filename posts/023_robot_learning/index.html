<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Parsa Idehpour">
<meta name="dcterms.date" content="2025-12-17">
<meta name="description" content="Real robots can’t afford to break themselves 10,000 times to learn a task. Sample efficiency is the central puzzle of robot learning.">

<title>From Simulation to Reality: The Sample Efficiency Problem in Robotics – My Explorations with LLMs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-0b37c64f34216b628666a8dac638b53b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Explorations with LLMs</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">From Simulation to Reality: The Sample Efficiency Problem in Robotics</h1>
                  <div>
        <div class="description">
          Real robots can’t afford to break themselves 10,000 times to learn a task. Sample efficiency is the central puzzle of robot learning.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">robotics</div>
                <div class="quarto-category">reinforcement-learning</div>
                <div class="quarto-category">simulation</div>
                <div class="quarto-category">machine-learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Parsa Idehpour </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 17, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Deep reinforcement learning has produced stunning results: superhuman game players, elegant locomotion in simulation, complex manipulation in controlled settings. But deploying these methods on real robots remains brutally difficult. The culprit is sample efficiency—or rather, the lack of it.</p>
<p>A real robot is slow, expensive, and fragile. It can’t run at 10,000x real-time. It can’t reset instantly after failure. Every interaction takes actual seconds, and failures can damage hardware or the environment. The algorithms that work beautifully in simulation often require millions of episodes—decades of real-time experience that no physical system can afford.</p>
<p>This post explores why the sample efficiency problem is so hard and the strategies researchers use to work around it.</p>
<section id="the-sim-to-real-gap" class="level2">
<h2 class="anchored" data-anchor-id="the-sim-to-real-gap">The Sim-to-Real Gap</h2>
<p>The obvious solution: train in simulation, then transfer to reality. Simulation is fast, cheap, safe, and parallelizable. You can run thousands of rollouts simultaneously, reset instantly, and never break anything.</p>
<p>The problem is that simulations are always wrong. They simplify physics, miss unmodeled dynamics, and present observations in idealized forms. A policy trained in simulation often fails dramatically when confronted with real-world friction, sensor noise, lighting variation, and latency.</p>
<p>This is the sim-to-real gap, and closing it is one of the central challenges of robot learning.</p>
<section id="domain-randomization" class="level3">
<h3 class="anchored" data-anchor-id="domain-randomization">Domain Randomization</h3>
<p>The most widely used technique is domain randomization: during simulation training, you randomize everything that might differ in the real world. Mass, friction, lighting, sensor noise, actuator delays—all get sampled from distributions that (hopefully) include the real values.</p>
<p>The idea is that if your policy works across a wide range of simulated conditions, it will be robust enough to handle the specific conditions it encounters in reality. You’re training for generalization rather than performance on any single simulation.</p>
<p>Domain randomization has produced real successes: OpenAI’s Rubik’s cube manipulation, quadruped locomotion, drone racing. But it has limitations:</p>
<ul>
<li><strong>Covering the real distribution is hard.</strong> You need to randomize the right things over the right ranges.</li>
<li><strong>Too much randomization hurts performance.</strong> The policy becomes conservative, optimizing for the worst case rather than exploiting structure.</li>
<li><strong>Some gaps can’t be randomized away.</strong> If your simulation systematically misses a dynamic effect (say, cable dynamics or deformable objects), no amount of randomization helps.</li>
</ul>
</section>
<section id="system-identification-and-adaptive-methods" class="level3">
<h3 class="anchored" data-anchor-id="system-identification-and-adaptive-methods">System Identification and Adaptive Methods</h3>
<p>An alternative is to make the simulation more accurate. System identification estimates the real physical parameters (mass, friction, etc.) from data and plugs them into the simulator. If your simulation matches reality closely enough, the gap shrinks.</p>
<p>The challenge is that real systems are complex, nonlinear, and partially observable. Some parameters are hard to measure. Others change over time (friction surfaces wear, joints loosen). And there’s always unmodeled dynamics you don’t know you’re missing.</p>
<p>Adaptive methods try to have it both ways: they train policies that can adapt to new dynamics online. Techniques include:</p>
<ul>
<li><strong>Context-conditioned policies</strong>: The policy takes an embedding of recent observations that implicitly encodes environment parameters</li>
<li><strong>Rapid motor adaptation (RMA)</strong>: A learned adaptation module adjusts policy behavior based on experienced dynamics</li>
<li><strong>Meta-learning</strong>: Train to adapt quickly from limited real-world data</li>
</ul>
<p>These methods shift the problem from “get the simulation right” to “learn to adapt quickly.” That’s often more tractable.</p>
</section>
</section>
<section id="model-based-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="model-based-reinforcement-learning">Model-Based Reinforcement Learning</h2>
<p>Model-free RL learns policies directly from reward signals. Model-based RL learns a model of the environment and uses it to plan or generate synthetic experience.</p>
<p>The sample efficiency argument for model-based methods is intuitive: if you can learn the dynamics of your environment, you can imagine trajectories without executing them. Every real interaction teaches you about the world, and you can extract much more learning signal from that knowledge than model-free methods do.</p>
<p>Key approaches:</p>
<ul>
<li><strong>Dreamer</strong> and variants: Learn a latent world model, then train a policy entirely inside imagined trajectories</li>
<li><strong>MBPO</strong> (Model-Based Policy Optimization): Use a learned model to generate synthetic rollouts that augment real data</li>
<li><strong>Planning through models</strong>: Use the model for lookahead during action selection (MPC-style)</li>
</ul>
<p>The trade-off: model-based methods are more sample-efficient but suffer when the model is wrong. Compounding errors during imagination can send the policy off into fantasy. The model-free methods are data-hungry but robust to model misspecification because they never rely on a model.</p>
<p>In practice, the best results often come from combining both: use a learned model for sample efficiency while retaining some model-free updates for robustness.</p>
</section>
<section id="imitation-learning" class="level2">
<h2 class="anchored" data-anchor-id="imitation-learning">Imitation Learning</h2>
<p>If optimal behavior is hard to specify via reward but easy to demonstrate, why not learn from demonstrations? Imitation learning uses human demonstrations (teleoperation, kinesthetic teaching, video) as the primary training signal.</p>
<p><strong>Behavioral cloning</strong> is the simplest approach: treat demonstration trajectories as supervised learning data and train a policy to predict the demonstrated actions. It’s fast and straightforward but suffers from distribution shift—the policy makes small errors that compound, taking it into states the demonstrator never visited.</p>
<p><strong>DAgger</strong> and related methods fix this by iteratively collecting demonstration labels for the states the learned policy actually visits, keeping the training distribution matched to deployment.</p>
<p><strong>Inverse reinforcement learning (IRL)</strong> infers the reward function from demonstrations, then optimizes a policy for that reward. This can generalize better than cloning because it learns the intent behind demonstrations, not just the surface behavior.</p>
<p>Imitation learning is now the dominant paradigm for many robotics applications. It’s sample-efficient in terms of robot time (demonstrations are cheap), and it provides a natural way to inject human knowledge without specifying rewards.</p>
</section>
<section id="offline-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="offline-reinforcement-learning">Offline Reinforcement Learning</h2>
<p>What if you have a large dataset of past experience but can’t run any new experiments? Offline RL (or batch RL) learns from fixed datasets without exploration.</p>
<p>The core challenge is distributional shift: standard RL algorithms will query the learned Q-function or model for out-of-distribution actions, get unreliable estimates, and diverge. Offline methods must constrain the policy to stay close to the behavior that generated the data.</p>
<p>Key techniques:</p>
<ul>
<li><strong>Conservative Q-Learning (CQL)</strong>: Penalize Q-values for actions not in the dataset</li>
<li><strong>Behavior-constrained methods (BCQ, BRAC)</strong>: Explicitly limit the policy to actions similar to demonstrated ones</li>
<li><strong>Decision transformers</strong>: Treat RL as sequence modeling, predicting actions conditioned on desired returns</li>
</ul>
<p>Offline RL is attractive for robotics because it can leverage historical data—past experiments, logged teleoperation, legacy systems. It’s particularly promising when combined with large pre-existing datasets from robot fleets.</p>
</section>
<section id="foundation-models-for-robotics" class="level2">
<h2 class="anchored" data-anchor-id="foundation-models-for-robotics">Foundation Models for Robotics</h2>
<p>The latest wave of robot learning borrows from the foundation model paradigm: train large, general models on diverse data, then fine-tune or prompt for specific tasks.</p>
<p><strong>RT-1</strong> and <strong>RT-2</strong> (Robotics Transformer): Google’s models train on large datasets of robot demonstrations, producing policies that generalize across tasks, objects, and environments. RT-2 goes further by connecting the policy to a vision-language model, enabling instruction-following and semantic generalization.</p>
<p><strong>PaLM-E</strong>: Connects PaLM language model to embodied sensing, allowing robots to ground language in visual and spatial context.</p>
<p><strong>OpenVLA</strong> and similar open-source efforts: Democratizing access to vision-language-action models.</p>
<p>The promise: if you train on enough diverse data, the model learns generalizable representations and control primitives. New tasks become few-shot adaptation rather than from-scratch training.</p>
<p>The challenges: collecting large-scale robot data is expensive, and generalization across embodiments (different robot morphologies) remains difficult. It’s not yet clear whether “scaling up data” can close the sim-to-real gap or whether real-world finetuning will always be needed.</p>
</section>
<section id="hardware-constraints-the-edge-inference-problem" class="level2">
<h2 class="anchored" data-anchor-id="hardware-constraints-the-edge-inference-problem">Hardware Constraints: The Edge Inference Problem</h2>
<p>Robot policies must run in real-time on the robot. This often means edge deployment on compute-constrained hardware with latency requirements.</p>
<p>Cloud inference introduces network latency (50-200ms round-trip), which is unacceptable for reactive control. So policies must be small enough to run locally. This creates tension with the “scale up the model” paradigm that has driven recent progress.</p>
<p>Solutions include:</p>
<ul>
<li><strong>Model distillation</strong>: Train a large model, then compress it into a smaller one that runs on-robot</li>
<li><strong>Edge-optimized architectures</strong>: Design models specifically for inference efficiency</li>
<li><strong>Hierarchical control</strong>: Use fast, small policies for low-level control with slower, larger models for high-level planning</li>
</ul>
<p>Hardware is improving (Apple Neural Engine, NVIDIA Jetson Orin, dedicated NPUs), but the physics of heat dissipation and battery capacity impose fundamental limits on mobile robots.</p>
</section>
<section id="the-last-10-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-last-10-problem">The “Last 10%” Problem</h2>
<p>Perhaps the most frustrating lesson from robot learning: getting 90% performance is often straightforward, but the last 10% requires orders of magnitude more effort.</p>
<p>Dexterous manipulation exemplifies this. You can quickly teach a robot to grasp most objects most of the time. But handling unusual shapes, recovering from failures, and operating reliably in unstructured environments takes years of engineering.</p>
<p>The long tail of edge cases—uncommon objects, adversarial situations, hardware degradation—dominates deployment difficulty. This is fundamentally a sample efficiency problem at a different level: edge cases are rare by definition, so collecting enough training data for them is expensive.</p>
<p>Partial solutions:</p>
<ul>
<li><strong>Failure-triggered data collection</strong>: Identify failure modes in deployment and specifically collect data for them</li>
<li><strong>Synthetic generation of edge cases</strong>: Use simulation or generative models to create adversarial scenarios</li>
<li><strong>Graceful degradation and hand-off</strong>: When uncertain, robots can ask for help or refuse to act rather than failing catastrophically</li>
</ul>
<p>The uncomfortable truth: robot learning is advancing rapidly, but robust deployment in the real world still requires extensive engineering beyond the learning algorithm itself.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ParsaIdp\.github\.io\/comm4190_F25_Using_LLMs_Blog\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>