{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Information-Theoretic Bounds and Training Dynamics of Transformers\"\n",
        "description: \"Explaining transformer training through entropy, cross-entropy, and bits-per-token\"\n",
        "author: \"Your Name\"\n",
        "date: \"2025-09-14\"\n",
        "categories:\n",
        "  - theory\n",
        "  - transformers\n",
        "  - information-theory\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "This post sketches how training a transformer can be framed in information-theoretic terms:\n",
        "\n",
        "- Training minimizes empirical cross-entropy, i.e., expected negative log-likelihood.\n",
        "- The optimal achievable loss is the entropy rate of the data (given the model class).\n",
        "- Bits-per-token (bpt) is simply loss in base-2 units.\n",
        "- Generalization can be discussed via compression and mutual information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image placeholder\n",
        "\n",
        "Add your figure here. Replace the path once you upload the image file into this folder.\n",
        "\n",
        "```{=html}\n",
        "<div style=\"text-align:center;\">\n",
        "  <img src=\"/Users/idea/comm4190_F25_Using_LLMs_Blog/entropy-27-00589-g001-550.jpg\" alt=\"Information-theoretic view of training\" width=\"60%\"/>\n",
        "  <p><em>Figure 1. Placeholder for information-theoretic diagram. Replace with your actual image file.</em></p>\n",
        "</div>\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-entropy, entropy rate, and bits-per-token\n",
        "\n",
        "- Cross-entropy (training loss) estimates how many nats per token the model spends to encode the data.\n",
        "- Bits-per-token (bpt) is just loss in base-2: bpt = loss_nat / ln 2.\n",
        "- The theoretical lower bound is the entropy rate H of the data-generating process; if the model class is misspecified, the optimum is H + D(P||Q*).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert loss (nats/token) to bits-per-token and perplexity\n",
        "import math\n",
        "from typing import Iterable, Tuple\n",
        "\n",
        "\n",
        "def loss_to_metrics(loss_nats: float) -> Tuple[float, float]:\n",
        "    \"\"\"Return (bits_per_token, perplexity) from loss in nats/token.\"\"\"\n",
        "    bits_per_token = loss_nats / math.log(2)\n",
        "    perplexity = math.exp(loss_nats)\n",
        "    return bits_per_token, perplexity\n",
        "\n",
        "\n",
        "example_losses = [3.5, 2.8, 2.2, 1.9]\n",
        "for step, loss in enumerate(example_losses, start=1):\n",
        "    bpt, ppl = loss_to_metrics(loss)\n",
        "    print(f\"step={step:02d} loss={loss:.3f} nats -> bpt={bpt:.3f}, ppl={ppl:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training dynamics as compression\n",
        "\n",
        "- Minimizing cross-entropy is equivalent to minimizing expected code length.\n",
        "- Early training reduces redundant predictability (frequent patterns); later, model learns rarer structure.\n",
        "- Capacity vs. data curve: more parameters lower achievable cross-entropy until compute/data bottlenecks.\n",
        "- Generalization: MDL viewâ€”good models compress both train and test data with similar code lengths.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
