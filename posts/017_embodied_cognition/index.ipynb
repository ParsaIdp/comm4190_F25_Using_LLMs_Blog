{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"Why Bodies Matter: The Case for Embodied Intelligence\"\n",
        "description: \"Intelligence didn't evolve to solve puzzles\u2014it evolved to control bodies. This reframes how we think about AI.\"\n",
        "author: \"Parsa Idehpour\"\n",
        "date: \"2025-12-17\"\n",
        "categories:\n",
        "  - robotics\n",
        "  - cognition\n",
        "  - embodiment\n",
        "  - philosophy\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dominant paradigm in AI treats intelligence as abstract computation: take in information, process it through learned functions, produce outputs. Bodies are incidental\u2014useful for gathering data, maybe, but not constitutive of intelligence itself. This post argues that this framing misses something important, and that understanding why could reshape how we build and evaluate AI systems.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Sensorimotor Grounding Hypothesis\n",
        "\n",
        "The core claim of embodied cognition is that meaning is grounded in action. When you understand the word \"grasp,\" you don't just retrieve a dictionary definition\u2014you activate the same sensorimotor circuits involved in actually grasping. Concepts aren't abstract symbols floating in a void; they're patterns of potential interaction with the world.\n",
        "\n",
        "This isn't mysticism. It's an empirical claim with testable predictions. If meaning is grounded in action, then:\n",
        "\n",
        "- Processing action words should activate motor cortex (it does)\n",
        "- Interfering with motor systems should slow language comprehension (it does)\n",
        "- Abstract concepts should be structured through bodily metaphors (they are: we \"grasp\" ideas, \"weigh\" options, \"move forward\" with plans)\n",
        "\n",
        "The implications for AI are immediate. If human intelligence is fundamentally structured by embodiment, then systems trained only on text might develop a qualitatively different kind of \"understanding\"\u2014one that correlates with human concepts but doesn't share their grounding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Moravec's Paradox Revisited\n",
        "\n",
        "In 1988, Hans Moravec observed something puzzling: tasks that seem hard for humans (chess, calculus) are easy for computers, while tasks that seem trivial (walking, recognizing objects, catching a ball) are extraordinarily difficult to automate.\n",
        "\n",
        "The standard explanation is that evolution had billions of years to optimize sensorimotor control, while abstract reasoning is a recent and still-imperfect add-on. But there's a deeper point: sensorimotor control is computationally harder than it looks because the real world is high-dimensional, noisy, and unforgiving.\n",
        "\n",
        "Consider walking. You're controlling ~600 muscles through a system with feedback delays of 50-200ms, balancing a top-heavy mass on two small contact points, while dealing with uneven terrain, unexpected obstacles, and perturbations. The state space is enormous. The dynamics are nonlinear. And failure (falling) has immediate, painful consequences.\n",
        "\n",
        "Chess, by contrast, is turn-based, fully observable, discrete, deterministic, and consequence-free. The only reason it seemed hard to us is that our evolved hardware isn't optimized for tree search. Once we build hardware that is, the problem evaporates.\n",
        "\n",
        "Moravec's paradox suggests that embodied intelligence isn't just one variety of intelligence\u2014it's the hard case. Abstract reasoning might be a simplified special case that happens to be useful in certain domains.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Active Inference and Predictive Processing\n",
        "\n",
        "One of the most compelling frameworks for embodied cognition comes from predictive processing. The idea: brains are fundamentally prediction machines. Perception isn't passive reception of sensory data\u2014it's active inference, where the brain generates predictions and uses sensory input to correct them.\n",
        "\n",
        "In this view, action and perception are two sides of the same coin:\n",
        "\n",
        "- **Perception** updates your model of the world to match sensory input\n",
        "- **Action** changes the world to match your predictions\n",
        "\n",
        "Both serve the same goal: minimizing prediction error (or \"free energy\" in the technical literature).\n",
        "\n",
        "This dissolves the traditional boundary between sensing and acting. An organism that just sits and models the world will accumulate prediction error. To minimize error, you must act\u2014not just to gather information, but to bring the world into alignment with your expectations. \"I predict I will be holding a coffee cup\" becomes true when you pick one up.\n",
        "\n",
        "Karl Friston's formalization of this idea has become influential in neuroscience and is starting to influence robotics and AI. The core insight for our purposes: you can't separate the \"intelligence\" part from the \"body\" part. The whole system is optimizing prediction, and action is essential to that optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Brooks and Intelligence Without Representation\n",
        "\n",
        "Rodney Brooks' subsumption architecture from the 1980s was a direct attack on classical AI. Classical AI builds explicit representations of the world, reasons over them, and then acts. Brooks argued this was backwards: you should build layers of sensorimotor competence that directly couple perception to action, with higher layers modulating lower ones rather than replacing them.\n",
        "\n",
        "His robots didn't have world models in the traditional sense. They had reactive behaviors that, when combined, produced surprisingly competent navigation and exploration. \"The world is its own best model,\" Brooks argued\u2014why build an expensive internal representation when you can query the real thing for free?\n",
        "\n",
        "The subsumption architecture fell out of favor as machine learning enabled powerful learned representations. But Brooks' core critique remains relevant:\n",
        "\n",
        "1. **Representations are expensive** in energy, time, and complexity\n",
        "2. **The world provides information for free** if you're set up to exploit it\n",
        "3. **Tight coupling between perception and action** can produce robust behavior without elaborate reasoning\n",
        "\n",
        "Modern robotics has partially rediscovered these ideas through end-to-end learning, where policies map directly from sensors to actions without explicit intermediate representations. The representation is there, but it's implicit and learned, not hand-designed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What Might LLMs Be Missing?\n",
        "\n",
        "If embodied cognition is right, what does that mean for language models trained only on text?\n",
        "\n",
        "The strong claim would be: LLMs can never truly understand language because they lack sensorimotor grounding. They manipulate symbols that refer to concepts they've never experienced. This is essentially Searle's Chinese Room argument updated for modern AI.\n",
        "\n",
        "The weak claim: LLMs develop a different kind of understanding\u2014one that captures statistical regularities in how words are used, which correlates strongly with grounded meaning but isn't identical to it. This might explain certain systematic failures (spatial reasoning, physical intuition) while allowing for genuine competence in many domains.\n",
        "\n",
        "There's also a deflationary view: maybe grounding doesn't matter as much as philosophers think. If statistical patterns in text capture enough of the structure of grounded concepts, then text-only training might be sufficient for most purposes. The correlation between \"how words are used\" and \"what words mean\" is so strong that you can get meaning for free by modeling usage.\n",
        "\n",
        "I don't think this debate is settled. But noticing it changes how you evaluate AI systems. \"Does it produce correct outputs?\" is different from \"Does it understand in the way we do?\" Both questions matter, depending on what you're building.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Enactivist Extension\n",
        "\n",
        "Enactivism pushes embodied cognition further: cognition isn't just grounded in the body\u2014it's constituted by the entire organism-environment system. Intelligence doesn't reside \"in the head\"; it emerges from the dynamic coupling between brain, body, and world.\n",
        "\n",
        "This sounds abstract, but it has concrete implications:\n",
        "\n",
        "- **Offloading**: Experts routinely offload cognitive work to external structures (notes, tools, social systems). The intelligence is in the coupled system, not just the brain.\n",
        "- **Stigmergy**: Social insects coordinate through environmental modifications (pheromone trails). The \"algorithm\" is distributed across agents and environment.\n",
        "- **Affordances**: Perception is already structured for action\u2014we see a chair as \"sittable,\" a handle as \"graspable.\" The world shows up as opportunities for interaction, not raw sensory data.\n",
        "\n",
        "If enactivism is right, then building embodied AI isn't just about putting a computer in a robot body. It's about designing the entire system\u2014agent, body, and environment\u2014so that intelligence can emerge from their interaction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Implications\n",
        "\n",
        "What does this mean for someone building AI systems?\n",
        "\n",
        "1. **Don't assume language is enough.** Text captures a lot, but systematic gaps in physical reasoning might require different training data or architectures.\n",
        "\n",
        "2. **Simulation has limits.** If real-world interaction shapes cognition in deep ways, then sim-to-real gaps might be more fundamental than we assume.\n",
        "\n",
        "3. **Multimodal training matters.** Grounding language in vision, action, and interaction might not just add capabilities\u2014it might change the nature of what's learned.\n",
        "\n",
        "4. **Evaluate for the right thing.** If you need grounded understanding, test for it. If you need statistical text competence, test for that. They might come apart.\n",
        "\n",
        "5. **Bodies aren't optional for some applications.** Household robots, surgical assistants, autonomous vehicles\u2014these require genuine sensorimotor intelligence, not just language competence.\n",
        "\n",
        "The bigger point: intelligence is not a single thing. There are many kinds, optimized for different niches. Embodied intelligence is one important kind that we're only beginning to understand how to build.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "```{=html}\n",
        "<div style=\"text-align:center;\">\n",
        "  <img src=\"image.png\" alt=\"Figure\" width=\"65%\"/>\n",
        "  <p><em>Figure 1. Closed-loop sensorimotor integration vs. open-loop processing</em></p>\n",
        "</div>\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}