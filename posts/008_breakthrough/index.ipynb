{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"Breakthrough #4 — Mentalizing: Second-Order Models, Theory of Mind, and Primate Intelligence\"\n",
        "description: \"My detailed AI-focused notes on Bennett’s ‘mentalizing’ breakthrough: how primates model their own minds and reuse it to infer others’ intent, enabling ToM and imitation.\"\n",
        "author: \"Parsa Idehpour\"\n",
        "date: \"2025-12-16\"\n",
        "categories:\n",
        "  - neuroscience\n",
        "  - theory-of-mind\n",
        "  - multi-agent\n",
        "  - cognition\n",
        "---\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mentalizing is Bennett’s attempt to compress a lot of primate uniqueness into a single computational upgrade. In his telling, primates don’t just simulate the external world; they simulate minds. They infer hidden variables—beliefs, knowledge, goals, intent—that generate observable behavior.\n",
        "\n",
        "That sounds like philosophy until you notice how algorithmic it is. If other agents exist, then the environment is no longer governed only by physics. It’s governed by policies. The biggest uncertainty in your world becomes “what will they do?” And the best predictors of what they will do are not visible objects; they are invisible internal states.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bennett’s strongest unification move is to connect theory of mind, imitation, and “future needs” under one umbrella.\n",
        "\n",
        "- **Theory of mind**: predicting others by inferring what they know and what they want.\n",
        "- **Imitation**: copying competence by inferring the underlying goal, not the surface motor pattern.\n",
        "- **Future needs**: planning for a future self whose drives differ from the present self.\n",
        "\n",
        "All three require the same thing: a model in which behavior is explained by latent mental causes. Once you have that model for yourself, it becomes reusable for others. That reuse story is the kind of thing evolution loves: build one abstraction, get multiple capabilities for “free.”\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imitation is where this gets concrete. Behavioral cloning is brittle: copying the exact moves only works when bodies, constraints, and contexts match. Skilled imitation requires intent inference. You watch someone do a task, you infer what objective they are achieving and what constraints they respect, and then you re-plan under your own dynamics. That is mentalizing as an algorithm.\n",
        "\n",
        "The “future needs” angle is similarly concrete once you think about sparse environments. If you only seek water when thirsty, you die in deserts. To carry water now for thirst later, you must represent a future self whose preferences differ from your present self. Bennett is effectively saying that primates can treat their future selves like other agents: they can reason about a self they are not currently experiencing.\n",
        "\n",
        "He also notes the characteristic bias this introduces: we project the present into the future and into other people. If mentalizing is inference with priors, projection is exactly what you should expect when your own state is a strong prior and compute is limited.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From an ML perspective, mentalizing is adjacent to inverse planning and inverse reinforcement learning: infer what objective a policy is optimizing from trajectories. It’s also adjacent to opponent modeling: you build an internal model of the other agent’s policy class, then condition on inferred intent.\n",
        "\n",
        "The interesting research question Bennett’s story raises is: what is the training signal? Inverse problems are ill-posed. How does a brain learn the latent variables that make behavior predictable? Social feedback is one candidate (you get punished when you mispredict people). Self-consistency is another (a self-model that doesn’t help you control yourself gets selected against). Culture and language likely turbocharge it later, but Bennett wants a primate-level version that predates full language.\n",
        "\n",
        "Whether or not you buy every anatomical mapping, the computational claim is strong: the world becomes much more predictable once you model agents as optimizing hidden objectives, and primates look like the lineage that doubled down on that move.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{=html}\n",
        "<div style=\"text-align:center;\">\n",
        "  <img src=\"mentalizing_gPFC.png\" alt=\"Self-model layer\" width=\"65%\"/>\n",
        "  <p><em>Figure 1. Mentalizing as a self-model layer that conditions simulation on hidden mental variables</em></p>\n",
        "</div>\n",
        "```\n",
        "\n",
        "```{=html}\n",
        "<div style=\"text-align:center;\">\n",
        "  <img src=\"mentalizing_latent_inference.png\" alt=\"Inferring hidden state\" width=\"65%\"/>\n",
        "  <p><em>Figure 2. Predicting behavior by inferring hidden goals, knowledge, and intent</em></p>\n",
        "</div>\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}