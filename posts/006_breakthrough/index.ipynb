{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"Breakthrough #2 — Reinforcing: Dopamine, Basal Ganglia, and the Birth of RL\"\n",
        "description: \"My AI-focused notes on Bennett’s ‘reinforcing’ breakthrough: temporal-difference learning as a biological design pattern.\"\n",
        "author: \"Parsa Idehpour\"\n",
        "date: \"2025-12-16\"\n",
        "categories:\n",
        "  - neuroscience\n",
        "  - reinforcement-learning\n",
        "  - dopamine\n",
        "  - evolution\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bennett’s second breakthrough—reinforcing—is where the story becomes recognizably “learning from reward.” Steering can produce competent behavior, but it cannot systematically improve itself when the environment changes. Reinforcing adds a mechanism that tunes action tendencies based on outcomes, and Bennett anchors this in vertebrate circuitry: basal ganglia plus dopamine.\n",
        "\n",
        "The part that matters is not the anatomical labels; it’s the design pattern. You have an action-selection system that decides what gets executed, and you have a scalar teaching signal that nudges that system so it repeats actions with better long-run consequences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bennett emphasizes that the teaching signal is not simply “reward happened.” It’s closer to “that was better or worse than expected.” That small conceptual twist is what makes reinforcement scale to real life, where outcomes are delayed and ambiguous.\n",
        "\n",
        "If your learning signal only arrives at the endpoint, you can’t assign credit to the earlier choices that made the endpoint possible. A prediction-based teaching signal solves this by moving learning earlier in time: cues become meaningful because they predict outcomes, and the learning signal lights up at those cues rather than only at the reward itself. When an expected reward fails to arrive, the teaching signal goes negative. This gives you a bidirectional correction mechanism that shapes behavior efficiently.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is also why Bennett pushes back on the “dopamine = pleasure” caricature. Pleasure is an experience; learning is an algorithm. A teaching signal must carry information about mismatch between expectation and reality. When you interpret the dopamine story through that lens, a lot of behavioral phenomena stop being mysterious.\n",
        "\n",
        "Anticipation is what it looks like when predictors of reward become valuable. Disappointment is what it looks like when expectation is violated. Motivation becomes the behavioral manifestation of predicted value biasing action selection before the reward exists. Bennett is implicitly saying: many affective states are not add-ons; they are control-relevant summaries of prediction and uncertainty.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A concept I think Bennett gets right is the separation between **what counts as reward** and **how behavior is shaped by reward**. Biological reward is anchored to survival: hunger, thirst, pain, thermoregulation, social safety. Reinforcement is the learning mechanism that figures out what to do in order to satisfy those needs in a particular environment.\n",
        "\n",
        "That separation is important for two reasons. First, it explains flexibility: the organism can learn novel strategies without evolution rewriting its physiology every generation. Second, it highlights the danger of misalignment in any system with powerful optimization: if the reward signal is corrupted, the learning machinery will still optimize it, often in pathological ways. Bennett’s later discussion of superintelligence rhymes with this: the bigger the optimizer, the more central the objective becomes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reinforcement creates habits. Habits are a feature, not a bug: they’re fast, robust, and cheap. But Bennett treats habit learning as having an inherent ceiling. It’s slow to acquire when outcomes are rare. It becomes rigid in distribution shift. It struggles when you need to evaluate novel actions you’ve never tried.\n",
        "\n",
        "That limitation sets up the next breakthrough. Once acting becomes expensive, evolution has an incentive to build a system that can learn without acting—by imagining. Reinforcing makes the agent trainable; simulating makes it sample efficient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\\n",
        "```{=html}\n",
        "<div style=\"text-align:center;\">\n",
        "  <img src=\"image.png\" alt=\"Action gating loop\" width=\"65%\"/>\n",
        "  <p><em>Figure 1. Action gating trained by scalar feedback (actor–critic motif as an intuition)</em></p>\n",
        "</div>\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
