{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Tokenization Laws and DNA Models\"\n",
        "description: \"Understanding k-mer tokenization and EVO-2 style nucleotide modeling\"\n",
        "author: \"Parsa Idehpour\"\n",
        "date: \"2025-09-14\"\n",
        "categories:\n",
        "  - LLMs\n",
        "  - biology\n",
        "  - tokenization\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "This post explores how tokenization choices shape model capacity and performance, and how those ideas translate to DNA models. We'll use k-mer tokenization on nucleotide sequences and discuss how EVO-2-style models work with nucleotide tokens.\n",
        "\n",
        "- What are tokenization laws?\n",
        "- How do k-mer vocabularies trade off context length vs. vocabulary size?\n",
        "- How might EVO-2-like models represent nucleotides and long-range dependencies?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "```{=html}\n",
        "<div style=\"text-align:center;\">\n",
        "  <img src=\"Evo2_banner.png\" alt=\"EVO-2 model diagram\" width=\"60%\"/>\n",
        "  <p><em>Figure 1. EVO-2 model.</em></p>\n",
        "</div>\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization laws in brief\n",
        "\n",
        "Tokenization laws describe empirical tradeoffs between model size, context length, and tokenizer vocabulary. For fixed compute, larger vocabularies shrink sequence length but increase embedding/softmax cost; smaller vocabularies do the opposite. The optimal point depends on data distribution and task (e.g., code vs. natural language vs. DNA).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DNA modeling with k-mers\n",
        "\n",
        "For DNA, a common tokenizer uses k-mers over the alphabet {A,C,G,T}. The vocabulary size is 4^k, and the stride determines overlap. Larger k compresses sequences but grows the vocab; smaller k expands sequences but keeps vocab small.\n",
        "\n",
        "- Example: k=3 (3-mers) ⇒ vocab size = 64\n",
        "- Example: k=6 (6-mers) ⇒ vocab size = 4096\n",
        "\n",
        "We can quickly demonstrate 3-mer tokenization with stride 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple 3-mer tokenizer demo\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "\n",
        "\n",
        "def kmers(sequence: str, k: int = 3, stride: int = 1) -> List[str]:\n",
        "    sequence = sequence.upper().replace(\"U\", \"T\")\n",
        "    tokens = []\n",
        "    for i in range(0, len(sequence) - k + 1, stride):\n",
        "        kmer = sequence[i:i+k]\n",
        "        if set(kmer) <= {\"A\", \"C\", \"G\", \"T\"}:\n",
        "            tokens.append(kmer)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "seq = \"ACGTACGTGACCT\"\n",
        "ks = kmers(seq, k=3, stride=1)\n",
        "print(\"Sequence:\", seq)\n",
        "print(\"3-mers:\", ks)\n",
        "print(\"Unique 3-mers:\", sorted(set(ks)))\n",
        "print(\"Counts:\", Counter(ks))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How EVO-2-style models use nucleotide tokens\n",
        "\n",
        "High-level idea:\n",
        "- Use a tokenizer over nucleotides (e.g., 3–6-mer tokens) to convert sequences into discrete tokens.\n",
        "- Train an autoregressive transformer over these tokens to model genomic sequences.\n",
        "- Incorporate long-range context (e.g., thousands to millions of bases) using efficient attention or memory mechanisms.\n",
        "- Optionally multitask with masked objectives or structure-aware heads.\n",
        "\n",
        "This lets the model learn motifs, regulatory patterns, and long-range interactions directly from token sequences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
