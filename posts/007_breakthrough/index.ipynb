{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"Breakthrough #3 — Simulating: Neocortex, World Models, and Learning by Imagining\"\n",
        "description: \"My detailed AI-focused notes on Bennett’s ‘simulating’ breakthrough: model-based RL, the search problem, episodic replay, and counterfactual credit assignment.\"\n",
        "author: \"Parsa Idehpour\"\n",
        "date: \"2025-12-16\"\n",
        "categories:\n",
        "  - neuroscience\n",
        "  - world-models\n",
        "  - planning\n",
        "  - model-based-RL\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simulating is the chapter where Bennett’s staircase starts to look like a blueprint for building stronger agents. Reinforcement learning alone can sculpt behavior, but it is expensive: the agent must experience consequences, and mistakes can kill you. Simulation changes the bargain. You can evaluate possible actions internally before you commit to them in the world.\n",
        "\n",
        "Bennett’s biological anchor here is neocortex (and, in his telling, the rise of mammals as a lineage that could trade metabolic cost for richer internal computation). The key claim is functional: brains evolve a generative modeling substrate that can run offline, producing coherent “what might happen” scenarios that guide choice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I find it helpful to distinguish three layers in Bennett’s simulating story:\n",
        "\n",
        "1) **Generative prediction.** The brain can generate plausible next states, not just react to current stimuli.\n",
        "\n",
        "2) **Counterfactual variation.** The brain can generate alternatives: “what if I did something else?”\n",
        "\n",
        "3) **Decision influence.** The brain uses those generated futures to bias present action selection.\n",
        "\n",
        "It’s that third step that turns prediction into planning. Prediction alone can exist without agency. Planning is prediction deployed as a control tool.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bennett is unusually explicit about the hard part: not modeling, but **search**. If the brain can imagine many futures, it must decide which ones to spend compute on. It must choose whether to go deep on one scenario or sample many shallow ones. It must decide when to stop thinking and act. This is not a poetic flourish; it’s the planning bottleneck that shows up in every serious AI planning system.\n",
        "\n",
        "The value of Bennett’s framing is that it forces you to treat “thinking” as compute allocation. An agent that thinks well is an agent that uses a limited budget of imagination in the places where it matters most. That is why the rise of frontal cortex-like structures is conceptually tied to simulation: you need a controller for the simulator itself.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "His treatment of episodic memory fits nicely into this. If simulation is a generative process, then memory is not just storage; it is the ability to re-render a specific past. Replay then becomes a way of training the generative model and the policy using experiences that are rare or important. Bennett’s two-speed story—fast episodic capture plus slow structural learning—maps onto a very common engineering problem: you want a system that can both remember exceptions and still generalize.\n",
        "\n",
        "Counterfactuals are the real accelerator. If you can replay an episode and evaluate unchosen alternatives, you squeeze more learning signal out of the same experience. In practical terms, you reduce the number of costly real-world trials required to become competent. This is why simulating feels like a qualitative jump: it’s not just that the agent is smarter; it’s that the agent can learn in a safer, cheaper regime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simulation also introduces new failure modes, and Bennett’s staircase implicitly depends on recognizing them. A model-free learner is slow and rigid. A simulator can be fast and imaginative—and wrong. It can hallucinate. It can over-plan using inaccurate dynamics. It can get trapped in internally coherent but externally false stories.\n",
        "\n",
        "That matters because it motivates why later breakthroughs focus on grounding: mentalizing introduces constraints from other minds (social correction), and language turns simulation into something that can be checked and aligned across people. In other words, the staircase is not “more compute forever.” It’s “more compute plus new ways to keep it pointed at reality.”\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{=html}\n",
        "<div style=\"text-align:center;\">\n",
        "  <img src=\"image.png\" alt=\"Vicarious trial-and-error\" width=\"65%\"/>\n",
        "  <p><em>Figure 1. Vicarious trial-and-error: pausing to evaluate options before acting</em></p>\n",
        "</div>\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
