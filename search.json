[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/002_information_theory_transformers/index.html",
    "href": "posts/002_information_theory_transformers/index.html",
    "title": "Information-Theoretic Bounds and Training Dynamics of Transformers",
    "section": "",
    "text": "This post sketches how training a transformer can be framed in information-theoretic terms:\n\nTraining minimizes empirical cross-entropy, i.e., expected negative log-likelihood.\nThe optimal achievable loss is the entropy rate of the data (given the model class).\nBits-per-token (bpt) is simply loss in base-2 units.\nGeneralization can be discussed via compression and mutual information.\n\n\n  \n  Figure 1. Transformers"
  },
  {
    "objectID": "posts/002_information_theory_transformers/index.html#overview",
    "href": "posts/002_information_theory_transformers/index.html#overview",
    "title": "Information-Theoretic Bounds and Training Dynamics of Transformers",
    "section": "",
    "text": "This post sketches how training a transformer can be framed in information-theoretic terms:\n\nTraining minimizes empirical cross-entropy, i.e., expected negative log-likelihood.\nThe optimal achievable loss is the entropy rate of the data (given the model class).\nBits-per-token (bpt) is simply loss in base-2 units.\nGeneralization can be discussed via compression and mutual information.\n\n\n  \n  Figure 1. Transformers"
  },
  {
    "objectID": "posts/002_information_theory_transformers/index.html#cross-entropy-entropy-rate-and-bits-per-token",
    "href": "posts/002_information_theory_transformers/index.html#cross-entropy-entropy-rate-and-bits-per-token",
    "title": "Information-Theoretic Bounds and Training Dynamics of Transformers",
    "section": "Cross-entropy, entropy rate, and bits-per-token",
    "text": "Cross-entropy, entropy rate, and bits-per-token\n\nCross-entropy (training loss) estimates how many nats per token the model spends to encode the data.\nBits-per-token (bpt) is just loss in base-2: bpt = loss_nat / ln 2.\nThe theoretical lower bound is the entropy rate H of the data-generating process; if the model class is misspecified, the optimum is H + D(P||Q*).\n\n\n# Convert loss (nats/token) to bits-per-token and perplexity\nimport math\nfrom typing import Iterable, Tuple\n\n\ndef loss_to_metrics(loss_nats: float) -&gt; Tuple[float, float]:\n    \"\"\"Return (bits_per_token, perplexity) from loss in nats/token.\"\"\"\n    bits_per_token = loss_nats / math.log(2)\n    perplexity = math.exp(loss_nats)\n    return bits_per_token, perplexity\n\n\nexample_losses = [3.5, 2.8, 2.2, 1.9]\nfor step, loss in enumerate(example_losses, start=1):\n    bpt, ppl = loss_to_metrics(loss)\n    print(f\"step={step:02d} loss={loss:.3f} nats -&gt; bpt={bpt:.3f}, ppl={ppl:.2f}\")"
  },
  {
    "objectID": "posts/002_information_theory_transformers/index.html#training-dynamics-as-compression",
    "href": "posts/002_information_theory_transformers/index.html#training-dynamics-as-compression",
    "title": "Information-Theoretic Bounds and Training Dynamics of Transformers",
    "section": "Training dynamics as compression",
    "text": "Training dynamics as compression\n\nMinimizing cross-entropy is equivalent to minimizing expected code length.\nEarly training reduces redundant predictability (frequent patterns); later, model learns rarer structure.\nCapacity vs.¬†data curve: more parameters lower achievable cross-entropy until compute/data bottlenecks.\nGeneralization: MDL view‚Äîgood models compress both train and test data with similar code lengths."
  },
  {
    "objectID": "posts/004_tokenizers/index.html",
    "href": "posts/004_tokenizers/index.html",
    "title": "Tokenization Exploration",
    "section": "",
    "text": "I explored tokenization with two models (GPT‚Äë5 and Sonnet 4.5) and reflected on what made the prompts effective. The discussion moved from DFT basics to whether a Fourier transform could serve as a tokenizer, then broadened into ways to improve on BPE and how to tailor a tokenizer to a specific corpus. A direct URL to my feedback will be added when it‚Äôs posted on the issues page."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#context",
    "href": "posts/004_tokenizers/index.html#context",
    "title": "Tokenization Exploration",
    "section": "",
    "text": "I explored tokenization with two models (GPT‚Äë5 and Sonnet 4.5) and reflected on what made the prompts effective. The discussion moved from DFT basics to whether a Fourier transform could serve as a tokenizer, then broadened into ways to improve on BPE and how to tailor a tokenizer to a specific corpus. A direct URL to my feedback will be added when it‚Äôs posted on the issues page."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#what-the-interactions-showed",
    "href": "posts/004_tokenizers/index.html#what-the-interactions-showed",
    "title": "Tokenization Exploration",
    "section": "What the interactions showed",
    "text": "What the interactions showed\nBoth models handled the DFT prompt comfortably, giving standard forward/inverse formulas and practical notes. When I asked if a DFT could be used as a tokenizer, the answers pushed back: a plain Fourier representation is lossy, fixed‚Äëlength, and not aligned to discrete linguistic structure; it‚Äôs better used inside the model or as auxiliary features. On ‚Äúbetter than BPE,‚Äù the conversation shifted to learned tokenizers (e.g., VQ with byte fallback), morphology‚Äëaware segmentation, adaptive/MDL‚Äëmotivated tokenization, and semantic or graph‚Äëbased grouping‚Äîapproaches that target predictability rather than just compression."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#models-and-outputs",
    "href": "posts/004_tokenizers/index.html#models-and-outputs",
    "title": "Tokenization Exploration",
    "section": "Models and outputs",
    "text": "Models and outputs\nI used two different model families (GPT‚Äë5 and Sonnet 4.5) and iterated across related prompts rather than stopping at one answer. That back‚Äëand‚Äëforth surfaced consensus on basics, clearer reasoning about feasibility, and a pathway from critique to concrete designs."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#conversation-excerpts",
    "href": "posts/004_tokenizers/index.html#conversation-excerpts",
    "title": "Tokenization Exploration",
    "section": "Conversation excerpts",
    "text": "Conversation excerpts\nI began with a simple check: ‚Äúdo you know discrete Fourier transform?‚Äù The reply included the standard definition ‚Äî ‚ÄúX[k] = Œ£ x[n] e^(‚àíj2œÄkn/N), with an inverse x[n] = (1/N) Œ£ X[k] e^(+j2œÄkn/N)‚Äù ‚Äî and reminders about linearity, modulation, and the FFT. From there I asked: ‚Äúimagine we want to use this as tokenizer to LLMs‚Ä¶ is that possible?‚Äù The answer argued a plain DFT is a poor tokenizer because it‚Äôs lossy, tied to fixed lengths, and not aligned to discrete symbols; it suggested using Fourier features inside models instead. Finally, I asked for alternatives ‚Äúbetter than BPE‚Äù and a tokenizer that can be tailored to data; the response proposed learned VQ schemes with a byte‚Äëfallback for exact reversibility, morphology‚Äëaware segmentation, adaptive tokenization optimized for predictability, and semantic/graph‚Äëbased grouping."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#what-worked-in-the-prompting",
    "href": "posts/004_tokenizers/index.html#what-worked-in-the-prompting",
    "title": "Tokenization Exploration",
    "section": "What worked in the prompting",
    "text": "What worked in the prompting\nUsing two different models and pushing beyond the first answer helped. Starting with definitions created a shared foundation; then asking ‚Äúis this actually usable as a tokenizer?‚Äù forced a feasibility check and concrete trade‚Äëoffs. Requesting ‚Äúnovel ideas‚Äù and a ‚Äútailorable tokenizer‚Äù nudged the models to move from critique to constructive designs (e.g., VQ with byte‚Äëfallback) rather than staying abstract. Framing the problem in terms of reversibility and predictability kept the discussion grounded in properties that matter for real systems."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#what-i-expected-vs-what-surprised-me",
    "href": "posts/004_tokenizers/index.html#what-i-expected-vs-what-surprised-me",
    "title": "Tokenization Exploration",
    "section": "What I expected vs what surprised me",
    "text": "What I expected vs what surprised me\nI expected the definitions to be solid and the DFT idea to be challenged on reversibility and fit to discrete text. What surprised me was how quickly the conversation moved from critique to a concrete, workable direction: a learned vector‚Äëquantized tokenizer with exact byte fallback, paired with a predictability‚Äëoriented objective. That made the idea immediately testable rather than purely conceptual."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#how-id-prompt-next-time",
    "href": "posts/004_tokenizers/index.html#how-id-prompt-next-time",
    "title": "Tokenization Exploration",
    "section": "How I‚Äôd prompt next time",
    "text": "How I‚Äôd prompt next time\nNext time I‚Äôd ask for two contrasting designs and when each is preferable, require a couple of citations, and force the model to spell out likely failure modes with mitigations. I‚Äôd also request a tiny code sketch and a toy evaluation plan so the proposal is actionable. Finally, I‚Äôd have one model critique the other‚Äôs answer and estimate simple trade‚Äëoffs like token savings versus predicted perplexity change."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#concrete-outputs-sampled-snippets",
    "href": "posts/004_tokenizers/index.html#concrete-outputs-sampled-snippets",
    "title": "Tokenization Exploration",
    "section": "Concrete outputs sampled (snippets)",
    "text": "Concrete outputs sampled (snippets)\n\nDFT feasibility as tokenizer: ‚ÄúUsing a plain DFT as the tokenizer for text LLMs isn‚Äôt practical‚Ä¶ it‚Äôs lossy, length‚Äërigid, and not semantically aligned. Use Fourier ideas inside the model or as features instead.‚Äù\n\n\nNovel tokenization ideas: ‚ÄúLearned VQ tokenizers, morphology‚Äëaware hybrids, adaptive/MDL tokenization, semantic/graph clustering, multi‚Äëresolution tokens.‚Äù\n\n\nData‚Äëtailorable tokenizer: ‚ÄúVQ‚ÄëText with byte fallback: learned codes for common spans; exact byte runs for everything else; train with reconstruction + commitment + predictability (MDL‚Äëstyle).‚Äù"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Tokenization Exploration\n\n\n\nprompting\n\nevaluation\n\nLLMs\n\n\n\nI explored tokenization with LLMs and evaluated prompting effectiveness\n\n\n\n\n\nNov 3, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nTokenization Laws and DNA Models\n\n\n\nLLMs\n\nbiology\n\ntokenization\n\n\n\nUnderstanding k-mer tokenization and EVO-2 style nucleotide modeling\n\n\n\n\n\nSep 14, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nInformation-Theoretic Bounds and Training Dynamics of Transformers\n\n\n\ntheory\n\ntransformers\n\ninformation-theory\n\n\n\nExplaining transformer training through entropy, cross-entropy, and bits-per-token\n\n\n\n\n\nSep 14, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nCUDA Inference Kernels and Serving Stacks\n\n\n\nsystems\n\ninference\n\ncuda\n\n\n\nKey kernels, memory layouts, and serving choices (with vLLM notes)\n\n\n\n\n\nSep 14, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/001_tokenization_dna_models/index.html",
    "href": "posts/001_tokenization_dna_models/index.html",
    "title": "Tokenization Laws and DNA Models",
    "section": "",
    "text": "This post explores how tokenization choices shape model capacity and performance, and how those ideas translate to DNA models. We‚Äôll use k-mer tokenization on nucleotide sequences and discuss how EVO-2-style models work with nucleotide tokens.\n\nWhat are tokenization laws?\nHow do k-mer vocabularies trade off context length vs.¬†vocabulary size?\nHow might EVO-2-like models represent nucleotides and long-range dependencies?\n\n\n  \n  Figure 1. EVO-2 model."
  },
  {
    "objectID": "posts/001_tokenization_dna_models/index.html#overview",
    "href": "posts/001_tokenization_dna_models/index.html#overview",
    "title": "Tokenization Laws and DNA Models",
    "section": "",
    "text": "This post explores how tokenization choices shape model capacity and performance, and how those ideas translate to DNA models. We‚Äôll use k-mer tokenization on nucleotide sequences and discuss how EVO-2-style models work with nucleotide tokens.\n\nWhat are tokenization laws?\nHow do k-mer vocabularies trade off context length vs.¬†vocabulary size?\nHow might EVO-2-like models represent nucleotides and long-range dependencies?\n\n\n  \n  Figure 1. EVO-2 model."
  },
  {
    "objectID": "posts/001_tokenization_dna_models/index.html#tokenization-laws-in-brief",
    "href": "posts/001_tokenization_dna_models/index.html#tokenization-laws-in-brief",
    "title": "Tokenization Laws and DNA Models",
    "section": "Tokenization laws in brief",
    "text": "Tokenization laws in brief\nTokenization laws describe empirical tradeoffs between model size, context length, and tokenizer vocabulary. For fixed compute, larger vocabularies shrink sequence length but increase embedding/softmax cost; smaller vocabularies do the opposite. The optimal point depends on data distribution and task (e.g., code vs.¬†natural language vs.¬†DNA)."
  },
  {
    "objectID": "posts/001_tokenization_dna_models/index.html#dna-modeling-with-k-mers",
    "href": "posts/001_tokenization_dna_models/index.html#dna-modeling-with-k-mers",
    "title": "Tokenization Laws and DNA Models",
    "section": "DNA modeling with k-mers",
    "text": "DNA modeling with k-mers\nFor DNA, a common tokenizer uses k-mers over the alphabet {A,C,G,T}. The vocabulary size is 4^k, and the stride determines overlap. Larger k compresses sequences but grows the vocab; smaller k expands sequences but keeps vocab small.\n\nExample: k=3 (3-mers) ‚áí vocab size = 64\nExample: k=6 (6-mers) ‚áí vocab size = 4096\n\nWe can quickly demonstrate 3-mer tokenization with stride 1.\n\n# Simple 3-mer tokenizer demo\nfrom collections import Counter\nfrom typing import List\n\n\ndef kmers(sequence: str, k: int = 3, stride: int = 1) -&gt; List[str]:\n    sequence = sequence.upper().replace(\"U\", \"T\")\n    tokens = []\n    for i in range(0, len(sequence) - k + 1, stride):\n        kmer = sequence[i:i+k]\n        if set(kmer) &lt;= {\"A\", \"C\", \"G\", \"T\"}:\n            tokens.append(kmer)\n    return tokens\n\n\nseq = \"ACGTACGTGACCT\"\nks = kmers(seq, k=3, stride=1)\nprint(\"Sequence:\", seq)\nprint(\"3-mers:\", ks)\nprint(\"Unique 3-mers:\", sorted(set(ks)))\nprint(\"Counts:\", Counter(ks))"
  },
  {
    "objectID": "posts/001_tokenization_dna_models/index.html#how-evo-2-style-models-use-nucleotide-tokens",
    "href": "posts/001_tokenization_dna_models/index.html#how-evo-2-style-models-use-nucleotide-tokens",
    "title": "Tokenization Laws and DNA Models",
    "section": "How EVO-2-style models use nucleotide tokens",
    "text": "How EVO-2-style models use nucleotide tokens\nHigh-level idea: - Use a tokenizer over nucleotides (e.g., 3‚Äì6-mer tokens) to convert sequences into discrete tokens. - Train an autoregressive transformer over these tokens to model genomic sequences. - Incorporate long-range context (e.g., thousands to millions of bases) using efficient attention or memory mechanisms. - Optionally multitask with masked objectives or structure-aware heads.\nThis lets the model learn motifs, regulatory patterns, and long-range interactions directly from token sequences."
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html",
    "href": "posts/003_cuda_inference_kernels/index.html",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "",
    "text": "What makes LLM inference fast on GPUs? This post summarizes the core CUDA kernels, memory layouts, and scheduling tricks that dominate latency and throughput.\n\nAttention with KV cache (paged KV, continuous batch, block-sparse)\nGEMM-heavy layers (QKV projections, MLP) and tensor cores\nQuantization (weight-only, KV cache) and dequant overhead\nServing stacks (vLLM, TensorRT-LLM, Triton) and scheduling\n\n\n  \n  Figure 1. vLLM logo"
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#overview",
    "href": "posts/003_cuda_inference_kernels/index.html#overview",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "",
    "text": "What makes LLM inference fast on GPUs? This post summarizes the core CUDA kernels, memory layouts, and scheduling tricks that dominate latency and throughput.\n\nAttention with KV cache (paged KV, continuous batch, block-sparse)\nGEMM-heavy layers (QKV projections, MLP) and tensor cores\nQuantization (weight-only, KV cache) and dequant overhead\nServing stacks (vLLM, TensorRT-LLM, Triton) and scheduling\n\n\n  \n  Figure 1. vLLM logo"
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#kernel-hotspots",
    "href": "posts/003_cuda_inference_kernels/index.html#kernel-hotspots",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "Kernel hotspots",
    "text": "Kernel hotspots\n\nAttention: QK^T, softmax, and AV; fused kernels reduce memory traffic.\nKV cache: paging and swizzling to keep contiguous access; block tables.\nGEMM: QKV, output projection, MLP GEMMs dominate FLOPs; use tensor cores.\nQuant: weight-only (W8A16) vs.¬†activation quant; KV cache quant (e.g., 8-bit) saves VRAM.\n\n\n# Simple throughput estimator\nfrom typing import Optional\n\ndef estimate_throughput(tokens_per_request: int,\n                        requests_per_second: float,\n                        decode_tokens_per_s: float,\n                        prefill_tokens_per_s: Optional[float] = None,\n                        prefill_tokens: int = 0) -&gt; float:\n    \"\"\"\n    Crude throughput estimate in tokens/sec.\n    - tokens_per_request: average generated tokens per request\n    - requests_per_second: steady-state request rate\n    - decode_tokens_per_s: steady-state decode speed (per token) across batch\n    - prefill_tokens_per_s: optional prefill speed; if None, ignored\n    - prefill_tokens: avg prompt tokens per request\n    \"\"\"\n    decode_toks = tokens_per_request * requests_per_second\n    total = decode_toks\n    if prefill_tokens_per_s is not None and prefill_tokens &gt; 0:\n        total += (prefill_tokens * requests_per_second) * (prefill_tokens_per_s / decode_tokens_per_s)\n    return total\n\nprint(estimate_throughput(tokens_per_request=200,\n                          requests_per_second=5.0,\n                          decode_tokens_per_s=200000.0,\n                          prefill_tokens_per_s=1200000.0,\n                          prefill_tokens=1000))"
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#vllm-vs-tensorrt-llm-vs-triton-high-level",
    "href": "posts/003_cuda_inference_kernels/index.html#vllm-vs-tensorrt-llm-vs-triton-high-level",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "vLLM vs TensorRT-LLM vs Triton (high level)",
    "text": "vLLM vs TensorRT-LLM vs Triton (high level)\n\nvLLM: Continuous batching, paged KV cache, flexible Python API; great for dynamic workloads; supports multi-model serving.\nTensorRT-LLM: Highly optimized CUDA/TensorRT kernels, graph capture, INT8/FP8 pipelines; excels on NVIDIA stacks for max perf.\nTriton (NVIDIA inference server): Orchestrates models/runtimes (can host vLLM or TensorRT-LLM backends), handles deployment/HTTP/gRPC/metrics.\n\nRule of thumb: For maximum single-GPU perf and static graphs, TensorRT-LLM can edge out; for dynamic batching/multitenancy ease, vLLM is excellent; Triton adds production serving features."
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#kv-cache-sizing-and-paging",
    "href": "posts/003_cuda_inference_kernels/index.html#kv-cache-sizing-and-paging",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "KV cache sizing and paging",
    "text": "KV cache sizing and paging\n\nKV size per token ‚âà 2 * num_layers * num_heads * head_dim * dtype_bytes.\nMultiply by max sequence length and batch to estimate worst-case VRAM.\nPaged KV: allocate in fixed-size blocks (e.g., 16/32/64 tokens) and map logical positions ‚Üí physical pages; reduces fragmentation and enables efficient eviction.\nSwizzling/contiguous layout within blocks keeps memory coalesced for attention reads.\n\n\n# KV cache memory estimator (bytes)\nfrom typing import Literal\n\ndtype_sizes = {\n    \"fp16\": 2,\n    \"bf16\": 2,\n    \"fp32\": 4,\n    \"int8\": 1,\n}\n\ndef kv_cache_bytes(num_layers: int,\n                   num_heads: int,\n                   head_dim: int,\n                   seq_len: int,\n                   batch_size: int,\n                   dtype: Literal[\"fp16\", \"bf16\", \"fp32\", \"int8\"] = \"fp16\") -&gt; int:\n    per_token = 2 * num_layers * num_heads * head_dim * dtype_sizes[dtype]\n    total_tokens = seq_len * batch_size\n    return per_token * total_tokens\n\n# Example: 32 layers, 32 heads, 128 dim, seq 4k, batch 16, fp16\nprint(kv_cache_bytes(32, 32, 128, 4096, 16, \"fp16\") / (1024**3), \"GiB\")"
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#batching-and-scheduling",
    "href": "posts/003_cuda_inference_kernels/index.html#batching-and-scheduling",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "Batching and scheduling",
    "text": "Batching and scheduling\n\nContinuous batching: backfill finished sequences to keep kernels busy; great for variable-length requests.\nShort-first / length-aware: schedule shorter jobs to reduce tail latency; mix for throughput.\nSpec decode: trade compute for fewer steps; helps throughput if memory is ample.\nPinned memory and overlap: overlap H2D/D2H copies with compute using streams."
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#kernel-level-optimization-details",
    "href": "posts/003_cuda_inference_kernels/index.html#kernel-level-optimization-details",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "Kernel-level optimization details",
    "text": "Kernel-level optimization details\n\nFused attention: fuse QK^T, scaling, softmax, and AV to reduce DRAM round-trips.\nFlashAttention-style tiling: block SRAM tiles to keep on-chip; minimize global reads/writes.\nLayouts: use row/col-major consistent with GEMM kernels; prefer tensor core-friendly shapes (multiple of 8/16).\nPersistent kernels: reduce launch overhead; good for steady-state decode.\nQuant & dequant fusion: fuse dequant‚ÜíGEMM to avoid bandwidth blowups."
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#system-level-tips",
    "href": "posts/003_cuda_inference_kernels/index.html#system-level-tips",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "System-level tips",
    "text": "System-level tips\n\nCUDA graphs: capture steady decode loop to reduce CPU overhead.\nStreams: separate prefill, decode, and IO with stream priorities.\nNUMA & pinning: pin host buffers and align worker affinity to avoid cross-socket traffic.\nOverlap: prefetch next batch to GPU while decoding current tokens."
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#references",
    "href": "posts/003_cuda_inference_kernels/index.html#references",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "References",
    "text": "References\n\nvLLM docs and papers (continuous batching, paged KV)\nTensorRT-LLM samples and perf guides\nFlashAttention papers and kernels\nNVIDIA Triton Inference Server docs"
  }
]