{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"Thinking Longer: Test-Time Compute and the Future of Inference Scaling\"\n",
        "description: \"Model quality isn't fixed at training time. With search, verification, and refinement, inference compute can substitute for parameters.\"\n",
        "author: \"Parsa Idehpour\"\n",
        "date: \"2025-12-17\"\n",
        "categories:\n",
        "  - LLMs\n",
        "  - inference\n",
        "  - reasoning\n",
        "  - scaling\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dominant paradigm for improving language models has been scaling training: more parameters, more data, more compute. But there's another dimension: **test-time compute**\u2014spending more resources at inference time to get better answers.\n",
        "\n",
        "Humans do this naturally. Hard problems require more thought. You don't solve a complex proof in one mental step; you work through intermediate results, backtrack when stuck, and verify your reasoning. What happens when we teach AI systems to do the same?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Test-Time Compute Hypothesis\n",
        "\n",
        "The core idea: a small model that \"thinks\" for 10 seconds might outperform a large model that answers in 100 milliseconds. If you're willing to spend compute at inference time, you can trade it for training-time compute.\n",
        "\n",
        "This isn't free. Inference compute costs money and time. But for many applications\u2014complex reasoning, code generation, mathematical proofs\u2014accuracy matters more than latency. And inference compute is more flexible: you can allocate it dynamically based on problem difficulty.\n",
        "\n",
        "The question is how to spend that compute productively. Simply generating more tokens doesn't help if those tokens are noise. You need structured ways to explore, evaluate, and refine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chain-of-Thought Prompting\n",
        "\n",
        "The simplest form of test-time compute: ask the model to show its work.\n",
        "\n",
        "**Chain-of-thought (CoT)** prompting includes examples with step-by-step reasoning, then asks the model to produce similar intermediate steps. This dramatically improves performance on math, logic, and multi-step reasoning problems.\n",
        "\n",
        "Why does it work? Several hypotheses:\n",
        "\n",
        "- **Serialized computation**: Transformers have limited depth. Generating intermediate tokens effectively adds more \"layers\" of computation.\n",
        "- **Error decomposition**: Breaking problems into steps exposes intermediate results that are easier to verify and correct.\n",
        "- **Training distribution**: Models are trained on text that includes reasoning. Prompting them to reason recovers abilities learned during training.\n",
        "\n",
        "**Zero-shot CoT**: Just adding \"Let's think step by step\" to the prompt induces reasoning without examples. This suggests CoT is unlocking something the model already knows how to do.\n",
        "\n",
        "Limitation: CoT is a single forward pass. The model generates one chain and commits to it. It can't explore alternatives or backtrack.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best-of-N and Self-Consistency\n",
        "\n",
        "Generate multiple answers; pick the best one.\n",
        "\n",
        "**Best-of-N sampling**: Generate N independent completions, score them somehow (model confidence, ground truth if available, external verifier), and return the best. Simple but effective. Often 4-8 samples provide most of the gain.\n",
        "\n",
        "**Self-consistency**: For problems with a single correct answer (math, factual questions), generate N reasoning chains and take the majority vote on the final answer. Different reasoning paths might make different intermediate errors but converge on the correct final answer.\n",
        "\n",
        "These methods use test-time compute to reduce variance\u2014sampling multiple times and aggregating. They work because model stochasticity produces diverse solutions, some of which are better than others.\n",
        "\n",
        "Limitation: Linear cost in N. Each sample requires a full forward pass. And if all N chains converge to the same wrong answer, more sampling doesn't help.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tree Search for Language\n",
        "\n",
        "Instead of generating complete sequences and scoring afterward, build a tree of partial sequences and search.\n",
        "\n",
        "**Monte Carlo Tree Search (MCTS)**: The approach that powered AlphaGo. Treat sequence generation as a game tree. At each position, expand promising branches, simulate to completion, and backpropagate value estimates. Balance exploration (trying new branches) and exploitation (deepening good ones).\n",
        "\n",
        "Applied to language:\n",
        "1. Start with prompt\n",
        "2. Generate several possible continuations (tokens or phrases)\n",
        "3. Score each branch with a value estimate\n",
        "4. Expand the most promising branches\n",
        "5. Continue until a complete answer, then backpropagate\n",
        "\n",
        "This enables backtracking: if a reasoning path leads to a dead end, you can return to an earlier branch and try differently.\n",
        "\n",
        "**Process Reward Models (PRMs)**: Train a model to score intermediate reasoning steps, not just final answers. A PRM can identify when reasoning goes wrong before reaching the conclusion, enabling earlier pruning.\n",
        "\n",
        "**Outcome Reward Models (ORMs)**: Score only final answers. Simpler to train (you just need answer labels) but less useful for guiding search.\n",
        "\n",
        "Tree search with PRMs is how recent \"reasoning models\" achieve their performance. The model doesn't just generate one chain; it explores a tree of possibilities, guided by learned value estimates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Critique and Iterative Refinement\n",
        "\n",
        "Generate once, then improve.\n",
        "\n",
        "**Self-critique**: Ask the model to evaluate its own answer. \"What might be wrong with this solution?\" \"Are there any errors in this proof?\" The model often identifies issues it failed to avoid during generation.\n",
        "\n",
        "**Iterative refinement**: Generate \u2192 critique \u2192 revise \u2192 repeat. Each pass can fix errors from the previous one. Analogous to how humans edit their writing.\n",
        "\n",
        "**Constitutional AI-style loops**: Define principles (\"be helpful, harmless, honest\"), generate candidate responses, rank them by the principles, and train on the rankings. The same idea applies at inference: generate, critique against principles, and revise.\n",
        "\n",
        "Why can models catch errors on review that they made during generation? Partly because generation is autoregressive and committing\u2014once tokens are emitted, they influence subsequent generation. Review operates on a complete artifact and can consider global coherence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verifier-Guided Generation\n",
        "\n",
        "Use a separate model to verify outputs, and let that verification guide generation.\n",
        "\n",
        "**Code**: Generate code \u2192 run tests \u2192 if tests fail, generate again with error context. The test suite is an external verifier. This is how many code-completion systems work in practice.\n",
        "\n",
        "**Math**: Generate proof steps \u2192 check with a formal verifier (Lean, Coq) \u2192 if verification fails, backtrack. The theorem prover provides ground truth.\n",
        "\n",
        "**Factual claims**: Generate \u2192 retrieve sources \u2192 verify claims against sources \u2192 revise. Retrieval-augmented generation as verification.\n",
        "\n",
        "External verifiers are powerful because they provide reliable signal. If your code doesn't compile, that's ground truth\u2014no model uncertainty involved. The challenge is that not all tasks have clean external verification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute-Optimal Inference\n",
        "\n",
        "When should you think more? Not every query deserves the same effort.\n",
        "\n",
        "**Adaptive compute**: Estimate problem difficulty and allocate inference compute accordingly. Simple questions get one-shot answers; hard questions get tree search and verification.\n",
        "\n",
        "Difficulty estimation is its own challenge:\n",
        "- Model confidence (entropy over tokens) is one signal but unreliable\n",
        "- Query characteristics (length, complexity, domain) provide hints\n",
        "- Start with cheap computation; escalate if initial results seem uncertain\n",
        "\n",
        "**Cascading**: Try a small, fast model first. If confidence is low, hand off to a larger model. Most queries might be handled cheaply; only hard cases pay the full cost.\n",
        "\n",
        "**Speculative decoding**: Use a small model to draft continuations; verify with the large model. If verification passes, you've generated many tokens cheaply. If not, fall back to the large model. This accelerates easy continuations while maintaining large-model quality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenAI's \"o-series\" and the Reasoning Model Paradigm\n",
        "\n",
        "OpenAI's o1 (and predecessors like \"Strawberry\") represent this paradigm taken seriously. Key characteristics:\n",
        "\n",
        "- **Extended reasoning**: The model generates substantial internal reasoning before answering. This might be hidden from users but consumes inference compute.\n",
        "- **Process supervision**: Trained with reward on intermediate reasoning steps, not just final answers.\n",
        "- **Search**: Likely uses some form of tree search or best-of-N at inference time.\n",
        "- **Specialized for reasoning**: Optimized for math, code, and logic where verification is possible.\n",
        "\n",
        "The result: dramatically better performance on hard reasoning benchmarks (AIME math, competition programming) at the cost of higher latency and expense.\n",
        "\n",
        "This points toward a future where you might choose between:\n",
        "- Fast, cheap models for simple queries\n",
        "- Slow, expensive reasoning models for hard problems\n",
        "\n",
        "Compute becomes a knob you turn based on problem difficulty and quality requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implications and Trade-offs\n",
        "\n",
        "**Latency vs. quality**: More thinking means slower responses. Acceptable for some applications (research, coding, analysis) but not others (chat, real-time decisions).\n",
        "\n",
        "**Cost**: Inference compute isn't free. Tree search with process reward models can cost 10-100\u00d7 single-pass inference. This changes the economics of AI applications.\n",
        "\n",
        "**Training incentives**: If models are deployed with test-time search, training should optimize for search performance, not just single-pass accuracy. This is an active research area.\n",
        "\n",
        "**Transparency**: Hidden reasoning (as in some o1 deployments) trades interpretability for performance. Users may not understand why answers take time or why they cost more.\n",
        "\n",
        "**Diminishing returns**: At some point, no amount of inference compute helps. If the model doesn't have the right knowledge or capability, thinking longer doesn't create it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Bigger Picture\n",
        "\n",
        "Test-time compute represents a shift in how we think about AI capability. Instead of a fixed model with fixed abilities, we have a continuum: spend more compute, get better answers.\n",
        "\n",
        "This is closer to how intelligence works in nature. Humans don't have one-shot answers to hard problems. We think, revise, verify, and iterate. The question is whether current architectures\u2014transformers with autoregressive generation\u2014can fully exploit this paradigm, or whether deeper architectural changes are needed.\n",
        "\n",
        "Either way, the message is clear: model quality at deployment isn't determined only by training. How you use the model matters too.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "```{=html}\n",
        "<div style=\"text-align:center;\">\n",
        "  <img src=\"image.png\" alt=\"Figure\" width=\"65%\"/>\n",
        "  <p><em>Figure 1. Trading off training compute vs. inference compute</em></p>\n",
        "</div>\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}