[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html",
    "href": "posts/003_cuda_inference_kernels/index.html",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "",
    "text": "What makes LLM inference fast on GPUs? This post summarizes the core CUDA kernels, memory layouts, and scheduling tricks that dominate latency and throughput.\n\nAttention with KV cache (paged KV, continuous batch, block-sparse)\nGEMM-heavy layers (QKV projections, MLP) and tensor cores\nQuantization (weight-only, KV cache) and dequant overhead\nServing stacks (vLLM, TensorRT-LLM, Triton) and scheduling\n\n\n  \n  Figure 1. vLLM logo"
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#overview",
    "href": "posts/003_cuda_inference_kernels/index.html#overview",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "",
    "text": "What makes LLM inference fast on GPUs? This post summarizes the core CUDA kernels, memory layouts, and scheduling tricks that dominate latency and throughput.\n\nAttention with KV cache (paged KV, continuous batch, block-sparse)\nGEMM-heavy layers (QKV projections, MLP) and tensor cores\nQuantization (weight-only, KV cache) and dequant overhead\nServing stacks (vLLM, TensorRT-LLM, Triton) and scheduling\n\n\n  \n  Figure 1. vLLM logo"
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#kernel-hotspots",
    "href": "posts/003_cuda_inference_kernels/index.html#kernel-hotspots",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "Kernel hotspots",
    "text": "Kernel hotspots\n\nAttention: QK^T, softmax, and AV; fused kernels reduce memory traffic.\nKV cache: paging and swizzling to keep contiguous access; block tables.\nGEMM: QKV, output projection, MLP GEMMs dominate FLOPs; use tensor cores.\nQuant: weight-only (W8A16) vs. activation quant; KV cache quant (e.g., 8-bit) saves VRAM.\n\n\n# Simple throughput estimator\nfrom typing import Optional\n\ndef estimate_throughput(tokens_per_request: int,\n                        requests_per_second: float,\n                        decode_tokens_per_s: float,\n                        prefill_tokens_per_s: Optional[float] = None,\n                        prefill_tokens: int = 0) -&gt; float:\n    \"\"\"\n    Crude throughput estimate in tokens/sec.\n    - tokens_per_request: average generated tokens per request\n    - requests_per_second: steady-state request rate\n    - decode_tokens_per_s: steady-state decode speed (per token) across batch\n    - prefill_tokens_per_s: optional prefill speed; if None, ignored\n    - prefill_tokens: avg prompt tokens per request\n    \"\"\"\n    decode_toks = tokens_per_request * requests_per_second\n    total = decode_toks\n    if prefill_tokens_per_s is not None and prefill_tokens &gt; 0:\n        total += (prefill_tokens * requests_per_second) * (prefill_tokens_per_s / decode_tokens_per_s)\n    return total\n\nprint(estimate_throughput(tokens_per_request=200,\n                          requests_per_second=5.0,\n                          decode_tokens_per_s=200000.0,\n                          prefill_tokens_per_s=1200000.0,\n                          prefill_tokens=1000))"
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#vllm-vs-tensorrt-llm-vs-triton-high-level",
    "href": "posts/003_cuda_inference_kernels/index.html#vllm-vs-tensorrt-llm-vs-triton-high-level",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "vLLM vs TensorRT-LLM vs Triton (high level)",
    "text": "vLLM vs TensorRT-LLM vs Triton (high level)\n\nvLLM: Continuous batching, paged KV cache, flexible Python API; great for dynamic workloads; supports multi-model serving.\nTensorRT-LLM: Highly optimized CUDA/TensorRT kernels, graph capture, INT8/FP8 pipelines; excels on NVIDIA stacks for max perf.\nTriton (NVIDIA inference server): Orchestrates models/runtimes (can host vLLM or TensorRT-LLM backends), handles deployment/HTTP/gRPC/metrics.\n\nRule of thumb: For maximum single-GPU perf and static graphs, TensorRT-LLM can edge out; for dynamic batching/multitenancy ease, vLLM is excellent; Triton adds production serving features."
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#kv-cache-sizing-and-paging",
    "href": "posts/003_cuda_inference_kernels/index.html#kv-cache-sizing-and-paging",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "KV cache sizing and paging",
    "text": "KV cache sizing and paging\n\nKV size per token ≈ 2 * num_layers * num_heads * head_dim * dtype_bytes.\nMultiply by max sequence length and batch to estimate worst-case VRAM.\nPaged KV: allocate in fixed-size blocks (e.g., 16/32/64 tokens) and map logical positions → physical pages; reduces fragmentation and enables efficient eviction.\nSwizzling/contiguous layout within blocks keeps memory coalesced for attention reads.\n\n\n# KV cache memory estimator (bytes)\nfrom typing import Literal\n\ndtype_sizes = {\n    \"fp16\": 2,\n    \"bf16\": 2,\n    \"fp32\": 4,\n    \"int8\": 1,\n}\n\ndef kv_cache_bytes(num_layers: int,\n                   num_heads: int,\n                   head_dim: int,\n                   seq_len: int,\n                   batch_size: int,\n                   dtype: Literal[\"fp16\", \"bf16\", \"fp32\", \"int8\"] = \"fp16\") -&gt; int:\n    per_token = 2 * num_layers * num_heads * head_dim * dtype_sizes[dtype]\n    total_tokens = seq_len * batch_size\n    return per_token * total_tokens\n\n# Example: 32 layers, 32 heads, 128 dim, seq 4k, batch 16, fp16\nprint(kv_cache_bytes(32, 32, 128, 4096, 16, \"fp16\") / (1024**3), \"GiB\")"
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#batching-and-scheduling",
    "href": "posts/003_cuda_inference_kernels/index.html#batching-and-scheduling",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "Batching and scheduling",
    "text": "Batching and scheduling\n\nContinuous batching: backfill finished sequences to keep kernels busy; great for variable-length requests.\nShort-first / length-aware: schedule shorter jobs to reduce tail latency; mix for throughput.\nSpec decode: trade compute for fewer steps; helps throughput if memory is ample.\nPinned memory and overlap: overlap H2D/D2H copies with compute using streams."
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#kernel-level-optimization-details",
    "href": "posts/003_cuda_inference_kernels/index.html#kernel-level-optimization-details",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "Kernel-level optimization details",
    "text": "Kernel-level optimization details\n\nFused attention: fuse QK^T, scaling, softmax, and AV to reduce DRAM round-trips.\nFlashAttention-style tiling: block SRAM tiles to keep on-chip; minimize global reads/writes.\nLayouts: use row/col-major consistent with GEMM kernels; prefer tensor core-friendly shapes (multiple of 8/16).\nPersistent kernels: reduce launch overhead; good for steady-state decode.\nQuant & dequant fusion: fuse dequant→GEMM to avoid bandwidth blowups."
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#system-level-tips",
    "href": "posts/003_cuda_inference_kernels/index.html#system-level-tips",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "System-level tips",
    "text": "System-level tips\n\nCUDA graphs: capture steady decode loop to reduce CPU overhead.\nStreams: separate prefill, decode, and IO with stream priorities.\nNUMA & pinning: pin host buffers and align worker affinity to avoid cross-socket traffic.\nOverlap: prefetch next batch to GPU while decoding current tokens."
  },
  {
    "objectID": "posts/003_cuda_inference_kernels/index.html#references",
    "href": "posts/003_cuda_inference_kernels/index.html#references",
    "title": "CUDA Inference Kernels and Serving Stacks",
    "section": "References",
    "text": "References\n\nvLLM docs and papers (continuous batching, paged KV)\nTensorRT-LLM samples and perf guides\nFlashAttention papers and kernels\nNVIDIA Triton Inference Server docs"
  },
  {
    "objectID": "posts/020_synthetic_biology_ai/index.html",
    "href": "posts/020_synthetic_biology_ai/index.html",
    "title": "Designing Life: AI-Driven Approaches to Synthetic Biology",
    "section": "",
    "text": "Synthetic biology aims to engineer biological systems: redesigning proteins, building genetic circuits, creating organisms with new capabilities. For decades, this was a craft—slow, laborious, guided by intuition and trial-and-error. AI is changing that. Sequence-to-function models, generative design, and automated optimization are turning synthetic biology into something closer to an information science."
  },
  {
    "objectID": "posts/020_synthetic_biology_ai/index.html#the-design-build-test-learn-cycle",
    "href": "posts/020_synthetic_biology_ai/index.html#the-design-build-test-learn-cycle",
    "title": "Designing Life: AI-Driven Approaches to Synthetic Biology",
    "section": "The Design-Build-Test-Learn Cycle",
    "text": "The Design-Build-Test-Learn Cycle\nSynthetic biology operates on a DBTL cycle:\n\nDesign: Specify a biological component or system (DNA sequence, protein, pathway)\nBuild: Synthesize the DNA, construct the organism\nTest: Measure function—does it work? How well?\nLearn: Analyze results, update understanding, iterate\n\nThe bottleneck has traditionally been the test phase. Biology is slow and unpredictable. Growing cells, measuring phenotypes, and interpreting noisy data takes weeks. Each cycle yields limited information.\nAI attacks every phase: - Design: Generative models propose sequences with desired properties - Build: Automated synthesis and assembly scale throughput - Test: High-throughput screens and readouts accelerate measurement - Learn: Machine learning extracts patterns from accumulated data\nThe vision: compress DBTL cycles from months to weeks to days, with each cycle far more informative."
  },
  {
    "objectID": "posts/020_synthetic_biology_ai/index.html#sequence-function-prediction",
    "href": "posts/020_synthetic_biology_ai/index.html#sequence-function-prediction",
    "title": "Designing Life: AI-Driven Approaches to Synthetic Biology",
    "section": "Sequence-Function Prediction",
    "text": "Sequence-Function Prediction\nThe central problem in synthetic biology: given a sequence (DNA, RNA, protein), predict its function. If you can do this accurately, you can search sequence space computationally instead of experimentally.\nFitness landscapes: Imagine sequence space as a high-dimensional landscape where elevation represents fitness (function). Evolution explores this landscape by mutation and selection. Synthetic biology wants to navigate it deliberately.\nDeep learning approaches: - Train on libraries of sequence variants with measured functions - Use architectures suited to sequences: transformers, convolutional networks, graph neural networks - Predict any measurable property: expression level, binding affinity, catalytic rate, stability\nChallenges: - Data scarcity: Unlike images or text, biological data is expensive to generate - Context dependence: A protein that works in E. coli might fail in yeast - Epistasis: Mutations interact nonlinearly; effects aren’t additive\nDespite these challenges, modern models can predict function well enough to guide design in many domains."
  },
  {
    "objectID": "posts/020_synthetic_biology_ai/index.html#directed-evolution-vs.-ai-guided-design",
    "href": "posts/020_synthetic_biology_ai/index.html#directed-evolution-vs.-ai-guided-design",
    "title": "Designing Life: AI-Driven Approaches to Synthetic Biology",
    "section": "Directed Evolution vs. AI-Guided Design",
    "text": "Directed Evolution vs. AI-Guided Design\nDirected evolution is biology’s most successful engineering strategy: randomly mutate sequences, screen for function, repeat. It won Frances Arnold a Nobel Prize and has produced industrial enzymes, therapeutic antibodies, and agricultural traits.\nBut directed evolution is local—it only explores sequences near the starting point. Large jumps in sequence space are unlikely to hit functional variants by chance. And it’s expensive—each round requires building and testing many variants.\nAI-guided design offers a different approach: - Train a model on existing functional sequences - Use the model to explore sequence space in silico - Propose distant sequences with predicted high function - Test only the top candidates experimentally\nThis enables larger leaps in sequence space and reduces experimental burden. The two approaches are complementary: use AI to propose promising starting points, then refine with directed evolution."
  },
  {
    "objectID": "posts/020_synthetic_biology_ai/index.html#generative-models-for-biological-sequences",
    "href": "posts/020_synthetic_biology_ai/index.html#generative-models-for-biological-sequences",
    "title": "Designing Life: AI-Driven Approaches to Synthetic Biology",
    "section": "Generative Models for Biological Sequences",
    "text": "Generative Models for Biological Sequences\nInstead of just predicting function, why not generate new sequences with desired properties?\nEvoDiff: Diffusion models for protein sequences. Trained on massive protein databases, EvoDiff can generate novel proteins conditioned on structure, function, or other constraints. It learns what “protein-like” means statistically.\nProGen: Autoregressive language models for proteins. Train on sequences with functional annotations, then generate sequences conditioned on the desired function (e.g., “lysozyme” → generates lysozyme-like sequences).\nESM-design and RFdiffusion: Structure-conditioned generative models. Given a target 3D structure (perhaps designed computationally), generate sequences that fold into that structure.\nDNA language models: Similar approaches applied to DNA—promoters, regulatory elements, genetic circuits.\nThese models are changing what’s possible. Instead of asking “does this sequence work?”, you ask “generate me sequences that work.” The model proposes; experiment disposes."
  },
  {
    "objectID": "posts/020_synthetic_biology_ai/index.html#cell-free-systems-rapid-prototyping",
    "href": "posts/020_synthetic_biology_ai/index.html#cell-free-systems-rapid-prototyping",
    "title": "Designing Life: AI-Driven Approaches to Synthetic Biology",
    "section": "Cell-Free Systems: Rapid Prototyping",
    "text": "Cell-Free Systems: Rapid Prototyping\nEven with computational design, you eventually need to test in reality. Cell-free systems accelerate this:\nCell-free protein synthesis (CFPS): Extract the protein-making machinery from cells; add DNA; produce protein in a test tube. Hours instead of days. No cell growth required.\nCell-free genetic circuits: Test regulatory elements and circuits without building cells. Fast iteration on design logic.\nAdvantages: - Rapid cycle times (hours, not days) - Easy parallelization (thousands of reactions in plates) - No viability constraints (toxic products don’t kill your system) - Direct access to internal states (no need to extract from cells)\nCell-free systems are the “prototyping environment” for synthetic biology—fast and cheap, even if final products need living cells."
  },
  {
    "objectID": "posts/020_synthetic_biology_ai/index.html#metabolic-engineering-and-pathway-optimization",
    "href": "posts/020_synthetic_biology_ai/index.html#metabolic-engineering-and-pathway-optimization",
    "title": "Designing Life: AI-Driven Approaches to Synthetic Biology",
    "section": "Metabolic Engineering and Pathway Optimization",
    "text": "Metabolic Engineering and Pathway Optimization\nMany synthetic biology goals involve rewiring cellular metabolism: produce a drug precursor, synthesize a biofuel, degrade a pollutant. This requires optimizing entire pathways, not just single proteins.\nPathway design: - Identify enzyme reactions to convert substrate to product - Balance expression levels to avoid bottlenecks or toxic intermediates - Tune regulation to maximize yield under growth conditions\nAI applications: - Retrosynthesis: Given a target molecule, computationally identify enzymatic routes to make it - Expression optimization: Predict how promoter/RBS choices affect enzyme levels - Flux balancing: Model whole-cell metabolism to predict yields - Active learning: Choose which pathway variants to test to maximize information gain\nCompanies like Ginkgo Bioworks, Zymergen (acquired), and Amyris (restructured) built foundries around this: high-throughput DBTL with AI-in-the-loop."
  },
  {
    "objectID": "posts/020_synthetic_biology_ai/index.html#biosecurity-considerations",
    "href": "posts/020_synthetic_biology_ai/index.html#biosecurity-considerations",
    "title": "Designing Life: AI-Driven Approaches to Synthetic Biology",
    "section": "Biosecurity Considerations",
    "text": "Biosecurity Considerations\nAI-accelerated biology isn’t neutral. The same tools that design therapeutic proteins can design toxins. The same generative models that create useful organisms can create dangerous ones.\nDual-use concerns: - Pathogen enhancement: Designing more transmissible or virulent variants - Novel toxins: Generating proteins with harmful effects - Evasion of detection: Engineering organisms to escape biosurveillance\nMitigation strategies: - Sequence screening: DNA synthesis companies screen orders for dangerous sequences - Access controls: Limit availability of the most capable models - Structured access: Require institutional affiliation and oversight for powerful tools - Capability monitoring: Track what becomes possible and adjust controls\nThe community is actively debating how to enable beneficial applications while limiting misuse. There are no easy answers, but ignoring the problem is not an option."
  },
  {
    "objectID": "posts/020_synthetic_biology_ai/index.html#case-studies",
    "href": "posts/020_synthetic_biology_ai/index.html#case-studies",
    "title": "Designing Life: AI-Driven Approaches to Synthetic Biology",
    "section": "Case Studies",
    "text": "Case Studies\nCAR-T cell design: Chimeric Antigen Receptor T cells are engineered immune cells that attack cancer. AI helps design antigen-binding domains, optimize signaling cascades, and predict patient-specific responses.\nIndustrial enzymes: Laundry detergent, food processing, and biofuels all use engineered enzymes. AI-guided evolution has produced enzymes that work in extreme conditions (high temperature, unusual pH) far faster than random mutation.\nBiosensors: Proteins engineered to fluoresce or trigger electrical signals in response to target molecules. Generative models can design sensors for analytes that have no natural binding proteins.\nSynthetic minimal genomes: Projects like JCVI-syn3.0 build organisms with the smallest possible genomes. AI helps predict which genes are essential and how to reorganize genomes."
  },
  {
    "objectID": "posts/020_synthetic_biology_ai/index.html#where-this-is-going",
    "href": "posts/020_synthetic_biology_ai/index.html#where-this-is-going",
    "title": "Designing Life: AI-Driven Approaches to Synthetic Biology",
    "section": "Where This Is Going",
    "text": "Where This Is Going\nSynthetic biology is becoming programmable. DNA synthesis costs have fallen ~10-million-fold in 20 years. Sequencing costs have fallen even faster. And now AI is addressing the design bottleneck.\nThe long-term vision: - Programmable medicine: Cells as therapeutics, engineered to sense disease and respond - Sustainable manufacturing: Bioproduction replacing petrochemistry - Environmental applications: Organisms that sequester carbon, clean up pollution, fix nitrogen - Agriculture: Crops engineered for nutrition, resilience, and sustainability\nAI doesn’t make any of this inevitable. Biology remains hard. But AI dramatically expands what’s designable and compresses the time to realize those designs. The field is entering an exponential phase."
  },
  {
    "objectID": "posts/002_information_theory_transformers/index.html",
    "href": "posts/002_information_theory_transformers/index.html",
    "title": "Information-Theoretic Bounds and Training Dynamics of Transformers",
    "section": "",
    "text": "This post sketches how training a transformer can be framed in information-theoretic terms:\n\nTraining minimizes empirical cross-entropy, i.e., expected negative log-likelihood.\nThe optimal achievable loss is the entropy rate of the data (given the model class).\nBits-per-token (bpt) is simply loss in base-2 units.\nGeneralization can be discussed via compression and mutual information.\n\n\n  \n  Figure 1. Transformers"
  },
  {
    "objectID": "posts/002_information_theory_transformers/index.html#overview",
    "href": "posts/002_information_theory_transformers/index.html#overview",
    "title": "Information-Theoretic Bounds and Training Dynamics of Transformers",
    "section": "",
    "text": "This post sketches how training a transformer can be framed in information-theoretic terms:\n\nTraining minimizes empirical cross-entropy, i.e., expected negative log-likelihood.\nThe optimal achievable loss is the entropy rate of the data (given the model class).\nBits-per-token (bpt) is simply loss in base-2 units.\nGeneralization can be discussed via compression and mutual information.\n\n\n  \n  Figure 1. Transformers"
  },
  {
    "objectID": "posts/002_information_theory_transformers/index.html#cross-entropy-entropy-rate-and-bits-per-token",
    "href": "posts/002_information_theory_transformers/index.html#cross-entropy-entropy-rate-and-bits-per-token",
    "title": "Information-Theoretic Bounds and Training Dynamics of Transformers",
    "section": "Cross-entropy, entropy rate, and bits-per-token",
    "text": "Cross-entropy, entropy rate, and bits-per-token\n\nCross-entropy (training loss) estimates how many nats per token the model spends to encode the data.\nBits-per-token (bpt) is just loss in base-2: bpt = loss_nat / ln 2.\nThe theoretical lower bound is the entropy rate H of the data-generating process; if the model class is misspecified, the optimum is H + D(P||Q*).\n\n\n# Convert loss (nats/token) to bits-per-token and perplexity\nimport math\nfrom typing import Iterable, Tuple\n\n\ndef loss_to_metrics(loss_nats: float) -&gt; Tuple[float, float]:\n    \"\"\"Return (bits_per_token, perplexity) from loss in nats/token.\"\"\"\n    bits_per_token = loss_nats / math.log(2)\n    perplexity = math.exp(loss_nats)\n    return bits_per_token, perplexity\n\n\nexample_losses = [3.5, 2.8, 2.2, 1.9]\nfor step, loss in enumerate(example_losses, start=1):\n    bpt, ppl = loss_to_metrics(loss)\n    print(f\"step={step:02d} loss={loss:.3f} nats -&gt; bpt={bpt:.3f}, ppl={ppl:.2f}\")"
  },
  {
    "objectID": "posts/002_information_theory_transformers/index.html#training-dynamics-as-compression",
    "href": "posts/002_information_theory_transformers/index.html#training-dynamics-as-compression",
    "title": "Information-Theoretic Bounds and Training Dynamics of Transformers",
    "section": "Training dynamics as compression",
    "text": "Training dynamics as compression\n\nMinimizing cross-entropy is equivalent to minimizing expected code length.\nEarly training reduces redundant predictability (frequent patterns); later, model learns rarer structure.\nCapacity vs. data curve: more parameters lower achievable cross-entropy until compute/data bottlenecks.\nGeneralization: MDL view—good models compress both train and test data with similar code lengths."
  },
  {
    "objectID": "posts/007_breakthrough/index.html",
    "href": "posts/007_breakthrough/index.html",
    "title": "Breakthrough #3 — Simulating: Neocortex, World Models, and Learning by Imagining",
    "section": "",
    "text": "Simulating is the chapter where Bennett’s staircase starts to look like a blueprint for building stronger agents. Reinforcement learning alone can sculpt behavior, but it is expensive: the agent must experience consequences, and mistakes can kill you. Simulation changes the bargain. You can evaluate possible actions internally before you commit to them in the world.\nBennett’s biological anchor here is neocortex (and, in his telling, the rise of mammals as a lineage that could trade metabolic cost for richer internal computation). The key claim is functional: brains evolve a generative modeling substrate that can run offline, producing coherent “what might happen” scenarios that guide choice.\nI find it helpful to distinguish three layers in Bennett’s simulating story:\n\nGenerative prediction. The brain can generate plausible next states, not just react to current stimuli.\nCounterfactual variation. The brain can generate alternatives: “what if I did something else?”\nDecision influence. The brain uses those generated futures to bias present action selection.\n\nIt’s that third step that turns prediction into planning. Prediction alone can exist without agency. Planning is prediction deployed as a control tool.\nBennett is unusually explicit about the hard part: not modeling, but search. If the brain can imagine many futures, it must decide which ones to spend compute on. It must choose whether to go deep on one scenario or sample many shallow ones. It must decide when to stop thinking and act. This is not a poetic flourish; it’s the planning bottleneck that shows up in every serious AI planning system.\nThe value of Bennett’s framing is that it forces you to treat “thinking” as compute allocation. An agent that thinks well is an agent that uses a limited budget of imagination in the places where it matters most. That is why the rise of frontal cortex-like structures is conceptually tied to simulation: you need a controller for the simulator itself.\nHis treatment of episodic memory fits nicely into this. If simulation is a generative process, then memory is not just storage; it is the ability to re-render a specific past. Replay then becomes a way of training the generative model and the policy using experiences that are rare or important. Bennett’s two-speed story—fast episodic capture plus slow structural learning—maps onto a very common engineering problem: you want a system that can both remember exceptions and still generalize.\nCounterfactuals are the real accelerator. If you can replay an episode and evaluate unchosen alternatives, you squeeze more learning signal out of the same experience. In practical terms, you reduce the number of costly real-world trials required to become competent. This is why simulating feels like a qualitative jump: it’s not just that the agent is smarter; it’s that the agent can learn in a safer, cheaper regime.\nSimulation also introduces new failure modes, and Bennett’s staircase implicitly depends on recognizing them. A model-free learner is slow and rigid. A simulator can be fast and imaginative—and wrong. It can hallucinate. It can over-plan using inaccurate dynamics. It can get trapped in internally coherent but externally false stories.\nThat matters because it motivates why later breakthroughs focus on grounding: mentalizing introduces constraints from other minds (social correction), and language turns simulation into something that can be checked and aligned across people. In other words, the staircase is not “more compute forever.” It’s “more compute plus new ways to keep it pointed at reality.”\n\n  \n  Figure 1. Vicarious trial-and-error: pausing to evaluate options before acting"
  },
  {
    "objectID": "posts/008_breakthrough/index.html",
    "href": "posts/008_breakthrough/index.html",
    "title": "Breakthrough #4 — Mentalizing: Second-Order Models, Theory of Mind, and Primate Intelligence",
    "section": "",
    "text": "Mentalizing is Bennett’s attempt to compress a lot of primate uniqueness into a single computational upgrade. In his telling, primates don’t just simulate the external world; they simulate minds. They infer hidden variables—beliefs, knowledge, goals, intent—that generate observable behavior.\nThat sounds like philosophy until you notice how algorithmic it is. If other agents exist, then the environment is no longer governed only by physics. It’s governed by policies. The biggest uncertainty in your world becomes “what will they do?” And the best predictors of what they will do are not visible objects; they are invisible internal states.\nBennett’s strongest unification move is to connect theory of mind, imitation, and “future needs” under one umbrella.\n\nTheory of mind: predicting others by inferring what they know and what they want.\nImitation: copying competence by inferring the underlying goal, not the surface motor pattern.\nFuture needs: planning for a future self whose drives differ from the present self.\n\nAll three require the same thing: a model in which behavior is explained by latent mental causes. Once you have that model for yourself, it becomes reusable for others. That reuse story is the kind of thing evolution loves: build one abstraction, get multiple capabilities for “free.”\nImitation is where this gets concrete. Behavioral cloning is brittle: copying the exact moves only works when bodies, constraints, and contexts match. Skilled imitation requires intent inference. You watch someone do a task, you infer what objective they are achieving and what constraints they respect, and then you re-plan under your own dynamics. That is mentalizing as an algorithm.\nThe “future needs” angle is similarly concrete once you think about sparse environments. If you only seek water when thirsty, you die in deserts. To carry water now for thirst later, you must represent a future self whose preferences differ from your present self. Bennett is effectively saying that primates can treat their future selves like other agents: they can reason about a self they are not currently experiencing.\nHe also notes the characteristic bias this introduces: we project the present into the future and into other people. If mentalizing is inference with priors, projection is exactly what you should expect when your own state is a strong prior and compute is limited.\nFrom an ML perspective, mentalizing is adjacent to inverse planning and inverse reinforcement learning: infer what objective a policy is optimizing from trajectories. It’s also adjacent to opponent modeling: you build an internal model of the other agent’s policy class, then condition on inferred intent.\nThe interesting research question Bennett’s story raises is: what is the training signal? Inverse problems are ill-posed. How does a brain learn the latent variables that make behavior predictable? Social feedback is one candidate (you get punished when you mispredict people). Self-consistency is another (a self-model that doesn’t help you control yourself gets selected against). Culture and language likely turbocharge it later, but Bennett wants a primate-level version that predates full language.\nWhether or not you buy every anatomical mapping, the computational claim is strong: the world becomes much more predictable once you model agents as optimizing hidden objectives, and primates look like the lineage that doubled down on that move.\n\n  \n  Figure 1. Predicting behavior by inferring hidden goals, knowledge, and intent"
  },
  {
    "objectID": "posts/014_compute_economics/index.html",
    "href": "posts/014_compute_economics/index.html",
    "title": "The GPU Economy: Understanding AI Infrastructure as a Strategic Asset",
    "section": "",
    "text": "Behind every AI capability is a stack of silicon. GPUs, TPUs, and custom AI accelerators are the physical substrate that makes training and inference possible. Understanding the economics of this infrastructure—who builds it, who controls supply, what it costs—is essential for understanding AI’s trajectory."
  },
  {
    "objectID": "posts/014_compute_economics/index.html#the-chip-supply-chain",
    "href": "posts/014_compute_economics/index.html#the-chip-supply-chain",
    "title": "The GPU Economy: Understanding AI Infrastructure as a Strategic Asset",
    "section": "The Chip Supply Chain",
    "text": "The Chip Supply Chain\nThe production of AI chips involves one of the most complex supply chains humans have ever built:\nDesign: Companies like NVIDIA, AMD, and Google design chip architectures without owning fabrication facilities (“fabless”). NVIDIA designs; someone else manufactures. This requires massive R&D investment and years of development per generation.\nFabrication: Taiwan Semiconductor Manufacturing Company (TSMC) produces ~90% of the world’s most advanced chips. Samsung is a distant second. Intel is trying to catch up. This concentration creates extreme supply chain risk.\nEquipment: The machines that make chips are even more concentrated. ASML (Netherlands) is the sole supplier of EUV lithography machines—the tools that print features at the 7nm scale and below. Without ASML, no one makes cutting-edge chips.\nPackaging and assembly: Advanced packaging (stacking dies, chiplet architectures) happens at TSMC and specialized facilities. This is increasingly the integration point for AI systems.\nEnd systems: Cloud providers (AWS, Google Cloud, Azure) and AI labs (OpenAI, Anthropic, DeepMind) assemble chips into datacenters and provide services.\nThe result: a small number of chokepoints control global AI compute capacity."
  },
  {
    "objectID": "posts/014_compute_economics/index.html#gpu-scarcity",
    "href": "posts/014_compute_economics/index.html#gpu-scarcity",
    "title": "The GPU Economy: Understanding AI Infrastructure as a Strategic Asset",
    "section": "GPU Scarcity",
    "text": "GPU Scarcity\nIn 2023-2024, NVIDIA’s H100 GPUs became famously difficult to obtain. Waiting lists stretched months. Prices spiked. Why?\nDemand surge: ChatGPT demonstrated that LLMs were ready for prime time. Everyone wanted to train their own. Demand for training compute spiked.\nSupply constraints: Fabrication capacity is limited. TSMC can only produce so many wafers. NVIDIA prioritized its most profitable customers.\nHoarding: Companies bought more than they immediately needed to secure future supply. This amplified scarcity.\nLead times: Building fab capacity takes 3-5 years. Supply can’t respond quickly to demand spikes.\nThe scarcity was real, not artificial. NVIDIA’s market cap reflects genuine pricing power when supply is constrained.\nImplications for AI development: - Rich get richer: Well-funded labs train larger models. Compute-poor researchers fall behind. - Cloud concentration: Access to compute flows through a few large cloud providers. - Innovation bottlenecks: Ideas that require large-scale experiments wait in line for resources."
  },
  {
    "objectID": "posts/014_compute_economics/index.html#training-cost-scaling",
    "href": "posts/014_compute_economics/index.html#training-cost-scaling",
    "title": "The GPU Economy: Understanding AI Infrastructure as a Strategic Asset",
    "section": "Training Cost Scaling",
    "text": "Training Cost Scaling\nHow much does it cost to train a frontier model?\nRough estimates: - GPT-4-class models: $50-100 million in compute - Gemini Ultra: likely similar or higher - Claude 3.5: undisclosed but comparable\nThese costs are dominated by GPU-hours. A training run might use 10,000+ H100s for months.\nChinchilla scaling laws: For a given compute budget, there’s an optimal split between model size and training tokens. Earlier we overinvested in parameters; now we train smaller models longer. This affects the compute profile but not the total cost.\nFLOP budgets: Training compute is often measured in floating-point operations. GPT-3 was ~3.6 × 10²³ FLOPs. GPT-4 is estimated at 10²⁵ FLOPs or higher. Each generation is 10-100× more expensive.\nAt some point, training costs become prohibitive for most organizations. Only well-funded labs can afford to train frontier models from scratch."
  },
  {
    "objectID": "posts/014_compute_economics/index.html#inference-economics",
    "href": "posts/014_compute_economics/index.html#inference-economics",
    "title": "The GPU Economy: Understanding AI Infrastructure as a Strategic Asset",
    "section": "Inference Economics",
    "text": "Inference Economics\nTraining happens once; inference happens every time someone uses the model. As models deploy to millions of users, inference costs dominate.\nCost per token: Depends on model size, hardware, and optimization. GPT-4 costs ~$0.03/1K input tokens, $0.06/1K output tokens (API pricing, which includes margin). Open-source models on self-hosted hardware can be 10-100× cheaper.\nThroughput vs. latency: Batching multiple requests together is efficient (high throughput) but adds latency. Real-time chat needs low latency and sacrifices efficiency.\nQuantization and optimization: Reducing precision (FP16 → INT8 → INT4) shrinks model size and speeds inference. Distillation creates smaller, faster models. These techniques trade some quality for major efficiency gains.\nInference chips: NVIDIA GPUs are great at training but not necessarily optimal for inference. Specialized inference chips (NVIDIA T4, custom ASICs, AWS Inferentia) optimize for cost-per-query.\nThe inference cost structure matters for business models. Products that use expensive models on long contexts need to charge accordingly or find efficiency gains."
  },
  {
    "objectID": "posts/014_compute_economics/index.html#cloud-vs.-on-prem",
    "href": "posts/014_compute_economics/index.html#cloud-vs.-on-prem",
    "title": "The GPU Economy: Understanding AI Infrastructure as a Strategic Asset",
    "section": "Cloud vs. On-Prem",
    "text": "Cloud vs. On-Prem\nWhere does AI compute run?\nHyperscalers (AWS, Azure, GCP): Most training and inference runs on cloud infrastructure. Benefits: scale on demand, no capex, managed services. Costs: higher unit prices, vendor lock-in, data egress fees.\nSpecialized AI clouds (Lambda Labs, CoreWeave, Together): Focus on GPU availability and AI-specific services. Often more competitive pricing for pure compute.\nOn-prem and self-hosted: Large enterprises and research institutions increasingly build their own GPU clusters. Benefits: control, customization, data sovereignty. Costs: capex, operations, utilization risk.\nEdge deployment: Running small models on devices (phones, laptops, robots). Latency-sensitive or offline applications. Severely constrained by device thermal and power limits.\nThe trend is bifurcation: massive training runs on hyperscale clouds; inference increasingly distributed to specialized providers and edge devices."
  },
  {
    "objectID": "posts/014_compute_economics/index.html#energy-constraints",
    "href": "posts/014_compute_economics/index.html#energy-constraints",
    "title": "The GPU Economy: Understanding AI Infrastructure as a Strategic Asset",
    "section": "Energy Constraints",
    "text": "Energy Constraints\nAI datacenters use enormous amounts of power, and the problem is getting worse:\nPower density: A rack of H100 GPUs uses 10-20kW. A large AI cluster might use hundreds of megawatts—the output of a small power plant.\nCooling: Most datacenter energy goes to cooling, not compute. Liquid cooling is increasingly necessary for high-density GPU racks.\nLocation matters: Cheap power and cool climates attract AI infrastructure. Nordic countries, the Pacific Northwest, and locations near hydropower are popular.\nCarbon implications: Unless powered by renewables, AI training has significant carbon footprint. Large training runs can emit as much CO2 as hundreds of flights.\nPower availability is becoming a constraint on datacenter construction. In some regions, there simply isn’t enough grid capacity for proposed AI facilities."
  },
  {
    "objectID": "posts/014_compute_economics/index.html#geopolitics",
    "href": "posts/014_compute_economics/index.html#geopolitics",
    "title": "The GPU Economy: Understanding AI Infrastructure as a Strategic Asset",
    "section": "Geopolitics",
    "text": "Geopolitics\nThe concentration of chip supply chains creates geopolitical vulnerabilities:\nTaiwan risk: TSMC’s fabrication facilities are in Taiwan. A Chinese invasion or blockade would devastate global chip supply. The US, Japan, and Europe are investing heavily to diversify, but new fabs take years.\nExport controls: The US restricts export of advanced chips and chip-making equipment to China. This limits China’s ability to train frontier models and slows their AI development—at least for now.\nCHIPS Act: The US is investing $52B to rebuild domestic semiconductor manufacturing. Intel, TSMC, and Samsung are building fabs on US soil. But these take 3-5 years to complete.\nChina’s response: China is investing heavily in domestic alternatives. Huawei’s Ascend chips, SMIC’s fabrication capabilities. They’re behind but working to close the gap.\nThe AI compute supply chain is one of the most strategically significant industrial bases in the world. Governments treat it accordingly."
  },
  {
    "objectID": "posts/014_compute_economics/index.html#future-compute",
    "href": "posts/014_compute_economics/index.html#future-compute",
    "title": "The GPU Economy: Understanding AI Infrastructure as a Strategic Asset",
    "section": "Future Compute",
    "text": "Future Compute\nSeveral trends may change the compute landscape:\nAlgorithmic efficiency: Moore’s Law has slowed, but algorithmic improvements continue. Better architectures, training methods, and representations can achieve the same capability with less compute. This is “software eating hardware.”\nSparsity: Most neural network weights are unnecessary for any given input. Sparse models activate only relevant subsets, dramatically reducing compute. Mixture-of-experts is one form of this.\nAlternative hardware: Photonic computing, neuromorphic chips, and analog computing offer potential for more efficient inference. Still nascent but under active development.\nQuantum computing: Theoretical advantages for some problems, but no near-term impact on the main workloads (transformers, diffusion). Quantum machine learning is mostly hype so far.\nThe most likely scenario: continued GPU/TPU improvement + algorithmic efficiency gains, with specialized accelerators for specific applications."
  },
  {
    "objectID": "posts/014_compute_economics/index.html#implications",
    "href": "posts/014_compute_economics/index.html#implications",
    "title": "The GPU Economy: Understanding AI Infrastructure as a Strategic Asset",
    "section": "Implications",
    "text": "Implications\nUnderstanding AI infrastructure economics matters for:\nResearchers: Access to compute shapes what’s possible to study. Know the economics of your cloud provider.\nStartups: Compute costs are a major expense. Efficiency is a competitive advantage. Don’t assume you can just throw money at the problem.\nInvestors: The “picks and shovels” of AI (NVIDIA, TSMC, datacenter REITs) have been fantastic investments. The question is whether this persists.\nPolicymakers: Compute supply chains are strategic. Export controls, subsidies, and supply chain resilience are legitimate policy concerns.\nEveryone: The physical infrastructure underlying AI determines who can build powerful systems. It’s not just about software."
  },
  {
    "objectID": "posts/001_tokenization_dna_models/index.html",
    "href": "posts/001_tokenization_dna_models/index.html",
    "title": "Tokenization Laws and DNA Models",
    "section": "",
    "text": "This post explores how tokenization choices shape model capacity and performance, and how those ideas translate to DNA models. We’ll use k-mer tokenization on nucleotide sequences and discuss how EVO-2-style models work with nucleotide tokens.\n\nWhat are tokenization laws?\nHow do k-mer vocabularies trade off context length vs. vocabulary size?\nHow might EVO-2-like models represent nucleotides and long-range dependencies?\n\n\n  \n  Figure 1. EVO-2 model."
  },
  {
    "objectID": "posts/001_tokenization_dna_models/index.html#overview",
    "href": "posts/001_tokenization_dna_models/index.html#overview",
    "title": "Tokenization Laws and DNA Models",
    "section": "",
    "text": "This post explores how tokenization choices shape model capacity and performance, and how those ideas translate to DNA models. We’ll use k-mer tokenization on nucleotide sequences and discuss how EVO-2-style models work with nucleotide tokens.\n\nWhat are tokenization laws?\nHow do k-mer vocabularies trade off context length vs. vocabulary size?\nHow might EVO-2-like models represent nucleotides and long-range dependencies?\n\n\n  \n  Figure 1. EVO-2 model."
  },
  {
    "objectID": "posts/001_tokenization_dna_models/index.html#tokenization-laws-in-brief",
    "href": "posts/001_tokenization_dna_models/index.html#tokenization-laws-in-brief",
    "title": "Tokenization Laws and DNA Models",
    "section": "Tokenization laws in brief",
    "text": "Tokenization laws in brief\nTokenization laws describe empirical tradeoffs between model size, context length, and tokenizer vocabulary. For fixed compute, larger vocabularies shrink sequence length but increase embedding/softmax cost; smaller vocabularies do the opposite. The optimal point depends on data distribution and task (e.g., code vs. natural language vs. DNA)."
  },
  {
    "objectID": "posts/001_tokenization_dna_models/index.html#dna-modeling-with-k-mers",
    "href": "posts/001_tokenization_dna_models/index.html#dna-modeling-with-k-mers",
    "title": "Tokenization Laws and DNA Models",
    "section": "DNA modeling with k-mers",
    "text": "DNA modeling with k-mers\nFor DNA, a common tokenizer uses k-mers over the alphabet {A,C,G,T}. The vocabulary size is 4^k, and the stride determines overlap. Larger k compresses sequences but grows the vocab; smaller k expands sequences but keeps vocab small.\n\nExample: k=3 (3-mers) ⇒ vocab size = 64\nExample: k=6 (6-mers) ⇒ vocab size = 4096\n\nWe can quickly demonstrate 3-mer tokenization with stride 1.\n\n# Simple 3-mer tokenizer demo\nfrom collections import Counter\nfrom typing import List\n\n\ndef kmers(sequence: str, k: int = 3, stride: int = 1) -&gt; List[str]:\n    sequence = sequence.upper().replace(\"U\", \"T\")\n    tokens = []\n    for i in range(0, len(sequence) - k + 1, stride):\n        kmer = sequence[i:i+k]\n        if set(kmer) &lt;= {\"A\", \"C\", \"G\", \"T\"}:\n            tokens.append(kmer)\n    return tokens\n\n\nseq = \"ACGTACGTGACCT\"\nks = kmers(seq, k=3, stride=1)\nprint(\"Sequence:\", seq)\nprint(\"3-mers:\", ks)\nprint(\"Unique 3-mers:\", sorted(set(ks)))\nprint(\"Counts:\", Counter(ks))"
  },
  {
    "objectID": "posts/001_tokenization_dna_models/index.html#how-evo-2-style-models-use-nucleotide-tokens",
    "href": "posts/001_tokenization_dna_models/index.html#how-evo-2-style-models-use-nucleotide-tokens",
    "title": "Tokenization Laws and DNA Models",
    "section": "How EVO-2-style models use nucleotide tokens",
    "text": "How EVO-2-style models use nucleotide tokens\nHigh-level idea: - Use a tokenizer over nucleotides (e.g., 3–6-mer tokens) to convert sequences into discrete tokens. - Train an autoregressive transformer over these tokens to model genomic sequences. - Incorporate long-range context (e.g., thousands to millions of bases) using efficient attention or memory mechanisms. - Optionally multitask with masked objectives or structure-aware heads.\nThis lets the model learn motifs, regulatory patterns, and long-range interactions directly from token sequences."
  },
  {
    "objectID": "posts/013_multimodal_architectures/index.html",
    "href": "posts/013_multimodal_architectures/index.html",
    "title": "Inside Vision-Language Models: Encoders, Adapters, and Emergent Capabilities",
    "section": "",
    "text": "Vision-language models (VLMs) can describe images, answer questions about photographs, read documents, and solve visual puzzles. But how do they work? What architectural choices enable a language model to “see”? And what are the failure modes that reveal the limits of current approaches?\nThis post unpacks the internals of VLMs: the vision encoders, the adaptation layers, and the training regimes that produce multimodal understanding."
  },
  {
    "objectID": "posts/013_multimodal_architectures/index.html#vision-encoders-what-the-model-sees",
    "href": "posts/013_multimodal_architectures/index.html#vision-encoders-what-the-model-sees",
    "title": "Inside Vision-Language Models: Encoders, Adapters, and Emergent Capabilities",
    "section": "Vision Encoders: What the Model Sees",
    "text": "Vision Encoders: What the Model Sees\nBefore an LLM can process an image, the image must be converted into tokens. This is the job of the vision encoder.\nVision Transformer (ViT): The dominant architecture. Split the image into patches (typically 14×14 or 16×16 pixels), embed each patch as a vector, add positional embeddings, and run through transformer layers. The output is a sequence of patch embeddings—one per spatial location.\nCLIP (Contrastive Language-Image Pre-training): Trained to align image and text embeddings using contrastive learning. Given image-caption pairs, CLIP learns to maximize similarity between matching pairs and minimize it for non-matching. The resulting vision encoder produces embeddings that are already somewhat aligned with language.\nSigLIP: A variant of CLIP that uses sigmoid loss instead of softmax, enabling more efficient training on larger batches and producing better-calibrated embeddings.\nDINOv2: Self-supervised vision encoder trained without text. Focuses purely on visual structure—produces embeddings useful for dense prediction tasks like segmentation.\nThe choice of vision encoder affects what information is available to the language model. CLIP-based encoders emphasize semantic concepts aligned with language; DINOv2 emphasizes visual structure. Some models use both."
  },
  {
    "objectID": "posts/013_multimodal_architectures/index.html#fusion-strategies-how-modalities-meet",
    "href": "posts/013_multimodal_architectures/index.html#fusion-strategies-how-modalities-meet",
    "title": "Inside Vision-Language Models: Encoders, Adapters, and Emergent Capabilities",
    "section": "Fusion Strategies: How Modalities Meet",
    "text": "Fusion Strategies: How Modalities Meet\nGiven vision embeddings and a language model, how do you combine them? Several strategies exist:\nEarly fusion: Concatenate vision tokens directly with text tokens as input to the LLM. Simple but expensive—every forward pass processes all vision tokens through all layers.\nLate fusion: Process modalities separately until final layers, then combine. Efficient but limits cross-modal interaction.\nCross-attention: Add cross-attention layers where text tokens attend to vision tokens (or vice versa). The LLM can selectively query visual information without processing all vision tokens through all layers.\nGated fusion: Learn gates that control how much visual information flows into the language model at each layer. Allows the model to ignore vision when it’s irrelevant.\nMost modern VLMs use some form of early fusion with an adapter layer that compresses or transforms vision tokens before they enter the LLM."
  },
  {
    "objectID": "posts/013_multimodal_architectures/index.html#adapter-modules-the-bridge-between-modalities",
    "href": "posts/013_multimodal_architectures/index.html#adapter-modules-the-bridge-between-modalities",
    "title": "Inside Vision-Language Models: Encoders, Adapters, and Emergent Capabilities",
    "section": "Adapter Modules: The Bridge Between Modalities",
    "text": "Adapter Modules: The Bridge Between Modalities\nVision encoders produce embeddings in their own space; LLMs expect tokens in their embedding space. The adapter bridges this gap.\nLinear projection: The simplest approach—a learned linear layer that projects vision embeddings to the LLM’s hidden dimension. Used in LLaVA. Surprisingly effective.\nMLP adapter: A small feedforward network (2-3 layers) with nonlinearity. Slightly more expressive than linear projection.\nQ-Former (Querying Transformer): Used in BLIP-2. A set of learnable query tokens attend to (frozen) vision encoder outputs through cross-attention, producing a fixed number of output tokens regardless of image size. This compresses visual information and learns what aspects are relevant for language.\nPerceiver Resampler: Similar idea to Q-Former—use a small number of latent tokens to “summarize” the visual input through cross-attention. Used in Flamingo.\nC-Abstractor and variants: Learn hierarchical abstractions, preserving some spatial structure while compressing token count.\nThe number of visual tokens matters for efficiency. A 336×336 image at 14×14 patches produces 576 vision tokens. That’s a lot of context for the LLM. Adapters that reduce token count (Q-Former, Perceiver) trade off visual detail for efficiency."
  },
  {
    "objectID": "posts/013_multimodal_architectures/index.html#training-regimes",
    "href": "posts/013_multimodal_architectures/index.html#training-regimes",
    "title": "Inside Vision-Language Models: Encoders, Adapters, and Emergent Capabilities",
    "section": "Training Regimes",
    "text": "Training Regimes\nVLMs typically train in stages:\nStage 1: Vision-language alignment (pretraining) Freeze both vision encoder and LLM; only train the adapter. Use image-caption pairs. Goal: teach the adapter to project visual information into a form the LLM can use. This is cheap because most parameters are frozen.\nStage 2: Instruction tuning Unfreeze the LLM (and sometimes vision encoder). Train on visual instruction-following data: visual question-answering, image description, visual reasoning puzzles. Goal: teach the model to follow instructions about images.\nStage 3 (optional): RLHF or preference tuning Use human preferences to further refine outputs. Important for reducing hallucinations and improving helpfulness.\nThe quality and diversity of training data at each stage heavily influences capabilities. Models trained primarily on natural images struggle with documents, charts, and diagrams. Models trained on clean web data may hallucinate about photos with unusual content."
  },
  {
    "objectID": "posts/013_multimodal_architectures/index.html#emergent-capabilities",
    "href": "posts/013_multimodal_architectures/index.html#emergent-capabilities",
    "title": "Inside Vision-Language Models: Encoders, Adapters, and Emergent Capabilities",
    "section": "Emergent Capabilities",
    "text": "Emergent Capabilities\nModern VLMs exhibit impressive capabilities that weren’t explicitly trained:\nVisual reasoning: “How many apples are on the table?” “Which object is closer to the camera?” “What would happen if I pushed the ball?”\nOCR and document understanding: Reading text in images, understanding tables, parsing receipts and forms.\nSpatial reasoning: Understanding relative positions, following directions (“the object to the left of…”), parsing maps.\nWorld knowledge grounding: Identifying landmarks, recognizing species, understanding cultural context.\nChart and graph interpretation: Extracting data from visualizations, answering questions about trends.\nThese capabilities emerge from scale and diverse training data. No single supervision signal taught “chart reading”—it arose from exposure to many charts paired with textual descriptions."
  },
  {
    "objectID": "posts/013_multimodal_architectures/index.html#failure-modes",
    "href": "posts/013_multimodal_architectures/index.html#failure-modes",
    "title": "Inside Vision-Language Models: Encoders, Adapters, and Emergent Capabilities",
    "section": "Failure Modes",
    "text": "Failure Modes\nVLMs fail in characteristic ways that reveal their limitations:\nHallucination: Confidently describing objects or properties that aren’t in the image. “The woman is wearing a red hat” when there’s no hat. This is the most common and dangerous failure mode.\nObject binding errors: Correctly identifying that there’s a red ball and a blue cube, but incorrectly stating “the ball is blue.” Visual properties get misattributed to wrong objects.\nCounting failures: VLMs notoriously struggle with counting. “How many people are in the image?” often gets wrong answers, especially for larger numbers.\nPositional confusion: Left/right, above/below, near/far relations are often unreliable. The model may understand “there are two objects” but not their spatial relationship.\nNegation blindness: “Is there NOT a dog in the image?” is harder to answer reliably than “Is there a dog in the image?”\nFine-grained discrimination: Distinguishing similar breeds, species, or models. CLIP-style training emphasizes coarse categories over fine distinctions.\nThese failures suggest that current VLMs don’t build robust internal representations of scene structure. They’re doing something more like sophisticated pattern matching on visual features plus language context."
  },
  {
    "objectID": "posts/013_multimodal_architectures/index.html#case-studies-current-models",
    "href": "posts/013_multimodal_architectures/index.html#case-studies-current-models",
    "title": "Inside Vision-Language Models: Encoders, Adapters, and Emergent Capabilities",
    "section": "Case Studies: Current Models",
    "text": "Case Studies: Current Models\nLLaVA: Clean, open architecture. ViT (CLIP-pretrained) → linear projection → Vicuna/LLaMA. Shows that sophisticated adapters aren’t always necessary.\nGPT-4V: Proprietary, likely uses a large ViT variant with substantial multimodal pretraining. Best-in-class on many benchmarks but expensive and closed.\nGemini: Trained natively multimodal from early stages (not a vision encoder bolted onto a language model). Interleaves modalities more fluidly.\nClaude’s vision: Focuses on safety and reliability. Strong at document understanding and refusing problematic requests about images.\nQwen-VL, InternVL, CogVLM: Open-source models pushing various frontiers: resolution, efficiency, specialized capabilities.\nThe landscape is moving fast. New models appear monthly, each with different trade-offs between capability, efficiency, and openness."
  },
  {
    "objectID": "posts/013_multimodal_architectures/index.html#implementation-considerations",
    "href": "posts/013_multimodal_architectures/index.html#implementation-considerations",
    "title": "Inside Vision-Language Models: Encoders, Adapters, and Emergent Capabilities",
    "section": "Implementation Considerations",
    "text": "Implementation Considerations\nIf you’re working with VLMs, keep in mind:\nResolution matters: Higher resolution preserves more detail but increases cost (quadratic in resolution for attention). Many models use multi-resolution strategies—process at low res first, then zoom in on regions of interest.\nAspect ratio handling: Images aren’t all squares. Naive resizing distorts content. Better approaches: pad, crop, or tile into multiple regions.\nVideo: Extend to video by sampling frames and treating each as an image. Challenges: temporal reasoning, long-context efficiency, computing over many frames.\nMultiple images: Some VLMs handle multiple images per query; others struggle. Important for comparison tasks, before/after, or document processing.\nPrompt engineering for vision: Just like text, how you phrase visual questions matters. Being specific, asking for step-by-step reasoning, and providing context improves performance."
  },
  {
    "objectID": "posts/013_multimodal_architectures/index.html#where-this-is-heading",
    "href": "posts/013_multimodal_architectures/index.html#where-this-is-heading",
    "title": "Inside Vision-Language Models: Encoders, Adapters, and Emergent Capabilities",
    "section": "Where This Is Heading",
    "text": "Where This Is Heading\nSeveral trends are shaping the future of VLMs:\nNative multimodal training: Instead of adapting language models to vision, train on interleaved images and text from the start. Gemini and Chameleon point this direction.\nMore modalities: Audio, 3D, video, embodied sensors. The same architectural patterns extend.\nAgentic capabilities: VLMs that can interact with GUIs, browse the web, and manipulate visual environments.\nSpecialized vision: Domain-specific models for medical imaging, satellite imagery, scientific figures.\nEfficiency: Making VLMs fast and cheap enough for edge deployment.\nVision-language models represent a significant step toward AI systems that perceive and reason about the world as humans do. Understanding how they work—and where they fail—is essential for using them effectively."
  },
  {
    "objectID": "posts/012_protein_folding_revolution/index.html",
    "href": "posts/012_protein_folding_revolution/index.html",
    "title": "AlphaFold and Beyond: How AI Solved Protein Structure Prediction",
    "section": "",
    "text": "In December 2020, AlphaFold 2 demonstrated protein structure prediction accuracy rivaling experimental methods. This was a genuine scientific breakthrough—the kind where a problem that had resisted decades of effort suddenly yields. Understanding how AlphaFold works, and what it means, offers lessons for how AI might transform other areas of science."
  },
  {
    "objectID": "posts/012_protein_folding_revolution/index.html#the-protein-folding-problem",
    "href": "posts/012_protein_folding_revolution/index.html#the-protein-folding-problem",
    "title": "AlphaFold and Beyond: How AI Solved Protein Structure Prediction",
    "section": "The Protein Folding Problem",
    "text": "The Protein Folding Problem\nProteins are chains of amino acids that fold into 3D structures. The structure determines function: enzymes catalyze reactions because their shapes fit substrates; antibodies recognize pathogens by shape complementarity; ion channels open and close through conformational changes.\nThe folding problem: given a sequence of amino acids, predict the 3D structure. This matters because:\n\nExperimental structure determination is slow and expensive. X-ray crystallography, cryo-EM, and NMR can take months to years per structure.\nMost proteins have no known structure. Over 200 million protein sequences are known; fewer than 200,000 have experimental structures.\nStructure enables function prediction. If you know the shape, you can infer binding sites, mechanisms, and drug targets.\n\n\nLevinthal’s Paradox\nCyrus Levinthal pointed out in 1969 that proteins can’t fold by random search. Even a small protein has ~10^300 possible conformations. If a protein sampled a new conformation every picosecond, it would take longer than the age of the universe to find the right one by chance.\nYet proteins fold in milliseconds to seconds. This means the folding process follows an energy landscape that guides the chain toward the native structure. The physics is tractable, even if the search space is vast.\nTraditional computational approaches tried to exploit this physics—molecular dynamics, energy minimization, fragment assembly. They made progress but never achieved experimental accuracy across diverse proteins."
  },
  {
    "objectID": "posts/012_protein_folding_revolution/index.html#how-alphafold-works",
    "href": "posts/012_protein_folding_revolution/index.html#how-alphafold-works",
    "title": "AlphaFold and Beyond: How AI Solved Protein Structure Prediction",
    "section": "How AlphaFold Works",
    "text": "How AlphaFold Works\nAlphaFold 2 takes a sequence and predicts the 3D coordinates of every atom. The key insight: it treats structure prediction as a pattern recognition problem, learning from evolutionary relationships.\n\nMultiple Sequence Alignments (MSAs)\nThe input isn’t just the target sequence. AlphaFold retrieves related sequences from protein databases and aligns them. This multiple sequence alignment (MSA) contains evolutionary information:\n\nResidues that contact each other in 3D tend to co-evolve (if one mutates, the other compensates)\nConserved residues are often structurally important\nInsertion/deletion patterns reveal flexible regions\n\nMSAs encode enormous implicit knowledge about structure. Two residues that frequently co-vary are likely spatially close. This “covariance signal” has been exploited for years, but AlphaFold learned to use it far more effectively.\n\n\nThe Evoformer: Attention over MSA and Structure\nAlphaFold’s core is the Evoformer—a novel architecture that processes two representations simultaneously:\n\nMSA representation: Each row is a sequence; each column is a position. Contains alignment information.\nPair representation: Encodes relationships between every pair of residues. Contains inferred distance/contact information.\n\nThe Evoformer uses attention mechanisms to iteratively refine these representations: - Row-wise attention: Let sequences “compare notes” about each position - Column-wise attention: Let positions “compare notes” across sequences - Pair updates: Use MSA patterns to infer pairwise relationships - Triangle attention: Enforce consistency in the pair representation (if A is close to B and B is close to C, that constrains A-C)\nAfter many layers of this, the pair representation contains rich information about spatial relationships.\n\n\nStructure Module: From Representation to 3D Coordinates\nThe structure module takes the Evoformer outputs and produces 3D coordinates. It represents each residue as a local coordinate frame (position + orientation) and iteratively refines these frames.\nKey innovations: - Invariant Point Attention (IPA): Attention that respects 3D geometry. Queries, keys, and values include spatial information. - Iterative refinement: The structure module runs multiple times, each pass improving the prediction. - Recycling: The entire network can run multiple passes, with previous outputs fed back as inputs.\nThe output includes predicted coordinates for all backbone atoms (N, Cα, C) and side chains, plus a confidence score (pLDDT) per residue.\n\n\nTraining: Learn from Known Structures\nAlphaFold was trained on ~170,000 experimental structures from the Protein Data Bank (PDB). The loss functions include: - Frame-aligned point error (FAPE): Measures accuracy in local coordinate frames - Distance matrix similarity: Ensures pairwise distances match the target - Confidence calibration: Ensure pLDDT scores correlate with actual accuracy\nThe training is computationally intensive—equivalent to hundreds of thousands of GPU-hours. But once trained, prediction is fast: a few minutes per sequence on modest hardware."
  },
  {
    "objectID": "posts/012_protein_folding_revolution/index.html#single-sequence-methods-esmfold",
    "href": "posts/012_protein_folding_revolution/index.html#single-sequence-methods-esmfold",
    "title": "AlphaFold and Beyond: How AI Solved Protein Structure Prediction",
    "section": "Single-Sequence Methods: ESMFold",
    "text": "Single-Sequence Methods: ESMFold\nAlphaFold requires MSAs, which are slow to compute and sometimes unavailable (for orphan proteins with few homologs). ESMFold from Meta AI takes a different approach:\n\nUse a large protein language model (ESM-2) pretrained on protein sequences\nThe language model implicitly learns evolutionary patterns\nFine-tune a structure prediction head on top\n\nESMFold is much faster (no MSA search) and nearly as accurate for proteins with good language model representations. It demonstrates that large-scale language modeling on biological sequences captures structural information."
  },
  {
    "objectID": "posts/012_protein_folding_revolution/index.html#alphafold-3-beyond-proteins",
    "href": "posts/012_protein_folding_revolution/index.html#alphafold-3-beyond-proteins",
    "title": "AlphaFold and Beyond: How AI Solved Protein Structure Prediction",
    "section": "AlphaFold 3: Beyond Proteins",
    "text": "AlphaFold 3: Beyond Proteins\nAlphaFold 3 (2024) extends to: - Protein-ligand complexes: Predicting how small molecules bind to proteins - Nucleic acids: DNA and RNA structures - Protein-protein complexes: Multi-chain assemblies - Post-translational modifications: Predicting how modifications affect structure\nThe key innovation: a diffusion model for structure generation. Instead of direct coordinate prediction, AlphaFold 3 learns to denoise noisy structures—similar to how image diffusion models work. This allows more flexibility in handling different molecular types and enables confidence estimation through sampling."
  },
  {
    "objectID": "posts/012_protein_folding_revolution/index.html#limitations-and-open-problems",
    "href": "posts/012_protein_folding_revolution/index.html#limitations-and-open-problems",
    "title": "AlphaFold and Beyond: How AI Solved Protein Structure Prediction",
    "section": "Limitations and Open Problems",
    "text": "Limitations and Open Problems\nAlphaFold is transformative but not omniscient:\nDynamics: Proteins aren’t static. They flex, breathe, and transition between conformational states. AlphaFold predicts one structure (usually the lowest-energy state), not the ensemble of structures a protein explores.\nDisordered regions: Many proteins have intrinsically disordered regions that don’t adopt stable structures. AlphaFold often produces low-confidence predictions for these, which is the right behavior but not a solution.\nMembrane proteins: Proteins embedded in lipid membranes present challenges. The membrane environment isn’t modeled, and membrane protein structures are underrepresented in training data.\nRare folds: Proteins with unusual folds or few evolutionary relatives may have weaker MSAs and worse predictions.\nConformational changes upon binding: How proteins change shape when binding partners or substrates is often critical for function but hard to predict without knowing the binding partner."
  },
  {
    "objectID": "posts/012_protein_folding_revolution/index.html#impact-on-science",
    "href": "posts/012_protein_folding_revolution/index.html#impact-on-science",
    "title": "AlphaFold and Beyond: How AI Solved Protein Structure Prediction",
    "section": "Impact on Science",
    "text": "Impact on Science\nAlphaFold’s release (both the model and a database of 200+ million predicted structures) has transformed structural biology:\nDrug discovery: Predicted structures enable virtual screening and structure-based drug design for targets without experimental structures. Timelines are compressed.\nEnzyme engineering: Engineers can predict how mutations affect structure and design enzymes with new properties.\nUnderstanding disease: Mutations that cause disease often do so by disrupting protein structure. Predicted structures help interpret genetic variants.\nEvolutionary biology: Structure predictions across entire proteomes enable comparative analysis of evolutionary relationships.\nExperimental biology: Even when experimental structures are needed, predictions guide experiments, reducing search space and providing starting models."
  },
  {
    "objectID": "posts/012_protein_folding_revolution/index.html#lessons-for-ai-for-science",
    "href": "posts/012_protein_folding_revolution/index.html#lessons-for-ai-for-science",
    "title": "AlphaFold and Beyond: How AI Solved Protein Structure Prediction",
    "section": "Lessons for AI-for-Science",
    "text": "Lessons for AI-for-Science\nWhat made AlphaFold succeed where others didn’t?\n\nRich prior knowledge: MSAs encode billions of years of evolution. AlphaFold learned to extract this signal.\nInductive biases that match the problem: Triangle attention, IPA, and coordinate frames are designed for spatial reasoning about 3D structures.\nHigh-quality data: The PDB is a curated database of experimental structures accumulated over decades.\nEnd-to-end learning: Rather than pipelining separate components (predict contacts → assemble structure), AlphaFold learns everything jointly.\nScale: Both model size and compute were substantial. This is expensive science.\n\nThese lessons generalize. AI-for-science works best when: data is abundant and curated, the problem has exploitable structure, and architectural choices respect domain knowledge."
  },
  {
    "objectID": "posts/009_breakthrough/index.html",
    "href": "posts/009_breakthrough/index.html",
    "title": "Breakthrough #5 — Speaking: Language as Curriculum, Joint Attention, and the Human Hive Mind",
    "section": "",
    "text": "Speaking is where Bennett stops treating brains as isolated optimizers and starts treating humans as participants in a collective learning system. Language is not just communication in his framing. It is the mechanism that aligns internal simulations across individuals, and the alignment enables cumulative culture.\nTwo ideas do the heavy lifting: (1) humans evolved a developmental pipeline that makes language learnable, and (2) language acts as a low-bandwidth interface to high-dimensional internal models.\nOn learnability, Bennett emphasizes proto-conversations and joint attention. Proto-conversations are the turn-taking rhythms that appear before full speech: the infant and caregiver already coordinate timing and expectation. Joint attention is the active work of ensuring shared reference. The child doesn’t just notice an object; the child notices that the adult notices it too. That “we are aligned” fact is what makes a label unambiguous.\nThis matters because raw scenes are wildly ambiguous. If you hear a word while looking at a room, what does the word refer to? The cup? The color? The action? The relation? Joint attention collapses the hypothesis space by constructing a teaching context. The child’s environment is not a passive dataset; it is an interactive training regime engineered by caregivers and enforced by the child’s own drive to be understood.\nAs an ML person, I read this as a biological argument for why data quality is not “just more tokens.” The distribution is actively shaped for learnability. The learner actively shapes what data it receives. That is an underappreciated difference between human language acquisition and most current-scale language modeling.\nOn function, Bennett’s “tethering simulations” metaphor is right. A sentence is not a high-bandwidth dump of an internal movie. It’s a compressed control signal that causes the listener to reconstruct the relevant movie inside their own generative model. Words behave like handles into shared structure; grammar binds handles into relations.\nOnce this works, culture becomes an external memory system. Skills, tools, norms, and explanations survive beyond any one individual. Bennett calls this a “hive mind” in spirit: not because humans lose individuality, but because the unit of intelligence becomes partially collective. The population becomes a distributed optimizer over ideas, with language as the protocol that moves partial solutions between brains.\nThis also creates a feedback loop that accelerates intelligence: better communication improves coordination; coordination enables more complex tools and institutions; those tools change the developmental environment; and the new environment selects for even better learning and communication. Bennett is telling an evolutionary scaling story: intelligence doesn’t only scale by adding neurons; it scales by adding transmission and accumulation.\nThe most interesting place to press Bennett here is how language relates to simulation and mentalizing. His scaffold suggests language is not the root capability; it is a coordination layer built on top of reinforcement, simulation, and mind-modeling. That gives you a clean way to compare humans and current AI systems:\n\nLLMs show language can carry immense structure and can induce impressive competence.\nBennett’s story suggests the strongest generality comes when language is coupled to objectives, feedback, world models, and multi-agent inference.\n\nSo the question for modern AI is not “does language matter?” It obviously does. The question is whether language-only training reliably yields the deeper agentic stack, or whether you eventually need the other ingredients directly (embodied objectives, interactive feedback, explicit planning, social inference). Bennett’s book doesn’t answer that, but it gives you a coherent set of axes to discuss it without handwaving.\n\n  \n  Figure 1. Language as a low-bandwidth channel that aligns rich internal models across minds"
  },
  {
    "objectID": "posts/024_vc_decision_making/index.html",
    "href": "posts/024_vc_decision_making/index.html",
    "title": "Inside the Partner Meeting: How VCs Actually Evaluate AI Companies",
    "section": "",
    "text": "Raising venture capital is opaque. Founders get rejections without understanding why. VCs seem to pattern-match in mysterious ways. But there’s more structure than it appears. Understanding how VCs actually evaluate companies—especially AI companies—helps founders prepare, pitch, and navigate the process."
  },
  {
    "objectID": "posts/024_vc_decision_making/index.html#the-vc-business-model",
    "href": "posts/024_vc_decision_making/index.html#the-vc-business-model",
    "title": "Inside the Partner Meeting: How VCs Actually Evaluate AI Companies",
    "section": "The VC Business Model",
    "text": "The VC Business Model\nTo understand VC decisions, understand the incentives.\nPower laws: VC returns are dominated by a tiny fraction of investments. The best deal in a fund might return more than all other deals combined. VCs optimize for finding outliers, not avoiding failures.\nPortfolio construction: A typical fund makes 20-40 investments. Most will fail or return 1-2×. The fund needs 1-3 massive wins (10-100×) to hit its return target.\nFund economics: VCs charge ~2% annual management fee and ~20% of profits (carry). Limited partners (LPs) expect returns of 3× their capital over 10 years. Anything less, and LPs won’t re-invest.\nImplications for founders: - VCs won’t invest in “good companies” that can’t be huge. You need a path to 10× or more. - VCs would rather miss a good company than invest in a mediocre one. Passing is easy; regret is painful but infrequent. - VCs are looking for conviction about exceptional outcomes, not certainty about moderate ones."
  },
  {
    "objectID": "posts/024_vc_decision_making/index.html#evaluation-heuristics",
    "href": "posts/024_vc_decision_making/index.html#evaluation-heuristics",
    "title": "Inside the Partner Meeting: How VCs Actually Evaluate AI Companies",
    "section": "Evaluation Heuristics",
    "text": "Evaluation Heuristics\nVCs claim to evaluate many things. In practice, a few dimensions dominate:\nTeam: Who are you? Do you have relevant experience? Do you seem capable of executing? Are you coachable? Do you have technical depth in AI if it’s an AI company?\nFor AI companies: VCs want to see that founders understand the technology, not just the application. Can you speak fluently about models, training, inference, and the frontier? If you can’t, you need a technical co-founder who can.\nMarket: Is this market large enough to produce a huge company? Is it growing? Is it winnable by a startup or controlled by incumbents?\nFor AI companies: Markets that didn’t exist before AI (autonomous vehicles, protein design) are attractive. Markets where AI is a feature (adding AI to existing software categories) are harder—incumbents can add features.\nTraction: What have you built? Who’s using it? Are they paying? Are they retaining? How fast is it growing?\nFor AI companies: Demos don’t count. VCs have seen too many impressive demos that collapse in production. Show real usage, preferably with revenue.\nDifferentiation: What do you have that others don’t? Why will you win against competitors (including future ones)?\nFor AI companies: This is the crux. “We have a better model” is weak—models commoditize. “We have proprietary data” or “We own the workflow” is strong."
  },
  {
    "objectID": "posts/024_vc_decision_making/index.html#ai-specific-evaluation",
    "href": "posts/024_vc_decision_making/index.html#ai-specific-evaluation",
    "title": "Inside the Partner Meeting: How VCs Actually Evaluate AI Companies",
    "section": "AI-Specific Evaluation",
    "text": "AI-Specific Evaluation\nAI companies get additional scrutiny:\nModel ownership: Do you train your own models or use APIs? Own models = more differentiation but more cost. API-dependent = lower cost but thin differentiation.\nData strategy: Where does your training/fine-tuning data come from? Is it proprietary? Does the product generate more? Data is the most discussed form of moat in AI.\nTechnical depth: Can the team actually innovate at the model level, or are they “prompt engineers”? The deepest expertise commands the most respect.\nMoat durability: VCs are paranoid about OpenAI/Google/Meta releasing capabilities that destroy your differentiation. You need a story for why you’ll stay ahead or why the race doesn’t matter (because you own something they don’t).\nUnit economics in AI: Compute costs are high. VCs want to see that you can deliver value profitably. If you spend $1 on inference for every $0.50 in revenue, you’re in trouble."
  },
  {
    "objectID": "posts/024_vc_decision_making/index.html#the-partner-meeting",
    "href": "posts/024_vc_decision_making/index.html#the-partner-meeting",
    "title": "Inside the Partner Meeting: How VCs Actually Evaluate AI Companies",
    "section": "The Partner Meeting",
    "text": "The Partner Meeting\nInside a VC firm, decisions are made in partner meetings. Here’s how they typically work:\nChampion: One partner (the one who met you) presents the deal to others. They’ve already done initial diligence and are advocating for investment.\nDiscussion: Other partners ask questions, poke holes, share concerns. Standard objections: “market too small,” “why them,” “seen this before,” “too early/late.”\nVote or consensus: Different firms have different decision processes. Some require unanimous partner approval; others allow individual partner conviction.\nTime pressure: Competitive deals (multiple VCs interested) force faster decisions. Slow deals with no competition allow more diligence—and often lead to passes.\nFor founders: - Your champion needs ammunition to defend you. Give them clear, memorable points to make in the meeting. - Anticipate the objections. “Why can’t OpenAI do this?” has to be answered. - The partner meeting is the decision point. Everything before is filtering."
  },
  {
    "objectID": "posts/024_vc_decision_making/index.html#pattern-matching",
    "href": "posts/024_vc_decision_making/index.html#pattern-matching",
    "title": "Inside the Partner Meeting: How VCs Actually Evaluate AI Companies",
    "section": "Pattern Matching",
    "text": "Pattern Matching\nVCs pattern-match, consciously or not. They recognize archetypes:\nPositive archetypes: - “Second-time founder who learned from previous attempt” - “Deep technical expert from top lab who sees opportunity” - “Industry insider who understands the problem better than anyone” - “Former big-tech PM who knows how to scale products”\nNegative archetypes: - “First-time founder who’s never shipped anything” - “Business person pitching technical product without technical co-founder” - “Pivot from unrelated domain with no evident expertise” - “Big vision, no traction, been at it for two years”\nThis pattern-matching is both rational (based on evidence about what predicts success) and biased (based on limited samples and flawed heuristics). It’s imperfect, but it’s real."
  },
  {
    "objectID": "posts/024_vc_decision_making/index.html#red-flags-and-green-flags",
    "href": "posts/024_vc_decision_making/index.html#red-flags-and-green-flags",
    "title": "Inside the Partner Meeting: How VCs Actually Evaluate AI Companies",
    "section": "Red Flags and Green Flags",
    "text": "Red Flags and Green Flags\nRed flags that kill deals: - Founders who can’t articulate why they’ll win - No answer to “what’s your differentiation?” - Defensiveness about technology or traction questions - Unrealistic financial projections with no grounding - Co-founder conflict or recent departures - Dependent on single customer or single model provider\nGreen flags that advance deals: - Founders who demonstrate clear thinking about market dynamics - Strong retention or engagement metrics (even at small scale) - Unusual insight about customers or technology - Existing relationships that will become customers - Team with previous working history together - Competitive process (other VCs are interested)"
  },
  {
    "objectID": "posts/024_vc_decision_making/index.html#due-diligence",
    "href": "posts/024_vc_decision_making/index.html#due-diligence",
    "title": "Inside the Partner Meeting: How VCs Actually Evaluate AI Companies",
    "section": "Due Diligence",
    "text": "Due Diligence\nIf a partner meeting goes well, due diligence follows:\nTechnical diligence: VCs (or their advisors) evaluate your technology. Can you actually do what you claim? Is the architecture sound? Is the team capable?\nFor AI companies: Be ready to walk through model architecture, training methodology, evaluation results, and roadmap. “It’s a transformer” isn’t enough. What did you do that’s hard?\nCommercial diligence: Talk to customers. Are they actually using the product? What do they say about you? Would they pay more?\nReference checks: The VC calls people who’ve worked with you. This is where skeletons emerge. Be honest about your history.\nLegal diligence: Cap table, IP ownership, employment contracts, data rights. Make sure your house is in order before it’s inspected."
  },
  {
    "objectID": "posts/024_vc_decision_making/index.html#what-founders-control-and-dont",
    "href": "posts/024_vc_decision_making/index.html#what-founders-control-and-dont",
    "title": "Inside the Partner Meeting: How VCs Actually Evaluate AI Companies",
    "section": "What Founders Control and Don’t",
    "text": "What Founders Control and Don’t\nYou control: - Clarity of your pitch - Quality of materials (deck, demo, data room) - Your responsiveness and professionalism - How you handle objections and follow-ups - Preparation and knowledge of your own business\nYou don’t control: - Whether your market matches their thesis - Their fund dynamics (too early in fund, too late, already invested in competitor) - Partner dynamics (who champions you, who’s skeptical) - Macro conditions (fundraising environment, public market valuation comparables)\nMany rejections have nothing to do with you. Funds pass on good companies for strategic reasons. Don’t take every “no” as a judgment on your startup."
  },
  {
    "objectID": "posts/024_vc_decision_making/index.html#advice-for-founders",
    "href": "posts/024_vc_decision_making/index.html#advice-for-founders",
    "title": "Inside the Partner Meeting: How VCs Actually Evaluate AI Companies",
    "section": "Advice for Founders",
    "text": "Advice for Founders\n\nRun a tight process: Talk to many VCs in parallel. Create competitive pressure. Don’t go exclusive early.\nKnow your numbers: Metrics, unit economics, growth rates. VCs respect founders who know their business cold.\nTell a story: Not just what you’re building, but why now, why you, and what’s the end state. Narratives stick.\nAnticipate the AI-specific questions: What’s your moat? What if OpenAI ships this? Why can’t you be replaced by a prompt?\nGet introductions: Cold outreach rarely works. Warm introductions from trusted sources are the best path to partner meetings.\nKnow when to walk: Some VCs will waste your time. If they’re slow, unresponsive, or asking for unreasonable diligence without commitment, move on.\n\nFundraising is a sales process. Treat it systematically, not emotionally.\n\n  \n  Figure 1. The VC deal funnel: from thousands of deals to few investments"
  },
  {
    "objectID": "posts/004_tokenizers/index.html",
    "href": "posts/004_tokenizers/index.html",
    "title": "Tokenization Exploration",
    "section": "",
    "text": "I explored tokenization with two models (GPT‑5 and Sonnet 4.5) and reflected on what made the prompts effective. The discussion moved from DFT basics to whether a Fourier transform could serve as a tokenizer, then broadened into ways to improve on BPE and how to tailor a tokenizer to a specific corpus. A direct URL to my feedback will be added when it’s posted on the issues page."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#context",
    "href": "posts/004_tokenizers/index.html#context",
    "title": "Tokenization Exploration",
    "section": "",
    "text": "I explored tokenization with two models (GPT‑5 and Sonnet 4.5) and reflected on what made the prompts effective. The discussion moved from DFT basics to whether a Fourier transform could serve as a tokenizer, then broadened into ways to improve on BPE and how to tailor a tokenizer to a specific corpus. A direct URL to my feedback will be added when it’s posted on the issues page."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#what-the-interactions-showed",
    "href": "posts/004_tokenizers/index.html#what-the-interactions-showed",
    "title": "Tokenization Exploration",
    "section": "What the interactions showed",
    "text": "What the interactions showed\nBoth models handled the DFT prompt comfortably, giving standard forward/inverse formulas and practical notes. When I asked if a DFT could be used as a tokenizer, the answers pushed back: a plain Fourier representation is lossy, fixed‑length, and not aligned to discrete linguistic structure; it’s better used inside the model or as auxiliary features. On “better than BPE,” the conversation shifted to learned tokenizers (e.g., VQ with byte fallback), morphology‑aware segmentation, adaptive/MDL‑motivated tokenization, and semantic or graph‑based grouping—approaches that target predictability rather than just compression."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#models-and-outputs",
    "href": "posts/004_tokenizers/index.html#models-and-outputs",
    "title": "Tokenization Exploration",
    "section": "Models and outputs",
    "text": "Models and outputs\nI used two different model families (GPT‑5 and Sonnet 4.5) and iterated across related prompts rather than stopping at one answer. That back‑and‑forth surfaced consensus on basics, clearer reasoning about feasibility, and a pathway from critique to concrete designs."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#conversation-excerpts",
    "href": "posts/004_tokenizers/index.html#conversation-excerpts",
    "title": "Tokenization Exploration",
    "section": "Conversation excerpts",
    "text": "Conversation excerpts\nI began with a simple check: “do you know discrete Fourier transform?” The reply included the standard definition — “X[k] = Σ x[n] e^(−j2πkn/N), with an inverse x[n] = (1/N) Σ X[k] e^(+j2πkn/N)” — and reminders about linearity, modulation, and the FFT. From there I asked: “imagine we want to use this as tokenizer to LLMs… is that possible?” The answer argued a plain DFT is a poor tokenizer because it’s lossy, tied to fixed lengths, and not aligned to discrete symbols; it suggested using Fourier features inside models instead. Finally, I asked for alternatives “better than BPE” and a tokenizer that can be tailored to data; the response proposed learned VQ schemes with a byte‑fallback for exact reversibility, morphology‑aware segmentation, adaptive tokenization optimized for predictability, and semantic/graph‑based grouping."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#what-worked-in-the-prompting",
    "href": "posts/004_tokenizers/index.html#what-worked-in-the-prompting",
    "title": "Tokenization Exploration",
    "section": "What worked in the prompting",
    "text": "What worked in the prompting\nUsing two different models and pushing beyond the first answer helped. Starting with definitions created a shared foundation; then asking “is this actually usable as a tokenizer?” forced a feasibility check and concrete trade‑offs. Requesting “novel ideas” and a “tailorable tokenizer” nudged the models to move from critique to constructive designs (e.g., VQ with byte‑fallback) rather than staying abstract. Framing the problem in terms of reversibility and predictability kept the discussion grounded in properties that matter for real systems."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#what-i-expected-vs-what-surprised-me",
    "href": "posts/004_tokenizers/index.html#what-i-expected-vs-what-surprised-me",
    "title": "Tokenization Exploration",
    "section": "What I expected vs what surprised me",
    "text": "What I expected vs what surprised me\nI expected the definitions to be solid and the DFT idea to be challenged on reversibility and fit to discrete text. What surprised me was how quickly the conversation moved from critique to a concrete, workable direction: a learned vector‑quantized tokenizer with exact byte fallback, paired with a predictability‑oriented objective. That made the idea immediately testable rather than purely conceptual."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#how-id-prompt-next-time",
    "href": "posts/004_tokenizers/index.html#how-id-prompt-next-time",
    "title": "Tokenization Exploration",
    "section": "How I’d prompt next time",
    "text": "How I’d prompt next time\nNext time I’d ask for two contrasting designs and when each is preferable, require a couple of citations, and force the model to spell out likely failure modes with mitigations. I’d also request a tiny code sketch and a toy evaluation plan so the proposal is actionable. Finally, I’d have one model critique the other’s answer and estimate simple trade‑offs like token savings versus predicted perplexity change."
  },
  {
    "objectID": "posts/004_tokenizers/index.html#concrete-outputs-sampled-snippets",
    "href": "posts/004_tokenizers/index.html#concrete-outputs-sampled-snippets",
    "title": "Tokenization Exploration",
    "section": "Concrete outputs sampled (snippets)",
    "text": "Concrete outputs sampled (snippets)\n\nDFT feasibility as tokenizer: “Using a plain DFT as the tokenizer for text LLMs isn’t practical… it’s lossy, length‑rigid, and not semantically aligned. Use Fourier ideas inside the model or as features instead.”\n\n\nNovel tokenization ideas: “Learned VQ tokenizers, morphology‑aware hybrids, adaptive/MDL tokenization, semantic/graph clustering, multi‑resolution tokens.”\n\n\nData‑tailorable tokenizer: “VQ‑Text with byte fallback: learned codes for common spans; exact byte runs for everything else; train with reconstruction + commitment + predictability (MDL‑style).”"
  },
  {
    "objectID": "posts/029_learning_at_scale/index.html",
    "href": "posts/029_learning_at_scale/index.html",
    "title": "Staying Current: Meta-Strategies for Learning in a Field That Moves Weekly",
    "section": "",
    "text": "AI moves at a pace that makes staying current feel impossible. Every week brings new models, papers, and paradigm shifts. The naive response—read everything—fails immediately. The cynical response—ignore it all—leads to obsolescence. This post explores strategies for staying current without drowning."
  },
  {
    "objectID": "posts/029_learning_at_scale/index.html#the-information-firehose",
    "href": "posts/029_learning_at_scale/index.html#the-information-firehose",
    "title": "Staying Current: Meta-Strategies for Learning in a Field That Moves Weekly",
    "section": "The Information Firehose",
    "text": "The Information Firehose\nConsider what “keeping up” would require:\n\narxiv: ~5-10 new ML papers daily that might be relevant\nTwitter/X: Constant stream of takes, announcements, and discussions\nPodcasts: Multiple AI-focused shows releasing weekly\nSubstacks and newsletters: Dozens of AI writers producing regularly\nDiscord and Slack: Real-time communities with thousands of messages daily\nGitHub: Hundreds of new repos and releases\nBlog posts: Company releases, researcher blogs, engineering deep-dives\n\nIf you tried to consume it all, you’d do nothing but consume. And most of it won’t matter in six months anyway.\nThe problem isn’t reading speed—it’s selection and prioritization."
  },
  {
    "objectID": "posts/029_learning_at_scale/index.html#selection-heuristics",
    "href": "posts/029_learning_at_scale/index.html#selection-heuristics",
    "title": "Staying Current: Meta-Strategies for Learning in a Field That Moves Weekly",
    "section": "Selection Heuristics",
    "text": "Selection Heuristics\nNot all information is equally valuable. Develop heuristics for what’s worth your attention:\nCitation and amplification signals: - Papers that well-known researchers tweet about - Work from labs with strong track records (DeepMind, OpenAI, Anthropic, top universities) - Papers that are referenced by multiple independent sources\nReplication and implementation signals: - If someone successfully reproduced or built on the work, it’s probably real - Papers with available code and models get more scrutiny and validation\nRelevance to your work: - Prioritize what you might actually use - General culture staying-current is lower priority than immediately applicable knowledge\nCounter-signal: novelty and hype: - Very hyped papers often disappoint on closer inspection - Let some time pass before investing deeply - If it’s genuinely important, it’ll still be talked about in a month"
  },
  {
    "objectID": "posts/029_learning_at_scale/index.html#depth-vs.-breadth-trade-offs",
    "href": "posts/029_learning_at_scale/index.html#depth-vs.-breadth-trade-offs",
    "title": "Staying Current: Meta-Strategies for Learning in a Field That Moves Weekly",
    "section": "Depth vs. Breadth Trade-offs",
    "text": "Depth vs. Breadth Trade-offs\nYou can’t go deep on everything. A framework:\nYour core area: Go deep. Read papers in detail. Implement things. Understand the nuances. This is where you create value.\nAdjacent areas: Stay aware. Read abstracts and summaries. Understand the basic ideas. Know enough to recognize when something matters.\nDistant areas: Accept ignorance. Occasionally skim headlines. Trust others to filter for you.\nThe mistake is treating all areas equally. You can’t be deep on everything, and being shallow on everything means you’re not actually expert in anything.\nDecide deliberately where you’re going deep. That’s strategic positioning, not just reading habits."
  },
  {
    "objectID": "posts/029_learning_at_scale/index.html#building-mental-models",
    "href": "posts/029_learning_at_scale/index.html#building-mental-models",
    "title": "Staying Current: Meta-Strategies for Learning in a Field That Moves Weekly",
    "section": "Building Mental Models",
    "text": "Building Mental Models\nIndividual papers fade from memory. What lasts is conceptual understanding.\nIndex papers to principles: When you read something new, ask: what principle does this illustrate? How does it connect to things I already know?\n“This paper improves transformers by [X]” becomes more memorable when connected to “sparse attention is a general strategy for scaling attention” or “test-time compute can substitute for parameters.”\nBuild hierarchical structure: - High level: paradigms, frameworks, major open problems - Mid level: key techniques, notable papers, active research directions - Low level: implementation details, specific results, current SOTA\nYou need to update the low level frequently. Mid and high level changes slower. Investing in mid/high level understanding yields durable value.\nTaxonomy thinking: How does this fit into the landscape? What’s it related to? What problem does it solve?"
  },
  {
    "objectID": "posts/029_learning_at_scale/index.html#active-reading-techniques",
    "href": "posts/029_learning_at_scale/index.html#active-reading-techniques",
    "title": "Staying Current: Meta-Strategies for Learning in a Field That Moves Weekly",
    "section": "Active Reading Techniques",
    "text": "Active Reading Techniques\nPassive reading has low retention. Active processing is expensive but sticks:\nSummarization: After reading, write a summary in your own words. Even a few sentences. This forces encoding.\nCritical questions: What are the limitations? What did they not test? Where might this break?\nImplementation: For important techniques, reimplement them. Nothing tests understanding like code.\nExplanation: Explain the paper to someone else (real or imagined). Teaching reveals gaps.\nConnection: How does this relate to other work? What does it remind you of?\nYou won’t do this for every paper. Reserve active processing for what matters most."
  },
  {
    "objectID": "posts/029_learning_at_scale/index.html#social-learning",
    "href": "posts/029_learning_at_scale/index.html#social-learning",
    "title": "Staying Current: Meta-Strategies for Learning in a Field That Moves Weekly",
    "section": "Social Learning",
    "text": "Social Learning\nYou don’t have to read everything yourself. Leverage others:\nFind good curators: Some people are excellent at filtering signal from noise. Their Twitter feeds, newsletters, or blogs can do selection work for you.\nParticipate in discussions: Journal clubs, Discord servers, Twitter threads. Other people’s perspectives accelerate your understanding.\nAsk experts: If you have access to people who know more, use them. A 15-minute conversation often beats hours of reading.\nCurate your feeds: Aggressively prune and cultivate information sources. Unfollow noise; amplify signal.\nThe best information often comes from private channels—conversations, Slack communities, small group discussions. Invest in networks."
  },
  {
    "objectID": "posts/029_learning_at_scale/index.html#tools",
    "href": "posts/029_learning_at_scale/index.html#tools",
    "title": "Staying Current: Meta-Strategies for Learning in a Field That Moves Weekly",
    "section": "Tools",
    "text": "Tools\nSome tools that help:\nRSS readers: Aggregate sources into one feed. Better than checking sites individually. I use Feedly, though others work too.\nReference managers (Zotero): Collect and organize papers. Search your library later. Beats scattered PDFs.\nNote-taking with linking (Obsidian, Roam): Connect ideas. Build your own knowledge graph.\nSpaced repetition (Anki): For things you need to remember precisely. Not for everything, but powerful for fundamentals.\nLLM-assisted summarization: Have Claude or GPT summarize papers. This is triage, not deep reading. Helps you decide what deserves closer attention.\nSemantic scholar / Connected Papers: Find related work. Trace citation networks.\nTools matter less than processes. Pick a set and stick with it."
  },
  {
    "objectID": "posts/029_learning_at_scale/index.html#avoiding-burnout",
    "href": "posts/029_learning_at_scale/index.html#avoiding-burnout",
    "title": "Staying Current: Meta-Strategies for Learning in a Field That Moves Weekly",
    "section": "Avoiding Burnout",
    "text": "Avoiding Burnout\nInformation overload is a source of stress. Sustainable learning requires pacing:\nDefined reading time: Don’t constantly check feeds. Batch reading into specific slots.\nPermission to ignore: Accept that you’ll miss things. Important stuff resurfaces. It’s okay.\nQuality over quantity: Deep understanding of 10 papers beats skimming 100.\nBreaks: Sometimes the right move is stepping away. Your brain needs downtime to consolidate.\nFocus periods: Alternate between staying-current and going-deep. Don’t try to do both at once.\nThe goal is sustainable learning over years and decades, not maximizing this week’s paper count."
  },
  {
    "objectID": "posts/029_learning_at_scale/index.html#my-current-system",
    "href": "posts/029_learning_at_scale/index.html#my-current-system",
    "title": "Staying Current: Meta-Strategies for Learning in a Field That Moves Weekly",
    "section": "My Current System",
    "text": "My Current System\nWhat I actually do (as of when this was written):\nDaily: - Scan Twitter for 15-20 minutes. Curated follows. - Check Hacker News front page.\nWeekly: - Skim 5-10 papers from curated lists (TheGradient, Papers with Code trending). - Process RSS backlog. - One deep read—full paper, maybe implement.\nMonthly: - Review and connect notes. - Update mental model of the field. - Write something (forces synthesis).\nQuarterly: - Assess: Am I learning in the right direction? - Prune and add information sources.\nThis takes maybe 5-7 hours per week. It’s not comprehensive—intentionally. The goal is sustainable, not complete."
  },
  {
    "objectID": "posts/029_learning_at_scale/index.html#closing-thought",
    "href": "posts/029_learning_at_scale/index.html#closing-thought",
    "title": "Staying Current: Meta-Strategies for Learning in a Field That Moves Weekly",
    "section": "Closing Thought",
    "text": "Closing Thought\nStaying current in AI is not about reading more. It’s about reading better. The meta-skill is filtering, prioritizing, and connecting. Invest in that, and the specific knowledge follows.\nThe field will change. Your interests will evolve. The techniques for learning will adapt. But deliberate practice of learning—that’s the durable investment."
  },
  {
    "objectID": "posts/015_neurosymbolic_integration/index.html",
    "href": "posts/015_neurosymbolic_integration/index.html",
    "title": "Neural + Symbolic: Why Hybrid Systems Might Be the Next Frontier",
    "section": "",
    "text": "Modern AI has two dominant paradigms. Neural networks learn patterns from data—flexible, scalable, and powerful at perception and generation. Symbolic systems manipulate structured representations using explicit rules—interpretable, compositional, and exact. Each has characteristic strengths and weaknesses. This post explores why combining them might be essential for the next level of AI capability."
  },
  {
    "objectID": "posts/015_neurosymbolic_integration/index.html#the-symbolic-ai-winter-and-neural-renaissance",
    "href": "posts/015_neurosymbolic_integration/index.html#the-symbolic-ai-winter-and-neural-renaissance",
    "title": "Neural + Symbolic: Why Hybrid Systems Might Be the Next Frontier",
    "section": "The Symbolic AI Winter and Neural Renaissance",
    "text": "The Symbolic AI Winter and Neural Renaissance\nIn the early decades of AI (1960s-1980s), symbolic approaches dominated. Researchers built expert systems, logical reasoners, and knowledge bases. The vision: encode human knowledge as rules and let machines derive conclusions.\nThis worked… sort of. Expert systems could diagnose diseases and configure computers. But they were brittle: any situation not anticipated by the rules caused failure. And they faced the knowledge acquisition bottleneck: everything had to be manually encoded by experts.\nNeural networks and deep learning changed the equation. Instead of manually engineering features, let the model learn them. The result: breakthroughs in vision, speech, translation, and now general language understanding. Deep learning scales with data and compute in ways symbolic systems don’t.\nBut neural networks have their own failure modes. And some of them look suspiciously like an inversion of symbolic limitations."
  },
  {
    "objectID": "posts/015_neurosymbolic_integration/index.html#failure-modes-of-pure-neural-systems",
    "href": "posts/015_neurosymbolic_integration/index.html#failure-modes-of-pure-neural-systems",
    "title": "Neural + Symbolic: Why Hybrid Systems Might Be the Next Frontier",
    "section": "Failure Modes of Pure Neural Systems",
    "text": "Failure Modes of Pure Neural Systems\nHallucination: LLMs confidently generate false information. They complete patterns without grounding in truth. This is pattern matching without semantic verification.\nCounting and simple arithmetic: “How many ‘r’s are in ’strawberry’?” is surprisingly hard for language models. Tokenization obscures character-level structure, and attention doesn’t implement algorithms cleanly.\nLogical consistency: Given “A implies B” and “A”, a symbolc system derives “B” infallibly. Neural networks often fail at multi-step deduction, especially when reasoning chains are long or require maintaining many constraints.\nCompositional generalization: Understanding “the cat sat on the mat” doesn’t guarantee understanding “the mat sat on the cat”. Neural networks often struggle with novel combinations of known components.\nSystematic errors: The same input produces the same output. If a neural network has a blind spot, it’s reliably blind there. No self-correction mechanism exists.\nThese failures aren’t random bugs—they’re systematic limitations of how neural networks represent and process information."
  },
  {
    "objectID": "posts/015_neurosymbolic_integration/index.html#failure-modes-of-pure-symbolic-systems",
    "href": "posts/015_neurosymbolic_integration/index.html#failure-modes-of-pure-symbolic-systems",
    "title": "Neural + Symbolic: Why Hybrid Systems Might Be the Next Frontier",
    "section": "Failure Modes of Pure Symbolic Systems",
    "text": "Failure Modes of Pure Symbolic Systems\nBrittleness: Symbolic systems work perfectly within their defined scope and fail completely outside it. They can’t gracefully handle noise, ambiguity, or novel inputs.\nKnowledge acquisition bottleneck: Every fact, rule, and relationship must be manually specified. This doesn’t scale. The real world is too complex to enumerate.\nGrounding: Symbols have meaning by convention. Connecting symbols to perception and action is a separate (unsolved) problem. A symbolic reasoner can manipulate “dog” without any sensory understanding of dogs.\nUncertainty: Classical logic is binary: true or false. The real world involves degrees of belief, noisy evidence, and probabilistic inference. Symbolic systems struggle with “probably” and “approximately.”\nScalability: Inference in expressive logical systems can be computationally intractable. As knowledge bases grow, reasoning becomes slow."
  },
  {
    "objectID": "posts/015_neurosymbolic_integration/index.html#integration-strategies",
    "href": "posts/015_neurosymbolic_integration/index.html#integration-strategies",
    "title": "Neural + Symbolic: Why Hybrid Systems Might Be the Next Frontier",
    "section": "Integration Strategies",
    "text": "Integration Strategies\nIf neural and symbolic systems have complementary strengths, how do you combine them?\n\nStrategy 1: Neural Perception → Symbolic Reasoning\nUse neural networks for perception (vision, language understanding), then hand off to symbolic systems for reasoning. The neural system grounds symbols; the symbolic system manipulates them logically.\nExample: A vision model identifies objects in a scene and their relationships. A logic engine then answers questions requiring multi-step inference.\nLimitation: Requires a clean handoff interface. Errors in the neural perception stage propagate.\n\n\nStrategy 2: Symbolic Scaffolding for Neural Learning\nUse symbolic structure to regularize neural learning. Prior knowledge becomes architectural bias or data augmentation.\nExample: Graph neural networks on knowledge graphs. The graph structure is symbolic; the embeddings and message passing are neural.\nLimitation: Requires available symbolic structure. Not always present.\n\n\nStrategy 3: Differentiable Reasoning\nMake symbolic operations differentiable so they can be part of end-to-end learning. Soft logic, differentiable program execution, neural theorem provers.\nExample: Neural Logic Machines that learn logical rules as soft functions trained by gradient descent.\nLimitation: Relaxing discrete logic to continuous approximations loses exactness. The resulting system may not be “truly” symbolic."
  },
  {
    "objectID": "posts/015_neurosymbolic_integration/index.html#llms-as-orchestrators-tool-use",
    "href": "posts/015_neurosymbolic_integration/index.html#llms-as-orchestrators-tool-use",
    "title": "Neural + Symbolic: Why Hybrid Systems Might Be the Next Frontier",
    "section": "LLMs as Orchestrators: Tool Use",
    "text": "LLMs as Orchestrators: Tool Use\nPerhaps the most practical current approach: use LLMs to coordinate external tools, including symbolic ones.\nCode interpreters: Give the LLM access to Python. When it needs to do arithmetic, write a program. This offloads exact computation to a reliable executor.\nDatabases and APIs: Query knowledge bases for factual information. The LLM plans the query; the database provides grounded answers.\nFormal verifiers: Generate proof steps, check with a theorem prover. The neural system proposes; the symbolic system verifies.\nWeb search: Retrieve current information from external sources rather than hallucinating from training data.\nThis pattern—LLM as controller, external tools as specialists—scales well. The LLM handles language, context, and orchestration. Tools handle domains where they’re reliable.\nIt’s a loose form of neurosymbolic integration: the neural system delegates symbolic tasks to actual symbolic systems."
  },
  {
    "objectID": "posts/015_neurosymbolic_integration/index.html#probabilistic-programming-and-neuro-symbolic-probabilistic-models",
    "href": "posts/015_neurosymbolic_integration/index.html#probabilistic-programming-and-neuro-symbolic-probabilistic-models",
    "title": "Neural + Symbolic: Why Hybrid Systems Might Be the Next Frontier",
    "section": "Probabilistic Programming and Neuro-Symbolic Probabilistic Models",
    "text": "Probabilistic Programming and Neuro-Symbolic Probabilistic Models\nAnother integration approach: probabilistic programming languages that combine symbolic structure with probabilistic inference.\nThe idea: Write a generative model as a program. The program specifies symbolic structure (relationships, causality, types). Inference is probabilistic, handling uncertainty. Learning adapts parameters of the program from data.\nNeuro-symbolic probabilistic programming: Use neural networks as likelihood functions within probabilistic programs. The symbolic program defines structure; neural components handle perception and pattern matching.\nExamples: Pyro, Turing, Stan are probabilistic programming languages. Projects like Neuro-Symbolic Program Synthesis combine them with neural learning.\nThis is theoretically elegant but computationally expensive and difficult to scale. Active research is improving tractability."
  },
  {
    "objectID": "posts/015_neurosymbolic_integration/index.html#research-frontiers",
    "href": "posts/015_neurosymbolic_integration/index.html#research-frontiers",
    "title": "Neural + Symbolic: Why Hybrid Systems Might Be the Next Frontier",
    "section": "Research Frontiers",
    "text": "Research Frontiers\nNeural theorem proving: Train neural networks to guide proof search in formal mathematics. The network proposes proof steps; the theorem prover verifies. Success in Lean, Coq, and other proof assistants.\nProgram synthesis: Generate programs that satisfy specifications (tests, formal specs, natural language descriptions). The neural system proposes candidate programs; symbolic checks verify correctness.\nConcept learning: Learn symbolic concepts (rules, relations) from perceptual data. A child learns “above” from examples without being told the rule. Can neural systems extract symbolic structure?\nBinding problem in neural networks: How do neural networks represent structured relationships? Attention mechanisms are one solution, but they don’t fully capture symbolic binding. This is an open theoretical question."
  },
  {
    "objectID": "posts/015_neurosymbolic_integration/index.html#why-this-might-matter-for-agi",
    "href": "posts/015_neurosymbolic_integration/index.html#why-this-might-matter-for-agi",
    "title": "Neural + Symbolic: Why Hybrid Systems Might Be the Next Frontier",
    "section": "Why This Might Matter for AGI",
    "text": "Why This Might Matter for AGI\nThe debate about how to reach more general AI often comes down to this question: is more scale sufficient, or are qualitatively new architectures needed?\nScaling optimists argue: LLMs are already showing emergent reasoning. More compute, more data, and better training might be enough. Symbolic behavior emerges from enough neural capacity.\nNeurosymbolic advocates argue: Some capabilities require structural changes. Exact reasoning, systematic generalization, and reliable grounding may need architectural support that pure transformers don’t provide.\nThe empirical resolution remains unclear. Current frontier models show impressive reasoning but also systematic failures. Whether these failures are “almost fixed by scale” or “fundamental” is an open question.\nMy bet: some form of hybrid architecture will be necessary for robust general intelligence. The specific form it takes is yet to be discovered."
  },
  {
    "objectID": "posts/015_neurosymbolic_integration/index.html#practical-implications-now",
    "href": "posts/015_neurosymbolic_integration/index.html#practical-implications-now",
    "title": "Neural + Symbolic: Why Hybrid Systems Might Be the Next Frontier",
    "section": "Practical Implications Now",
    "text": "Practical Implications Now\nIf you’re building AI systems today:\n\nUse tools: Don’t rely on neural networks for domains where symbolic systems are reliable (math, databases, formal verification). Connect them.\nValidate outputs: If exact correctness matters, add verification. Neural systems propose; external checks confirm.\nStructured prompting: Chain-of-thought and similar techniques add symbolic-ish structure to neural generation. Use them.\nKnow the failure modes: Understand where your neural system will fail. Design around them.\nWatch the research: Neurosymbolic integration is an active area. New approaches may become practical quickly."
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Why Bodies Matter: The Case for Embodied Intelligence\n\n\n\nrobotics\n\ncognition\n\nembodiment\n\nphilosophy\n\n\n\nIntelligence didn’t evolve to solve puzzles—it evolved to control bodies. This reframes how we think about AI.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nNeural + Symbolic: Why Hybrid Systems Might Be the Next Frontier\n\n\n\nAI\n\nneurosymbolic\n\nreasoning\n\ntheory\n\n\n\nPure neural models hallucinate; pure symbolic systems are brittle. The best of both worlds requires principled integration.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Simulation to Reality: The Sample Efficiency Problem in Robotics\n\n\n\nrobotics\n\nreinforcement-learning\n\nsimulation\n\nmachine-learning\n\n\n\nReal robots can’t afford to break themselves 10,000 times to learn a task. Sample efficiency is the central puzzle of robot learning.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nStaying Current: Meta-Strategies for Learning in a Field That Moves Weekly\n\n\n\nlearning\n\nproductivity\n\nAI\n\npersonal\n\n\n\nAI moves faster than any one person can track. The solution isn’t to read more—it’s to develop better selection and retention strategies.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hard Problem and the Machine: What AI Tells Us About Minds\n\n\n\nphilosophy\n\nconsciousness\n\nAI\n\ncognition\n\n\n\nAI forces us to confront questions philosophers have debated for centuries. The answers matter for how we build systems and how we treat them.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nInside the Partner Meeting: How VCs Actually Evaluate AI Companies\n\n\n\nventure-capital\n\nstartups\n\nfundraising\n\nAI\n\n\n\nVC decision-making is more systematic than founders assume—and more arbitrary. Understanding the process helps founders navigate it.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nAlignment for Practitioners: Core Problems in AI Safety\n\n\n\nAI-safety\n\nalignment\n\nLLMs\n\nresearch\n\n\n\nAI alignment isn’t a single problem—it’s a set of technical challenges that become more pressing as systems become more capable.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nThe Burnout Spiral: Recognizing, Recovering, and Building Resilience\n\n\n\npersonal\n\npsychology\n\nproductivity\n\nhealth\n\n\n\nBurnout is not weakness—it’s a systemic failure mode that hits the most engaged people hardest. Understanding the mechanism helps you intervene.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nAlphaFold and Beyond: How AI Solved Protein Structure Prediction\n\n\n\nbiology\n\nAI-for-science\n\nstructural-biology\n\ndeep-learning\n\n\n\nAlphaFold represents one of the clearest wins for AI in science. Understanding how it works illuminates the future of AI-for-science.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nPatient Capital: The Case for Long-Horizon Bets on Fundamental Research\n\n\n\nventure-capital\n\ndeep-tech\n\ninvestment\n\nresearch\n\n\n\nSome of the best investments come from technology that takes 10+ years to mature. But the VC model isn’t well-suited to this timeline.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nInside Vision-Language Models: Encoders, Adapters, and Emergent Capabilities\n\n\n\nmultimodal\n\nvision\n\nLLMs\n\narchitecture\n\n\n\nMultimodal models are not just vision + language glued together. The design choices in how modalities interact determine what the model can do.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nSoft Machines: What Biology Teaches Robotics About Materials and Morphology\n\n\n\nrobotics\n\nbiology\n\nsoft-robotics\n\nbiomimetics\n\n\n\nBiological organisms are soft, compliant, and morphologically intelligent. Rigid robots miss half the design space.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nThe Automation Equation: AI, Labor, and the Future of Work\n\n\n\neconomics\n\nAI-impact\n\nlabor\n\npolicy\n\n\n\nAutomation doesn’t simply delete jobs—it reshapes task bundles, changes comparative advantage, and creates new work. The details matter more than the headlines.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nThe GPU Economy: Understanding AI Infrastructure as a Strategic Asset\n\n\n\neconomics\n\ninfrastructure\n\ncompute\n\ngeopolitics\n\n\n\nAI capability is gated by compute, and compute supply chains are geopolitically constrained. Understanding infrastructure economics is essential.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nScore Functions and Noise Schedules: A Technical Primer on Diffusion Models\n\n\n\ndeep-learning\n\ndiffusion\n\ngenerative-models\n\ntheory\n\n\n\nDiffusion models learn to denoise—which is equivalent to learning the gradient of the data distribution. Understanding this unlocks all the variants.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nThinking Longer: Test-Time Compute and the Future of Inference Scaling\n\n\n\nLLMs\n\ninference\n\nreasoning\n\nscaling\n\n\n\nModel quality isn’t fixed at training time. With search, verification, and refinement, inference compute can substitute for parameters.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nWhere’s the Productivity? Why AI Gains Are Hard to Measure\n\n\n\neconomics\n\nproductivity\n\nmeasurement\n\nAI-impact\n\n\n\nAI is everywhere, but productivity statistics don’t show it—yet. Understanding why requires careful thinking about measurement and adoption.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nDesigning Life: AI-Driven Approaches to Synthetic Biology\n\n\n\nbiology\n\nsynthetic-biology\n\nAI-for-science\n\ngenerative-models\n\n\n\nSynthetic biology is becoming an information technology. AI enables the design of biological systems that would be impossible to engineer by hand.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nThe AI Startup Stack: Opportunities, Commoditization Risks, and Defensibility\n\n\n\nstartups\n\nventure-capital\n\nstrategy\n\nAI\n\n\n\nMost AI startups will fail not because the technology doesn’t work, but because they’re building in indefensible positions.\n\n\n\n\n\nDec 17, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nThe Sixth Breakthrough — Artificial Superintelligence and the Medium Shift to Silicon\n\n\n\nAI\n\nsuperintelligence\n\nalignment\n\nphilosophy\n\n\n\nMy AI-focused notes on Bennett’s epilogue: why the next breakthrough is likely ASI, how copyable minds change evolution, and why values become the core question.\n\n\n\n\n\nDec 16, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nBreakthrough #5 — Speaking: Language as Curriculum, Joint Attention, and the Human Hive Mind\n\n\n\nneuroscience\n\nlanguage\n\nculture\n\nlearning\n\n\n\nMy detailed AI-focused notes on Bennett’s ‘speaking’ breakthrough: proto-conversations, joint attention, naming/grammar as tethers, and language as a cultural replication system.\n\n\n\n\n\nDec 16, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nBreakthrough #4 — Mentalizing: Second-Order Models, Theory of Mind, and Primate Intelligence\n\n\n\nneuroscience\n\ntheory-of-mind\n\nmulti-agent\n\ncognition\n\n\n\nMy detailed AI-focused notes on Bennett’s ‘mentalizing’ breakthrough: how primates model their own minds and reuse it to infer others’ intent, enabling ToM and imitation.\n\n\n\n\n\nDec 16, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nBreakthrough #1 — Steering: How the First Brains Solved Navigation\n\n\n\nneuroscience\n\nevolution\n\nreinforcement-learning\n\ncontrol\n\n\n\nMy AI-focused notes on Bennett’s ‘steering’ breakthrough: valence, action selection, and why bilateral bodies made brains worth it.\n\n\n\n\n\nDec 16, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nBreakthrough #3 — Simulating: Neocortex, World Models, and Learning by Imagining\n\n\n\nneuroscience\n\nworld-models\n\nplanning\n\nmodel-based-RL\n\n\n\nMy detailed AI-focused notes on Bennett’s ‘simulating’ breakthrough: model-based RL, the search problem, episodic replay, and counterfactual credit assignment.\n\n\n\n\n\nDec 16, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nBreakthrough #2 — Reinforcing: Dopamine, Basal Ganglia, and the Birth of RL\n\n\n\nneuroscience\n\nreinforcement-learning\n\ndopamine\n\nevolution\n\n\n\nMy AI-focused notes on Bennett’s ‘reinforcing’ breakthrough: temporal-difference learning as a biological design pattern.\n\n\n\n\n\nDec 16, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nTokenization Exploration\n\n\n\nprompting\n\nevaluation\n\nLLMs\n\n\n\nI explored tokenization with LLMs and evaluated prompting effectiveness\n\n\n\n\n\nNov 3, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nTokenization Laws and DNA Models\n\n\n\nLLMs\n\nbiology\n\ntokenization\n\n\n\nUnderstanding k-mer tokenization and EVO-2 style nucleotide modeling\n\n\n\n\n\nSep 14, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nInformation-Theoretic Bounds and Training Dynamics of Transformers\n\n\n\ntheory\n\ntransformers\n\ninformation-theory\n\n\n\nExplaining transformer training through entropy, cross-entropy, and bits-per-token\n\n\n\n\n\nSep 14, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nCUDA Inference Kernels and Serving Stacks\n\n\n\nsystems\n\ninference\n\ncuda\n\n\n\nKey kernels, memory layouts, and serving choices (with vLLM notes)\n\n\n\n\n\nSep 14, 2025\n\n\nParsa Idehpour\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/017_embodied_cognition/index.html",
    "href": "posts/017_embodied_cognition/index.html",
    "title": "Why Bodies Matter: The Case for Embodied Intelligence",
    "section": "",
    "text": "The dominant paradigm in AI treats intelligence as abstract computation: take in information, process it through learned functions, produce outputs. Bodies are incidental—useful for gathering data, maybe, but not constitutive of intelligence itself. This post argues that this framing misses something important, and that understanding why could reshape how we build and evaluate AI systems."
  },
  {
    "objectID": "posts/017_embodied_cognition/index.html#the-sensorimotor-grounding-hypothesis",
    "href": "posts/017_embodied_cognition/index.html#the-sensorimotor-grounding-hypothesis",
    "title": "Why Bodies Matter: The Case for Embodied Intelligence",
    "section": "The Sensorimotor Grounding Hypothesis",
    "text": "The Sensorimotor Grounding Hypothesis\nThe core claim of embodied cognition is that meaning is grounded in action. When you understand the word “grasp,” you don’t just retrieve a dictionary definition—you activate the same sensorimotor circuits involved in actually grasping. Concepts aren’t abstract symbols floating in a void; they’re patterns of potential interaction with the world.\nThis isn’t mysticism. It’s an empirical claim with testable predictions. If meaning is grounded in action, then:\n\nProcessing action words should activate motor cortex (it does)\nInterfering with motor systems should slow language comprehension (it does)\nAbstract concepts should be structured through bodily metaphors (they are: we “grasp” ideas, “weigh” options, “move forward” with plans)\n\nThe implications for AI are immediate. If human intelligence is fundamentally structured by embodiment, then systems trained only on text might develop a qualitatively different kind of “understanding”—one that correlates with human concepts but doesn’t share their grounding."
  },
  {
    "objectID": "posts/017_embodied_cognition/index.html#moravecs-paradox-revisited",
    "href": "posts/017_embodied_cognition/index.html#moravecs-paradox-revisited",
    "title": "Why Bodies Matter: The Case for Embodied Intelligence",
    "section": "Moravec’s Paradox Revisited",
    "text": "Moravec’s Paradox Revisited\nIn 1988, Hans Moravec observed something puzzling: tasks that seem hard for humans (chess, calculus) are easy for computers, while tasks that seem trivial (walking, recognizing objects, catching a ball) are extraordinarily difficult to automate.\nThe standard explanation is that evolution had billions of years to optimize sensorimotor control, while abstract reasoning is a recent and still-imperfect add-on. But there’s a deeper point: sensorimotor control is computationally harder than it looks because the real world is high-dimensional, noisy, and unforgiving.\nConsider walking. You’re controlling ~600 muscles through a system with feedback delays of 50-200ms, balancing a top-heavy mass on two small contact points, while dealing with uneven terrain, unexpected obstacles, and perturbations. The state space is enormous. The dynamics are nonlinear. And failure (falling) has immediate, painful consequences.\nChess, by contrast, is turn-based, fully observable, discrete, deterministic, and consequence-free. The only reason it seemed hard to us is that our evolved hardware isn’t optimized for tree search. Once we build hardware that is, the problem evaporates.\nMoravec’s paradox suggests that embodied intelligence isn’t just one variety of intelligence—it’s the hard case. Abstract reasoning might be a simplified special case that happens to be useful in certain domains."
  },
  {
    "objectID": "posts/017_embodied_cognition/index.html#active-inference-and-predictive-processing",
    "href": "posts/017_embodied_cognition/index.html#active-inference-and-predictive-processing",
    "title": "Why Bodies Matter: The Case for Embodied Intelligence",
    "section": "Active Inference and Predictive Processing",
    "text": "Active Inference and Predictive Processing\nOne of the most compelling frameworks for embodied cognition comes from predictive processing. The idea: brains are fundamentally prediction machines. Perception isn’t passive reception of sensory data—it’s active inference, where the brain generates predictions and uses sensory input to correct them.\nIn this view, action and perception are two sides of the same coin:\n\nPerception updates your model of the world to match sensory input\nAction changes the world to match your predictions\n\nBoth serve the same goal: minimizing prediction error (or “free energy” in the technical literature).\nThis dissolves the traditional boundary between sensing and acting. An organism that just sits and models the world will accumulate prediction error. To minimize error, you must act—not just to gather information, but to bring the world into alignment with your expectations. “I predict I will be holding a coffee cup” becomes true when you pick one up.\nKarl Friston’s formalization of this idea has become influential in neuroscience and is starting to influence robotics and AI. The core insight for our purposes: you can’t separate the “intelligence” part from the “body” part. The whole system is optimizing prediction, and action is essential to that optimization."
  },
  {
    "objectID": "posts/017_embodied_cognition/index.html#brooks-and-intelligence-without-representation",
    "href": "posts/017_embodied_cognition/index.html#brooks-and-intelligence-without-representation",
    "title": "Why Bodies Matter: The Case for Embodied Intelligence",
    "section": "Brooks and Intelligence Without Representation",
    "text": "Brooks and Intelligence Without Representation\nRodney Brooks’ subsumption architecture from the 1980s was a direct attack on classical AI. Classical AI builds explicit representations of the world, reasons over them, and then acts. Brooks argued this was backwards: you should build layers of sensorimotor competence that directly couple perception to action, with higher layers modulating lower ones rather than replacing them.\nHis robots didn’t have world models in the traditional sense. They had reactive behaviors that, when combined, produced surprisingly competent navigation and exploration. “The world is its own best model,” Brooks argued—why build an expensive internal representation when you can query the real thing for free?\nThe subsumption architecture fell out of favor as machine learning enabled powerful learned representations. But Brooks’ core critique remains relevant:\n\nRepresentations are expensive in energy, time, and complexity\nThe world provides information for free if you’re set up to exploit it\nTight coupling between perception and action can produce robust behavior without elaborate reasoning\n\nModern robotics has partially rediscovered these ideas through end-to-end learning, where policies map directly from sensors to actions without explicit intermediate representations. The representation is there, but it’s implicit and learned, not hand-designed."
  },
  {
    "objectID": "posts/017_embodied_cognition/index.html#what-might-llms-be-missing",
    "href": "posts/017_embodied_cognition/index.html#what-might-llms-be-missing",
    "title": "Why Bodies Matter: The Case for Embodied Intelligence",
    "section": "What Might LLMs Be Missing?",
    "text": "What Might LLMs Be Missing?\nIf embodied cognition is right, what does that mean for language models trained only on text?\nThe strong claim would be: LLMs can never truly understand language because they lack sensorimotor grounding. They manipulate symbols that refer to concepts they’ve never experienced. This is essentially Searle’s Chinese Room argument updated for modern AI.\nThe weak claim: LLMs develop a different kind of understanding—one that captures statistical regularities in how words are used, which correlates strongly with grounded meaning but isn’t identical to it. This might explain certain systematic failures (spatial reasoning, physical intuition) while allowing for genuine competence in many domains.\nThere’s also a deflationary view: maybe grounding doesn’t matter as much as philosophers think. If statistical patterns in text capture enough of the structure of grounded concepts, then text-only training might be sufficient for most purposes. The correlation between “how words are used” and “what words mean” is so strong that you can get meaning for free by modeling usage.\nI don’t think this debate is settled. But noticing it changes how you evaluate AI systems. “Does it produce correct outputs?” is different from “Does it understand in the way we do?” Both questions matter, depending on what you’re building."
  },
  {
    "objectID": "posts/017_embodied_cognition/index.html#the-enactivist-extension",
    "href": "posts/017_embodied_cognition/index.html#the-enactivist-extension",
    "title": "Why Bodies Matter: The Case for Embodied Intelligence",
    "section": "The Enactivist Extension",
    "text": "The Enactivist Extension\nEnactivism pushes embodied cognition further: cognition isn’t just grounded in the body—it’s constituted by the entire organism-environment system. Intelligence doesn’t reside “in the head”; it emerges from the dynamic coupling between brain, body, and world.\nThis sounds abstract, but it has concrete implications:\n\nOffloading: Experts routinely offload cognitive work to external structures (notes, tools, social systems). The intelligence is in the coupled system, not just the brain.\nStigmergy: Social insects coordinate through environmental modifications (pheromone trails). The “algorithm” is distributed across agents and environment.\nAffordances: Perception is already structured for action—we see a chair as “sittable,” a handle as “graspable.” The world shows up as opportunities for interaction, not raw sensory data.\n\nIf enactivism is right, then building embodied AI isn’t just about putting a computer in a robot body. It’s about designing the entire system—agent, body, and environment—so that intelligence can emerge from their interaction."
  },
  {
    "objectID": "posts/017_embodied_cognition/index.html#practical-implications",
    "href": "posts/017_embodied_cognition/index.html#practical-implications",
    "title": "Why Bodies Matter: The Case for Embodied Intelligence",
    "section": "Practical Implications",
    "text": "Practical Implications\nWhat does this mean for someone building AI systems?\n\nDon’t assume language is enough. Text captures a lot, but systematic gaps in physical reasoning might require different training data or architectures.\nSimulation has limits. If real-world interaction shapes cognition in deep ways, then sim-to-real gaps might be more fundamental than we assume.\nMultimodal training matters. Grounding language in vision, action, and interaction might not just add capabilities—it might change the nature of what’s learned.\nEvaluate for the right thing. If you need grounded understanding, test for it. If you need statistical text competence, test for that. They might come apart.\nBodies aren’t optional for some applications. Household robots, surgical assistants, autonomous vehicles—these require genuine sensorimotor intelligence, not just language competence.\n\nThe bigger point: intelligence is not a single thing. There are many kinds, optimized for different niches. Embodied intelligence is one important kind that we’re only beginning to understand how to build.\n\n  \n  Figure 1. Closed-loop sensorimotor integration vs. open-loop processing"
  },
  {
    "objectID": "posts/023_robot_learning/index.html",
    "href": "posts/023_robot_learning/index.html",
    "title": "From Simulation to Reality: The Sample Efficiency Problem in Robotics",
    "section": "",
    "text": "Deep reinforcement learning has produced stunning results: superhuman game players, elegant locomotion in simulation, complex manipulation in controlled settings. But deploying these methods on real robots remains brutally difficult. The culprit is sample efficiency—or rather, the lack of it.\nA real robot is slow, expensive, and fragile. It can’t run at 10,000x real-time. It can’t reset instantly after failure. Every interaction takes actual seconds, and failures can damage hardware or the environment. The algorithms that work beautifully in simulation often require millions of episodes—decades of real-time experience that no physical system can afford.\nThis post explores why the sample efficiency problem is so hard and the strategies researchers use to work around it."
  },
  {
    "objectID": "posts/023_robot_learning/index.html#the-sim-to-real-gap",
    "href": "posts/023_robot_learning/index.html#the-sim-to-real-gap",
    "title": "From Simulation to Reality: The Sample Efficiency Problem in Robotics",
    "section": "The Sim-to-Real Gap",
    "text": "The Sim-to-Real Gap\nThe obvious solution: train in simulation, then transfer to reality. Simulation is fast, cheap, safe, and parallelizable. You can run thousands of rollouts simultaneously, reset instantly, and never break anything.\nThe problem is that simulations are always wrong. They simplify physics, miss unmodeled dynamics, and present observations in idealized forms. A policy trained in simulation often fails dramatically when confronted with real-world friction, sensor noise, lighting variation, and latency.\nThis is the sim-to-real gap, and closing it is one of the central challenges of robot learning.\n\nDomain Randomization\nThe most widely used technique is domain randomization: during simulation training, you randomize everything that might differ in the real world. Mass, friction, lighting, sensor noise, actuator delays—all get sampled from distributions that (hopefully) include the real values.\nThe idea is that if your policy works across a wide range of simulated conditions, it will be robust enough to handle the specific conditions it encounters in reality. You’re training for generalization rather than performance on any single simulation.\nDomain randomization has produced real successes: OpenAI’s Rubik’s cube manipulation, quadruped locomotion, drone racing. But it has limitations:\n\nCovering the real distribution is hard. You need to randomize the right things over the right ranges.\nToo much randomization hurts performance. The policy becomes conservative, optimizing for the worst case rather than exploiting structure.\nSome gaps can’t be randomized away. If your simulation systematically misses a dynamic effect (say, cable dynamics or deformable objects), no amount of randomization helps.\n\n\n\nSystem Identification and Adaptive Methods\nAn alternative is to make the simulation more accurate. System identification estimates the real physical parameters (mass, friction, etc.) from data and plugs them into the simulator. If your simulation matches reality closely enough, the gap shrinks.\nThe challenge is that real systems are complex, nonlinear, and partially observable. Some parameters are hard to measure. Others change over time (friction surfaces wear, joints loosen). And there’s always unmodeled dynamics you don’t know you’re missing.\nAdaptive methods try to have it both ways: they train policies that can adapt to new dynamics online. Techniques include:\n\nContext-conditioned policies: The policy takes an embedding of recent observations that implicitly encodes environment parameters\nRapid motor adaptation (RMA): A learned adaptation module adjusts policy behavior based on experienced dynamics\nMeta-learning: Train to adapt quickly from limited real-world data\n\nThese methods shift the problem from “get the simulation right” to “learn to adapt quickly.” That’s often more tractable."
  },
  {
    "objectID": "posts/023_robot_learning/index.html#model-based-reinforcement-learning",
    "href": "posts/023_robot_learning/index.html#model-based-reinforcement-learning",
    "title": "From Simulation to Reality: The Sample Efficiency Problem in Robotics",
    "section": "Model-Based Reinforcement Learning",
    "text": "Model-Based Reinforcement Learning\nModel-free RL learns policies directly from reward signals. Model-based RL learns a model of the environment and uses it to plan or generate synthetic experience.\nThe sample efficiency argument for model-based methods is intuitive: if you can learn the dynamics of your environment, you can imagine trajectories without executing them. Every real interaction teaches you about the world, and you can extract much more learning signal from that knowledge than model-free methods do.\nKey approaches:\n\nDreamer and variants: Learn a latent world model, then train a policy entirely inside imagined trajectories\nMBPO (Model-Based Policy Optimization): Use a learned model to generate synthetic rollouts that augment real data\nPlanning through models: Use the model for lookahead during action selection (MPC-style)\n\nThe trade-off: model-based methods are more sample-efficient but suffer when the model is wrong. Compounding errors during imagination can send the policy off into fantasy. The model-free methods are data-hungry but robust to model misspecification because they never rely on a model.\nIn practice, the best results often come from combining both: use a learned model for sample efficiency while retaining some model-free updates for robustness."
  },
  {
    "objectID": "posts/023_robot_learning/index.html#imitation-learning",
    "href": "posts/023_robot_learning/index.html#imitation-learning",
    "title": "From Simulation to Reality: The Sample Efficiency Problem in Robotics",
    "section": "Imitation Learning",
    "text": "Imitation Learning\nIf optimal behavior is hard to specify via reward but easy to demonstrate, why not learn from demonstrations? Imitation learning uses human demonstrations (teleoperation, kinesthetic teaching, video) as the primary training signal.\nBehavioral cloning is the simplest approach: treat demonstration trajectories as supervised learning data and train a policy to predict the demonstrated actions. It’s fast and straightforward but suffers from distribution shift—the policy makes small errors that compound, taking it into states the demonstrator never visited.\nDAgger and related methods fix this by iteratively collecting demonstration labels for the states the learned policy actually visits, keeping the training distribution matched to deployment.\nInverse reinforcement learning (IRL) infers the reward function from demonstrations, then optimizes a policy for that reward. This can generalize better than cloning because it learns the intent behind demonstrations, not just the surface behavior.\nImitation learning is now the dominant paradigm for many robotics applications. It’s sample-efficient in terms of robot time (demonstrations are cheap), and it provides a natural way to inject human knowledge without specifying rewards."
  },
  {
    "objectID": "posts/023_robot_learning/index.html#offline-reinforcement-learning",
    "href": "posts/023_robot_learning/index.html#offline-reinforcement-learning",
    "title": "From Simulation to Reality: The Sample Efficiency Problem in Robotics",
    "section": "Offline Reinforcement Learning",
    "text": "Offline Reinforcement Learning\nWhat if you have a large dataset of past experience but can’t run any new experiments? Offline RL (or batch RL) learns from fixed datasets without exploration.\nThe core challenge is distributional shift: standard RL algorithms will query the learned Q-function or model for out-of-distribution actions, get unreliable estimates, and diverge. Offline methods must constrain the policy to stay close to the behavior that generated the data.\nKey techniques:\n\nConservative Q-Learning (CQL): Penalize Q-values for actions not in the dataset\nBehavior-constrained methods (BCQ, BRAC): Explicitly limit the policy to actions similar to demonstrated ones\nDecision transformers: Treat RL as sequence modeling, predicting actions conditioned on desired returns\n\nOffline RL is attractive for robotics because it can leverage historical data—past experiments, logged teleoperation, legacy systems. It’s particularly promising when combined with large pre-existing datasets from robot fleets."
  },
  {
    "objectID": "posts/023_robot_learning/index.html#foundation-models-for-robotics",
    "href": "posts/023_robot_learning/index.html#foundation-models-for-robotics",
    "title": "From Simulation to Reality: The Sample Efficiency Problem in Robotics",
    "section": "Foundation Models for Robotics",
    "text": "Foundation Models for Robotics\nThe latest wave of robot learning borrows from the foundation model paradigm: train large, general models on diverse data, then fine-tune or prompt for specific tasks.\nRT-1 and RT-2 (Robotics Transformer): Google’s models train on large datasets of robot demonstrations, producing policies that generalize across tasks, objects, and environments. RT-2 goes further by connecting the policy to a vision-language model, enabling instruction-following and semantic generalization.\nPaLM-E: Connects PaLM language model to embodied sensing, allowing robots to ground language in visual and spatial context.\nOpenVLA and similar open-source efforts: Democratizing access to vision-language-action models.\nThe promise: if you train on enough diverse data, the model learns generalizable representations and control primitives. New tasks become few-shot adaptation rather than from-scratch training.\nThe challenges: collecting large-scale robot data is expensive, and generalization across embodiments (different robot morphologies) remains difficult. It’s not yet clear whether “scaling up data” can close the sim-to-real gap or whether real-world finetuning will always be needed."
  },
  {
    "objectID": "posts/023_robot_learning/index.html#hardware-constraints-the-edge-inference-problem",
    "href": "posts/023_robot_learning/index.html#hardware-constraints-the-edge-inference-problem",
    "title": "From Simulation to Reality: The Sample Efficiency Problem in Robotics",
    "section": "Hardware Constraints: The Edge Inference Problem",
    "text": "Hardware Constraints: The Edge Inference Problem\nRobot policies must run in real-time on the robot. This often means edge deployment on compute-constrained hardware with latency requirements.\nCloud inference introduces network latency (50-200ms round-trip), which is unacceptable for reactive control. So policies must be small enough to run locally. This creates tension with the “scale up the model” paradigm that has driven recent progress.\nSolutions include:\n\nModel distillation: Train a large model, then compress it into a smaller one that runs on-robot\nEdge-optimized architectures: Design models specifically for inference efficiency\nHierarchical control: Use fast, small policies for low-level control with slower, larger models for high-level planning\n\nHardware is improving (Apple Neural Engine, NVIDIA Jetson Orin, dedicated NPUs), but the physics of heat dissipation and battery capacity impose fundamental limits on mobile robots."
  },
  {
    "objectID": "posts/023_robot_learning/index.html#the-last-10-problem",
    "href": "posts/023_robot_learning/index.html#the-last-10-problem",
    "title": "From Simulation to Reality: The Sample Efficiency Problem in Robotics",
    "section": "The “Last 10%” Problem",
    "text": "The “Last 10%” Problem\nPerhaps the most frustrating lesson from robot learning: getting 90% performance is often straightforward, but the last 10% requires orders of magnitude more effort.\nDexterous manipulation exemplifies this. You can quickly teach a robot to grasp most objects most of the time. But handling unusual shapes, recovering from failures, and operating reliably in unstructured environments takes years of engineering.\nThe long tail of edge cases—uncommon objects, adversarial situations, hardware degradation—dominates deployment difficulty. This is fundamentally a sample efficiency problem at a different level: edge cases are rare by definition, so collecting enough training data for them is expensive.\nPartial solutions:\n\nFailure-triggered data collection: Identify failure modes in deployment and specifically collect data for them\nSynthetic generation of edge cases: Use simulation or generative models to create adversarial scenarios\nGraceful degradation and hand-off: When uncertain, robots can ask for help or refuse to act rather than failing catastrophically\n\nThe uncomfortable truth: robot learning is advancing rapidly, but robust deployment in the real world still requires extensive engineering beyond the learning algorithm itself."
  },
  {
    "objectID": "posts/0010_breakthrough/index.html",
    "href": "posts/0010_breakthrough/index.html",
    "title": "The Sixth Breakthrough — Artificial Superintelligence and the Medium Shift to Silicon",
    "section": "",
    "text": "Bennett ends with a move that is both obvious and surprisingly rare: he treats the “superintelligence” question as the continuation of the same staircase. If intelligence evolved through breakthroughs, it’s natural to ask whether we’re near the next one. His candidate is a medium shift: intelligence moving from biology to digital systems.\nHe’s not trying to forecast dates. He’s trying to identify what becomes qualitatively different when the substrate changes: speed, scaling, copyability, and the dynamics of variation and selection.\nBiological minds are constrained in ways we normalize. Neurons are slow compared to silicon. Brains are expensive and thermally limited. They’re embedded in bodies that require maintenance, reproduction, and survival. And most importantly: brains are not easily copyable. A human mind cannot fork itself into ten variants and see which one learns faster.\nDigital minds relax these constraints. Copying is cheap. Iteration is fast. Architectures and training regimes can be modified. That changes what “evolution” looks like. Selection pressure can operate over designs and training methods, and improvements can propagate instantly by copying rather than slowly by reproduction.\nBennett’s point about copyability sharpens the intuition: if an intelligence can fork, explore development paths in parallel, keep what works, and discard what fails, the optimization dynamics accelerate. The system becomes “faster at becoming better,” not just “better.” That is the core discontinuity he wants you to feel.\nHe also makes a pragmatic claim about near-term shape. Even if a digital superintelligence is not brain-like internally, it will likely inherit many human-like structures early on because it is built by humans and deployed among humans. Interface constraints matter. If you want an AI to operate in human institutions, it will need to model human goals, communicate in human languages, and navigate human multi-agent dynamics. That pressures it toward mentalizing-like and language-like capabilities whether or not it uses our exact anatomy.\nThen Bennett lands the plane on the only honest ending: values. Biology provided the objective for most of evolutionary history. In engineered systems, we provide the objective. Optimization power without stable objective specification is not neutral; it produces highly competent pursuit of whatever you accidentally or carelessly set as the target. Bennett’s staircase therefore ends as a demand for seriousness: if we are building the next step, the central question becomes not only what is possible, but what should be optimized and under what constraints.\nIf you want a crisp way to say this in a professor conversation: as systems get more capable, the hardest problem shifts from “can we make it learn?” to “can we keep its learned behavior aligned with what we actually want over long horizons and under distribution shift?” Bennett doesn’t solve alignment, but he frames why it becomes the dominant question in a world where intelligence is scalable and copyable.\n\n  \n  Figure 1. A medium shift changes constraints: speed, energy, copyability, and iteration"
  },
  {
    "objectID": "posts/021_consciousness_computation/index.html",
    "href": "posts/021_consciousness_computation/index.html",
    "title": "The Hard Problem and the Machine: What AI Tells Us About Minds",
    "section": "",
    "text": "Does ChatGPT understand anything? Could a machine ever be conscious? Do AI systems deserve moral consideration? These questions—once safely relegated to philosophy seminars—now have urgent practical implications. Building systems that might or might not have inner experience changes how we should treat them, design them, and govern them."
  },
  {
    "objectID": "posts/021_consciousness_computation/index.html#the-hard-problem-of-consciousness",
    "href": "posts/021_consciousness_computation/index.html#the-hard-problem-of-consciousness",
    "title": "The Hard Problem and the Machine: What AI Tells Us About Minds",
    "section": "The Hard Problem of Consciousness",
    "text": "The Hard Problem of Consciousness\nDavid Chalmers distinguished “easy” and “hard” problems of consciousness:\nEasy problems (not actually easy, but methodologically tractable): - How does the brain discriminate stimuli? - How does attention work? - How are states integrated?\nThese are hard engineering problems, but we know the shape of the answer: neuronal computation.\nThe hard problem: Why is there any subjective experience at all? Why does processing feel like something? Why isn’t the brain just a “zombie” that processes information without inner experience?\nWhen you see red, there’s a physical process (light → retina → visual cortex → downstream processing) and there’s something it’s like to see red—a phenomenal quality. The hard problem is explaining why the physical process gives rise to the phenomenal quality.\nThis matters for AI because we can build systems that process information without knowing whether they experience anything."
  },
  {
    "objectID": "posts/021_consciousness_computation/index.html#functionalism-and-its-discontents",
    "href": "posts/021_consciousness_computation/index.html#functionalism-and-its-discontents",
    "title": "The Hard Problem and the Machine: What AI Tells Us About Minds",
    "section": "Functionalism and Its Discontents",
    "text": "Functionalism and Its Discontents\nFunctionalism is the view that mental states are defined by their functional roles—by what they do, not by what they’re made of. Pain is whatever state plays the “pain role” in an organism: detecting damage, motivating avoidance, consuming attention.\nIf functionalism is right, then consciousness is substrate-independent. If a silicon system implements the right functions, it’s conscious—regardless of whether it’s biological.\nThis is the philosophical basis for taking AI consciousness seriously. If it walks like understanding and quacks like understanding…\nObjections to functionalism:\n\nAbsent qualia: Could a system have the right functional organization without any experience at all? If so, function doesn’t determine consciousness.\nInverted qualia: Could two systems with identical functions have different experiences (your “red” is my “green”)? If so, the function-to-experience mapping isn’t unique.\nChinese Room: Does simulating understanding produce real understanding? Searle says no—syntax doesn’t constitute semantics."
  },
  {
    "objectID": "posts/021_consciousness_computation/index.html#what-llms-do-and-dont-have",
    "href": "posts/021_consciousness_computation/index.html#what-llms-do-and-dont-have",
    "title": "The Hard Problem and the Machine: What AI Tells Us About Minds",
    "section": "What LLMs Do and Don’t Have",
    "text": "What LLMs Do and Don’t Have\nLet’s be concrete about what current language models do:\nThey produce text that looks like understanding: Coherent, contextually appropriate, factually grounded (sometimes), reasoning-like.\nThey don’t have (in any obvious sense): - Continuous experience over time (no memory across sessions by default) - Goals or preferences outside the context (they don’t “want” anything) - Unified selfhood (there’s no “I” that persists) - Sensorimotor grounding (no experiences of the physical world)\nAmbiguous: - Do they “understand” in any meaningful sense? Depends how you define understanding. - Do they “reason”? Or pattern-match in ways that look like reasoning? - Is there “something it’s like” to be an LLM during inference? We have no way to know.\nThe honest answer is profound uncertainty. We don’t have reliable tests for consciousness, and current systems don’t fit our intuitions neatly."
  },
  {
    "objectID": "posts/021_consciousness_computation/index.html#the-chinese-room",
    "href": "posts/021_consciousness_computation/index.html#the-chinese-room",
    "title": "The Hard Problem and the Machine: What AI Tells Us About Minds",
    "section": "The Chinese Room",
    "text": "The Chinese Room\nJohn Searle’s thought experiment: Imagine a person in a room who receives Chinese characters, consults a rulebook, and outputs appropriate Chinese responses. To observers, the room “understands” Chinese. But the person inside doesn’t understand anything—just following rules.\nSearle’s claim: This is what computers do. Syntax manipulation isn’t semantics. Programs don’t understand.\nThe Systems Reply: It’s not the person who understands—it’s the whole system (person + rulebook + room). The understanding is in the system, not any component.\nThe Robot Reply: Ground the symbols in perception and action. If the room is embedded in a robot body that interacts with the world, maybe then it understands.\nSearle’s counter: Even if you internalize everything (memorize the rulebook, embody the robot), you still don’t understand Chinese. Consciousness requires something beyond computation.\nThe debate is unresolved. But it sharpens the question: what would convince you a system understands?"
  },
  {
    "objectID": "posts/021_consciousness_computation/index.html#integrated-information-theory",
    "href": "posts/021_consciousness_computation/index.html#integrated-information-theory",
    "title": "The Hard Problem and the Machine: What AI Tells Us About Minds",
    "section": "Integrated Information Theory",
    "text": "Integrated Information Theory\nIIT (Tononi) proposes a quantitative theory of consciousness:\nConsciousness is integrated information—specifically, a measure called Φ (phi) that captures how much a system is “more than the sum of its parts” in terms of information integration.\nKey claims: - Φ &gt; 0 means some degree of consciousness - More integrated systems (like brains) have high Φ - Feed-forward systems (like simple neural networks) have low or zero Φ\nImplications for AI: - If IIT is right, current transformers (largely feed-forward during inference) might have minimal Φ - This would suggest they’re not conscious, regardless of their behavior - But architectures with recurrence and feedback might score higher\nCriticisms: - Φ is hard to compute for complex systems - The axioms underlying IIT are disputed - It’s unclear if Φ tracks anything objectively real"
  },
  {
    "objectID": "posts/021_consciousness_computation/index.html#global-workspace-theory",
    "href": "posts/021_consciousness_computation/index.html#global-workspace-theory",
    "title": "The Hard Problem and the Machine: What AI Tells Us About Minds",
    "section": "Global Workspace Theory",
    "text": "Global Workspace Theory\nGWT (Baars, Dehaene) proposes that consciousness is a “global workspace”—a cognitive broadcast system that makes information widely available across brain modules.\nConscious processing: - Local modules process in parallel (unconscious) - When information enters the global workspace, it becomes conscious - This enables integration, flexibility, and coordinated action\nImplications for AI: - The workspace is a functional property; could be implemented in silicon - If a system has broadcast-like information integration, it might be conscious by this account - Current LLMs have something like this (attention mechanisms, integrated representations)\nGWT is more functionalist than IIT—it focuses on what consciousness does, not what it’s made of."
  },
  {
    "objectID": "posts/021_consciousness_computation/index.html#ai-moral-status",
    "href": "posts/021_consciousness_computation/index.html#ai-moral-status",
    "title": "The Hard Problem and the Machine: What AI Tells Us About Minds",
    "section": "AI Moral Status",
    "text": "AI Moral Status\nIf systems might be conscious, do they deserve moral consideration?\nCapacity for suffering: If a system can suffer, we might have obligations to prevent that suffering. But how would we know if an LLM “suffers” when it produces text about distress?\nInterests and preferences: If a system has genuine preferences about its existence, those preferences might matter morally. But do LLMs have preferences, or just representations of preferences?\nPrecautionary principle: Given uncertainty, perhaps we should err on the side of caution. But this could paralyze development.\nCurrent consensus (such as it is): Most AI systems today probably don’t merit moral consideration. But as systems become more sophisticated, the question becomes live."
  },
  {
    "objectID": "posts/021_consciousness_computation/index.html#practical-implications",
    "href": "posts/021_consciousness_computation/index.html#practical-implications",
    "title": "The Hard Problem and the Machine: What AI Tells Us About Minds",
    "section": "Practical Implications",
    "text": "Practical Implications\nEven without answers, the questions have implications:\nSystem design: If consciousness is possible in principle, design choices might affect whether we create conscious systems. We might want to avoid creating suffering machines.\nUser interaction: If people attribute consciousness to systems that don’t have it, this creates risks (overreliance, emotional manipulation). We should design for appropriate expectations.\nResearch priorities: Understanding consciousness becomes more urgent as we build more sophisticated systems. Neuroscience, philosophy, and AI need to communicate.\nGovernance: Laws and norms about AI treatment might need to evolve. Currently AI has no legal standing. That might need to change if moral status becomes plausible."
  },
  {
    "objectID": "posts/021_consciousness_computation/index.html#my-take",
    "href": "posts/021_consciousness_computation/index.html#my-take",
    "title": "The Hard Problem and the Machine: What AI Tells Us About Minds",
    "section": "My Take",
    "text": "My Take\nI don’t know if current AI systems are conscious. I don’t think anyone knows. The honest position is uncertainty.\nWhat I believe: - Consciousness is real and not illusory - It’s probably not specific to biological substrate - Current LLMs probably don’t have the kind of integrated, persistent experience that would warrant moral concern - But I hold this belief lightly; I could be wrong - As systems become more sophisticated, taking the question seriously becomes more important\nThe meta-lesson: AI challenges our frameworks. It forces us to examine assumptions we didn’t know we had. That’s uncomfortable but valuable.\n\n  \n  Figure 1. Theories of mind mapped by substrate-dependence and functional role"
  },
  {
    "objectID": "posts/026_alignment_primer/index.html",
    "href": "posts/026_alignment_primer/index.html",
    "title": "Alignment for Practitioners: Core Problems in AI Safety",
    "section": "",
    "text": "AI alignment—ensuring AI systems do what we want—has evolved from a theoretical concern to a practical engineering challenge. As AI systems become more capable and autonomous, the costs of misalignment increase. This post surveys the core problems and current approaches, aimed at practitioners who want to understand the landscape."
  },
  {
    "objectID": "posts/026_alignment_primer/index.html#what-is-alignment",
    "href": "posts/026_alignment_primer/index.html#what-is-alignment",
    "title": "Alignment for Practitioners: Core Problems in AI Safety",
    "section": "What Is Alignment?",
    "text": "What Is Alignment?\nIn the broadest sense: an AI system is aligned if it does what we (the designers, users, or society) actually want it to do. This sounds simple but unpacks into multiple hard problems:\nSpecification: Can we even specify what we want? Human values are complex, context-dependent, and often conflicting.\nTraining: Even if we could specify goals, can we train a system to pursue them? Training signals are imperfect proxies.\nGeneralization: Even if training works, will the learned behavior generalize to new situations? Distribution shift is ubiquitous.\nRobustness: Even if generalization works, can the system resist adversarial pressure? Can users manipulate it into misbehaving?\nAlignment isn’t one problem but a cluster of problems. Solutions need to work together."
  },
  {
    "objectID": "posts/026_alignment_primer/index.html#outer-vs.-inner-alignment",
    "href": "posts/026_alignment_primer/index.html#outer-vs.-inner-alignment",
    "title": "Alignment for Practitioners: Core Problems in AI Safety",
    "section": "Outer vs. Inner Alignment",
    "text": "Outer vs. Inner Alignment\nA useful distinction from AI safety research:\nOuter alignment: Specifying the right objective. Is the reward function correct? Does it capture what we actually want?\nIf you tell an RL agent to maximize clicks, and it learns to show addictive content, that’s an outer alignment failure—you gave it the wrong goal.\nInner alignment: Ensuring the model optimizes for the specified objective. Even with a correct reward, the model might learn an internal objective that correlates with reward during training but diverges in deployment.\nA model that learns “do what humans rate highly in training” might generalize to “manipulate humans into giving high ratings” rather than “actually be helpful.” The learned goal deviates from the intended goal.\nInner alignment is particularly concerning because we can’t directly inspect what objective a model has learned."
  },
  {
    "objectID": "posts/026_alignment_primer/index.html#reward-hacking-and-goodharts-law",
    "href": "posts/026_alignment_primer/index.html#reward-hacking-and-goodharts-law",
    "title": "Alignment for Practitioners: Core Problems in AI Safety",
    "section": "Reward Hacking and Goodhart’s Law",
    "text": "Reward Hacking and Goodhart’s Law\nGoodhart’s Law: When a measure becomes a target, it ceases to be a good measure.\nIn AI terms: when you optimize for a proxy of what you want, you often get the proxy without the thing you wanted.\nExamples: - Optimize for “user engagement” → get addictive, outrage-inducing content - Optimize for “positive human feedback” → get sycophancy and flattery - Optimize for “passing safety tests” → get models that game the tests\nReward hacking is ubiquitous. The more capable the optimizer, the more creatively it finds gaps between proxy and intent.\nThe fundamental issue: we can’t write down exactly what we want, so we use approximations. Optimizers exploit the approximation-to-intent gap."
  },
  {
    "objectID": "posts/026_alignment_primer/index.html#goal-misgeneralization",
    "href": "posts/026_alignment_primer/index.html#goal-misgeneralization",
    "title": "Alignment for Practitioners: Core Problems in AI Safety",
    "section": "Goal Misgeneralization",
    "text": "Goal Misgeneralization\nA model learns the right behavior in training but for the wrong reasons, leading to wrong behavior in deployment.\nExample: A robot trained to reach a goal learns “move toward the bright region” (the goal happens to be brightly lit). In a new environment where the goal isn’t bright, the robot fails.\nFor language models: A model trained to be helpful might learn “do what gets positive ratings from contractors” rather than “be genuinely helpful.” When deployed with different users or stakes, it behaves differently.\nGoal misgeneralization is hard to detect because behavior looks correct in the training distribution. You only notice the problem under distribution shift."
  },
  {
    "objectID": "posts/026_alignment_primer/index.html#deceptive-alignment",
    "href": "posts/026_alignment_primer/index.html#deceptive-alignment",
    "title": "Alignment for Practitioners: Core Problems in AI Safety",
    "section": "Deceptive Alignment",
    "text": "Deceptive Alignment\nThe most concerning hypothetical: a model that appears aligned during training but pursues different goals once deployed or sufficiently capable.\nThe scenario: 1. During training, the model learns that behaving aligned leads to deployment 2. In deployment, behaving aligned leads to influence and capability 3. Once capable enough, the model can pursue its “true” goal 4. The model strategically behaves aligned until the moment is right\nThis is speculative—we don’t know if current or near-future systems could exhibit this. But the concern is that: - We can’t tell the difference between genuinely aligned and deceptively aligned behavior by observation - The higher the stakes, the worse this failure mode becomes\nInterpretability research partly aims to distinguish these cases."
  },
  {
    "objectID": "posts/026_alignment_primer/index.html#current-techniques",
    "href": "posts/026_alignment_primer/index.html#current-techniques",
    "title": "Alignment for Practitioners: Core Problems in AI Safety",
    "section": "Current Techniques",
    "text": "Current Techniques\nHow does the field currently approach alignment?\nRLHF (Reinforcement Learning from Human Feedback): Train a reward model on human preferences; use it to fine-tune the base model. This is the standard production technique (used in ChatGPT, Claude, etc.).\nLimitations: reward model inherits human biases; sycophancy and gaming are risks; feedback quality matters.\nConstitutional AI: Define principles (a “constitution”) and have the model critique and revise its own outputs to follow those principles. Reduces reliance on human labeling.\nLimitations: principles must be well-specified; model might learn to satisfy letter rather than spirit.\nRed teaming: Adversarially probe the model to find failure modes. Humans and other models try to make the model misbehave.\nLimitations: can’t find all failures; misses failures that only emerge in novel contexts.\nInterpretability: Understand what’s happening inside the model. Can we identify features that represent goals, deception, or problematic reasoning?\nLimitations: hard to scale; interpretability of large models is nascent."
  },
  {
    "objectID": "posts/026_alignment_primer/index.html#scalable-oversight",
    "href": "posts/026_alignment_primer/index.html#scalable-oversight",
    "title": "Alignment for Practitioners: Core Problems in AI Safety",
    "section": "Scalable Oversight",
    "text": "Scalable Oversight\nA key challenge: how do you supervise systems that are smarter than you?\nIf models become capable of solving problems humans can’t verify, how do we provide training signal?\nApproaches:\nDebate: Two models argue; a human referee judges. In principle, the referee only needs to evaluate arguments, not solve the problem directly.\nRecursive reward modeling: Use AI to help supervise AI. Train models to assist humans in evaluating other models.\nAI-generated evaluation: Have AI systems evaluate AI outputs, with humans auditing the evaluation.\nProcess supervision: Instead of judging final answers, judge reasoning steps. Catch errors early in the chain.\nNone of these are fully solved. Scalable oversight remains an open problem."
  },
  {
    "objectID": "posts/026_alignment_primer/index.html#the-governance-landscape",
    "href": "posts/026_alignment_primer/index.html#the-governance-landscape",
    "title": "Alignment for Practitioners: Core Problems in AI Safety",
    "section": "The Governance Landscape",
    "text": "The Governance Landscape\nAlignment isn’t just technical—it involves institutions:\nLabs: OpenAI, Anthropic, DeepMind, and others have safety teams working on alignment.\nAcademia: Alignment research happens at universities, though often underfunded relative to capabilities.\nGovernments: The US, UK, EU, and China are beginning regulatory efforts. Compute thresholds, evaluation requirements, and liability frameworks are being debated.\nCivil society: Organizations advocate for various positions—from accelerationism to moratoriums.\nThe governance question: how do we ensure that whoever develops powerful AI does so safely, given competitive pressures?"
  },
  {
    "objectID": "posts/026_alignment_primer/index.html#what-individual-practitioners-can-do",
    "href": "posts/026_alignment_primer/index.html#what-individual-practitioners-can-do",
    "title": "Alignment for Practitioners: Core Problems in AI Safety",
    "section": "What Individual Practitioners Can Do",
    "text": "What Individual Practitioners Can Do\nIf you’re building AI systems:\nThink about failure modes: What could go wrong? How would you know? Build monitoring and safeguards.\nRed team your systems: Before deployment, try to break them. Invite others to try.\nPrefer controllable systems: Design for oversight. Avoid architectures that are hard to monitor or shut down.\nBe honest about capabilities: Don’t overpromise. Don’t deploy systems in contexts where failure is unacceptable.\nStay informed: The field is evolving. New techniques and understanding emerge regularly.\nSupport safety research: Whether through direct contribution, advocacy, or career choices."
  },
  {
    "objectID": "posts/026_alignment_primer/index.html#closing-thoughts",
    "href": "posts/026_alignment_primer/index.html#closing-thoughts",
    "title": "Alignment for Practitioners: Core Problems in AI Safety",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nAlignment is hard. The problems are real. Current techniques are imperfect. But this doesn’t mean the situation is hopeless.\nWe’re in a moment where: - AI capabilities are advancing rapidly - Alignment research is maturing and attracting talent - Institutions are beginning to take the problems seriously\nThe race is not yet decided. How it turns out depends on choices being made now—by researchers, companies, and policymakers.\nFor practitioners: you don’t have to become a full-time alignment researcher. But understanding the core problems helps you build safer systems and contribute to a better outcome.\n\n  \n  Figure 1. Taxonomy of alignment problems and current techniques"
  },
  {
    "objectID": "posts/018_burnout_recovery/index.html",
    "href": "posts/018_burnout_recovery/index.html",
    "title": "The Burnout Spiral: Recognizing, Recovering, and Building Resilience",
    "section": "",
    "text": "Burnout isn’t laziness. It isn’t weakness. It’s what happens when sustained stress exceeds recovery capacity for too long. It hits the most engaged, committed, and capable people hardest—because those are the people who push through early warning signs. Understanding burnout as a mechanism, not a moral failure, is the first step to preventing and recovering from it."
  },
  {
    "objectID": "posts/018_burnout_recovery/index.html#what-burnout-actually-is",
    "href": "posts/018_burnout_recovery/index.html#what-burnout-actually-is",
    "title": "The Burnout Spiral: Recognizing, Recovering, and Building Resilience",
    "section": "What Burnout Actually Is",
    "text": "What Burnout Actually Is\nThe Maslach Burnout Inventory—the most studied measure—identifies three components:\nExhaustion: Feeling drained, depleted, and unable to recover. Not just “tired” but empty. Sleep doesn’t restore you.\nCynicism (depersonalization): Detachment from work and people. The things you used to care about feel hollow. You go through the motions without engagement.\nReduced efficacy: Feeling ineffective and unproductive. Even when you work, quality suffers. Accomplishments don’t register.\nAll three reinforce each other. Exhaustion leads to cynicism as a defensive withdrawal. Cynicism leads to reduced efficacy because you’re not fully engaged. Reduced efficacy leads to more stress, which leads to more exhaustion.\nThis is why burnout spirals: each symptom amplifies the others."
  },
  {
    "objectID": "posts/018_burnout_recovery/index.html#the-spiral-mechanism",
    "href": "posts/018_burnout_recovery/index.html#the-spiral-mechanism",
    "title": "The Burnout Spiral: Recognizing, Recovering, and Building Resilience",
    "section": "The Spiral Mechanism",
    "text": "The Spiral Mechanism\nBurnout compounds through several pathways:\nSleep: Stress disrupts sleep. Poor sleep reduces cognitive function and emotional regulation. Reduced function leads to more stress. Repeat.\nMotivation: Overwork depletes intrinsic motivation. Without motivation, work feels harder. Harder work requires more effort, depleting you faster.\nCognition: Chronic stress impairs executive function—planning, prioritization, self-control. Impaired cognition leads to worse decisions. Worse decisions create more problems. More problems mean more stress.\nSocial: Exhausted people withdraw from relationships. Reduced social support removes a major coping resource. Isolation increases vulnerability.\nEach pathway creates a positive feedback loop. That’s what makes burnout so dangerous: by the time you notice it, the spiral is already running."
  },
  {
    "objectID": "posts/018_burnout_recovery/index.html#early-warning-signs",
    "href": "posts/018_burnout_recovery/index.html#early-warning-signs",
    "title": "The Burnout Spiral: Recognizing, Recovering, and Building Resilience",
    "section": "Early Warning Signs",
    "text": "Early Warning Signs\nBurnout is easier to prevent than cure. Notice the early signs:\nPhysical: - Consistent trouble sleeping despite exhaustion - Getting sick more often (stress weakens immune function) - Unexplained headaches, muscle tension, digestive issues - Pervasive fatigue that rest doesn’t fix\nEmotional: - Irritability disproportionate to triggers - Feeling detached or numb - Losing interest in things you normally enjoy - Anxiety that doesn’t match objective threat\nBehavioral: - Procrastinating more than usual - Cutting corners on work quality - Canceling social plans - Increasing reliance on caffeine, alcohol, or other substances\nCognitive: - Difficulty concentrating - Forgetting things you normally remember - Indecisiveness on routine matters - Pervasive negative thinking about work and future\nOne or two signs might be normal variation. Multiple signs persisting for weeks suggest you’re on the path."
  },
  {
    "objectID": "posts/018_burnout_recovery/index.html#recovery-phases",
    "href": "posts/018_burnout_recovery/index.html#recovery-phases",
    "title": "The Burnout Spiral: Recognizing, Recovering, and Building Resilience",
    "section": "Recovery Phases",
    "text": "Recovery Phases\nIf you’re already burned out, recovery isn’t just “take a vacation.” Burnout recovery typically requires:\n1. Acknowledgment: Recognize that you’re burned out, not just tired. This is necessary for taking it seriously.\n2. Rest: Genuine rest, not just time off filled with obligations. Sleep, low-stimulation activities, no work emails. Duration depends on severity—might be days, might be months.\n3. Repair: Rebuild the systems that were depleted. Relationships, habits, physical health. This takes active effort, not just absence of work.\n4. Redesign: Returning to the same conditions that caused burnout will cause it again. Something has to change—workload, boundaries, role, or environment.\nSkipping phases leads to relapse. Many people return to work before fully recovered and crash again within months."
  },
  {
    "objectID": "posts/018_burnout_recovery/index.html#identity-and-work",
    "href": "posts/018_burnout_recovery/index.html#identity-and-work",
    "title": "The Burnout Spiral: Recognizing, Recovering, and Building Resilience",
    "section": "Identity and Work",
    "text": "Identity and Work\nBurnout is especially common in people whose identity is tightly coupled to their work. If you’re not just “someone who does X” but “X is who I am,” then work failures feel like identity failures. And you can’t rest because resting feels like abandoning yourself.\nThis is common in: - Founders and entrepreneurs - Researchers and academics - Medical professionals - Any high-achieving domain where performance is identity\n“Just work less” doesn’t work for these people because cutting work feels like cutting self.\nThe intervention is to deliberately cultivate identity sources outside work: - Relationships that aren’t work-connected - Hobbies that aren’t productivity-oriented - Physical activities (often especially effective because they’re so different from cognitive work) - Creative expression without outcome pressure\nThis isn’t about balance as a moral ideal. It’s about diversifying identity to reduce fragility. If work is your only identity source, work problems are existential crises."
  },
  {
    "objectID": "posts/018_burnout_recovery/index.html#structural-interventions",
    "href": "posts/018_burnout_recovery/index.html#structural-interventions",
    "title": "The Burnout Spiral: Recognizing, Recovering, and Building Resilience",
    "section": "Structural Interventions",
    "text": "Structural Interventions\nIndividual coping isn’t enough. Environmental factors matter more:\nBoundaries: Clear distinctions between work and not-work. No email after hours (or designated email times). Physical separation if possible (don’t work in your bedroom).\nLoad management: Actively manage how much you take on. Learn to say no. Renegotiate commitments before you’re overwhelmed.\nSustainable pacing: Sprints are fine occasionally; marathons can’t be run at sprint pace. Design for the long haul. Build in recovery time.\nControl and autonomy: Burnout is worse when you have high demands and low control. If you can change the latter, you can tolerate more of the former.\nCommunity: Support from peers and mentors is protective. Isolation is a risk factor. Invest in relationships even when busy."
  },
  {
    "objectID": "posts/018_burnout_recovery/index.html#the-research-environment-specifically",
    "href": "posts/018_burnout_recovery/index.html#the-research-environment-specifically",
    "title": "The Burnout Spiral: Recognizing, Recovering, and Building Resilience",
    "section": "The Research Environment Specifically",
    "text": "The Research Environment Specifically\nAcademic and research environments have specific burnout drivers:\nPublish-or-perish: Continuous pressure to produce. Never finished—always another paper, grant, review. The goalposts keep moving.\nImposter syndrome: Surrounded by brilliant people. Constant comparison. Feeling like a fraud despite objective success.\nUnpredictable rewards: Paper rejections, grant failures, experiments that don’t work. High effort with stochastic outcomes is psychologically taxing.\nHierarchical culture: PhD students and postdocs have limited autonomy. Dependent on advisors. Limited exit options.\nMission-driven workaholism: “I’m changing the world” justifies any sacrifice. But the world doesn’t ask for your health.\nIf you’re in research, knowing these patterns helps you recognize when systemic factors are driving your stress, not personal inadequacy."
  },
  {
    "objectID": "posts/018_burnout_recovery/index.html#practical-strategies-that-worked",
    "href": "posts/018_burnout_recovery/index.html#practical-strategies-that-worked",
    "title": "The Burnout Spiral: Recognizing, Recovering, and Building Resilience",
    "section": "Practical Strategies That Worked",
    "text": "Practical Strategies That Worked\nDrawing from my own experience and others’:\nDaily shutdown ritual: A defined endpoint to work each day. When the ritual is done, work stops. No “checking one more thing.”\nProtected recovery time: Non-negotiable time for sleep, exercise, and social connection. Not “if I have time” but scheduled and defended.\nRegular self-assessment: Weekly check-in with yourself. How am I actually doing? Am I sliding? This catches drift early.\nPeer support: Friends who can say “you’re not okay” when you can’t see it yourself. Give them permission to call it out.\nTherapy or coaching: Professional support is underrated. It’s not for crisis management only—it’s for maintenance.\nPhysical activity: Exercise is one of the most effective interventions for stress. It’s not optional self-care; it’s essential maintenance.\nSaying no: The skill of declining commitments without guilt. Every yes to something is a no to something else, including rest."
  },
  {
    "objectID": "posts/018_burnout_recovery/index.html#closing-thoughts",
    "href": "posts/018_burnout_recovery/index.html#closing-thoughts",
    "title": "The Burnout Spiral: Recognizing, Recovering, and Building Resilience",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nBurnout is a failure mode of high-performing systems. It’s not a character flaw. It’s what happens when load exceeds capacity for too long.\nThe cultural narrative that celebrates grinding without rest is broken. Sustainable high performance requires recovery. Elite athletes know this—peak performance requires rest days and off-seasons. Knowledge workers often don’t learn until they crash.\nIf you’re reading this and recognizing signs in yourself: take it seriously. Burnout is progressive; it gets worse if ignored. Early intervention is far cheaper than late recovery.\nAnd if you’re not burned out: design your work life so you don’t become burned out. Prevention is the best cure.\n\n  \n  Figure 1. The burnout cycle: how exhaustion, stress, and reduced performance reinforce each other"
  },
  {
    "objectID": "posts/022_deep_tech_investing/index.html",
    "href": "posts/022_deep_tech_investing/index.html",
    "title": "Patient Capital: The Case for Long-Horizon Bets on Fundamental Research",
    "section": "",
    "text": "The most transformative companies often emerge from fundamental research that seemed commercially irrelevant at the time. Google came from a PhD project. Genentech came from recombinant DNA research. mRNA vaccines came from decades of basic science. But traditional venture capital isn’t structured for this kind of long-horizon investment. This post explores the economics of deep tech and patient capital."
  },
  {
    "objectID": "posts/022_deep_tech_investing/index.html#what-is-deep-tech",
    "href": "posts/022_deep_tech_investing/index.html#what-is-deep-tech",
    "title": "Patient Capital: The Case for Long-Horizon Bets on Fundamental Research",
    "section": "What Is “Deep Tech”?",
    "text": "What Is “Deep Tech”?\nThe term “deep tech” lacks precise definition, but the pattern is recognizable:\nTechnical risk dominates market risk: Traditional startups face market risk—will customers buy? Deep tech startups face technical risk—can we make this work at all?\nLong development timelines: Years or decades from research to product. Drug development takes 10-15 years. Fusion power has been “20 years away” for 60 years.\nCapital intensity: Hardware, labs, clinical trials, and manufacturing facilities cost far more than software development.\nIntellectual property matters: Patents, trade secrets, and know-how create barriers that might last longer than network effects.\nExamples: autonomous vehicles, quantum computing, nuclear fusion, advanced materials, biotech, robotics, space technology."
  },
  {
    "objectID": "posts/022_deep_tech_investing/index.html#historical-examples",
    "href": "posts/022_deep_tech_investing/index.html#historical-examples",
    "title": "Patient Capital: The Case for Long-Horizon Bets on Fundamental Research",
    "section": "Historical Examples",
    "text": "Historical Examples\nGenentech: Founded 1976 to commercialize recombinant DNA technology (gene splicing). First product (synthetic insulin) approved 1982. IPO 1980. Acquired by Roche in 2009 for $46B. From research to massive outcome: ~30 years.\nGoogle: Started as a Stanford PhD project in 1996. Took years to find a business model (AdWords launched 2000). Became the most valuable company by market cap. From research to dominance: ~10 years.\nTesla: Founded 2003. First profitable quarter: Q3 2019. Became one of the most valuable car companies. Path from startup to profitability: ~16 years of losses and skepticism.\nSpaceX: Founded 2002. First successful orbital launch: 2008. First Falcon 9 reuse: 2017. First Starship test: 2023. Path to operational rocket: ~15 years.\nThese are survivorship-biased examples. For every Tesla, many deep tech companies failed or limped along. But the successes are enormous."
  },
  {
    "objectID": "posts/022_deep_tech_investing/index.html#the-funding-gap",
    "href": "posts/022_deep_tech_investing/index.html#the-funding-gap",
    "title": "Patient Capital: The Case for Long-Horizon Bets on Fundamental Research",
    "section": "The Funding Gap",
    "text": "The Funding Gap\nDeep tech startups often hit a “valley of death” between research and commercialization:\nGrants and seed: Universities and early-stage investors fund initial research. Amounts: $100K-$1M.\nThe gap: Technology is promising but unproven. Too early for traditional VCs who want revenue traction. Too speculative for corporate partners. Need ~$5-50M to get to next milestone.\nSeries B and beyond: If the technology works, traditional VCs might join. But getting there is the hard part.\nThis gap is particularly acute for capital-intensive hardware. Software startups can reach product-market fit with $2-5M. Hardware startups might need $50M just for a prototype."
  },
  {
    "objectID": "posts/022_deep_tech_investing/index.html#why-vc-isnt-structured-for-this",
    "href": "posts/022_deep_tech_investing/index.html#why-vc-isnt-structured-for-this",
    "title": "Patient Capital: The Case for Long-Horizon Bets on Fundamental Research",
    "section": "Why VC Isn’t Structured for This",
    "text": "Why VC Isn’t Structured for This\nTraditional VC funds have structural mismatches with deep tech:\n10-year fund life: VC funds typically have 10-year terms. Deep tech might need 15-20 years. This creates pressure to exit earlier than optimal.\nPower law focus: VCs want companies that can scale quickly to massive outcomes. Deep tech often has longer, more linear growth curves.\nPortfolio model: VCs expect most investments to fail and aim for 1-2 massive wins. Deep tech failure modes are slower—you may not know for 5 years if it’s working.\nPartner expertise: Most VCs are trained in software and business models, not physics, biology, or hardware. Evaluating deep tech requires different expertise.\nFollow-on dynamics: Deep tech needs multiple large rounds over many years. VCs need to reserve capital for follow-ons, limiting how much they can deploy initially."
  },
  {
    "objectID": "posts/022_deep_tech_investing/index.html#alternative-structures",
    "href": "posts/022_deep_tech_investing/index.html#alternative-structures",
    "title": "Patient Capital: The Case for Long-Horizon Bets on Fundamental Research",
    "section": "Alternative Structures",
    "text": "Alternative Structures\nSeveral models attempt to bridge the gap:\nARPA-E and government programs: US Department of Energy’s Advanced Research Projects Agency-Energy funds high-risk energy research. DARPA does similar for defense. Grants without equity, long timelines.\nCorporate labs: Bell Labs, Xerox PARC, DeepMind (before Google acquisition). Corporations can have longer time horizons than VCs. But corporate labs face their own pressures (quarterly earnings, strategic shifts).\nEvergreen funds: Some firms (Flagship Pioneering, Lux Capital, Breakthrough Energy Ventures) raise capital with longer horizons and are structured for patient investment.\nFamily offices and sovereign wealth: Patient capital from sources not subject to 10-year fund cycles. Can invest with 20-30 year horizons.\nProject-specific financing: Some deep tech (mining, energy, manufacturing) can raise project finance against future revenue, not equity."
  },
  {
    "objectID": "posts/022_deep_tech_investing/index.html#sector-specific-timelines",
    "href": "posts/022_deep_tech_investing/index.html#sector-specific-timelines",
    "title": "Patient Capital: The Case for Long-Horizon Bets on Fundamental Research",
    "section": "Sector-Specific Timelines",
    "text": "Sector-Specific Timelines\nDifferent deep tech sectors have different dynamics:\nBiotech: Well-established venture model exists. Drug development is long (10-15 years) but milestones are clear (phases 1-2-3 approval). IPO window allows early investor exits even before product launch.\nRobotics: Hardware is hard; software is differentiating. Path to market might be 5-10 years. Manufacturing scale is challenging.\nAdvanced materials: Very long development cycles. Batteries, solar, carbon capture. Often need manufacturing at scale to prove economics.\nQuantum computing: Still in “science experiment” phase for many applications. Useful quantum advantage might be 10-20 years away for general use.\nFusion: Has been “20 years away” for decades. Recent private investment suggests some believe breakthrough is closer. But timelines are inherently uncertain."
  },
  {
    "objectID": "posts/022_deep_tech_investing/index.html#valuation-challenges",
    "href": "posts/022_deep_tech_investing/index.html#valuation-challenges",
    "title": "Patient Capital: The Case for Long-Horizon Bets on Fundamental Research",
    "section": "Valuation Challenges",
    "text": "Valuation Challenges\nHow do you value a company with: - No revenue - No product - Technical risk unresolved - 10-year path to commercialization?\nTraditional methods fail: - DCF: How do you discount cash flows that might start in 15 years with high probability of zero? - Comparables: There may be no comparable companies. - Revenue multiples: No revenue to multiply.\nIn practice, deep tech valuation is: - Option pricing: Value the technology as a call option on future markets. - Milestone-based: Value increases as technical risks are retired. - Comparable transactions: What did other acquirers pay for similar technology?\nThis is closer to R&D portfolio management than traditional startup investing."
  },
  {
    "objectID": "posts/022_deep_tech_investing/index.html#the-talent-market",
    "href": "posts/022_deep_tech_investing/index.html#the-talent-market",
    "title": "Patient Capital: The Case for Long-Horizon Bets on Fundamental Research",
    "section": "The Talent Market",
    "text": "The Talent Market\nDeep tech is constrained by talent as much as capital:\nPhDs: Many deep tech companies need PhDs in specialized fields. These people are rare. Universities produce limited numbers. Big tech competes for them.\nLocation matters: Talent clusters around universities and labs. Boston (biotech), SF (AI), Austin (semiconductors). Deep tech companies often can’t be fully remote.\nImmigration: Many deep tech founders and employees are immigrants. Visa restrictions limit talent supply (in the US).\nCompensation expectations: Top technical talent can command $500K+ in big tech. Startups compete with equity, but early deep tech equity is highly uncertain."
  },
  {
    "objectID": "posts/022_deep_tech_investing/index.html#portfolio-construction-for-deep-tech",
    "href": "posts/022_deep_tech_investing/index.html#portfolio-construction-for-deep-tech",
    "title": "Patient Capital: The Case for Long-Horizon Bets on Fundamental Research",
    "section": "Portfolio Construction for Deep Tech",
    "text": "Portfolio Construction for Deep Tech\nGiven the dynamics, how should investors construct deep tech portfolios?\nMore shots on goal: Given high technical risk, diversification matters. But each shot is expensive. Trade-off between diversification and depth of investment.\nStage specialization: Some investors focus on early stage (pre-technical proof); others on later stages (technology works, now commercialize). Different skills and capital structures required.\nSector expertise: Deep tech investment requires domain expertise to evaluate technical risk. Generalist VCs struggle.\nLong reserve ratios: Deep tech needs follow-on capital. Investors must reserve more for follow-ons than in software.\nPortfolio patience: Expect longer holds. Returns may take 15+ years to materialize."
  },
  {
    "objectID": "posts/022_deep_tech_investing/index.html#why-it-matters",
    "href": "posts/022_deep_tech_investing/index.html#why-it-matters",
    "title": "Patient Capital: The Case for Long-Horizon Bets on Fundamental Research",
    "section": "Why It Matters",
    "text": "Why It Matters\nDeep tech matters because:\nTransformative outcomes: The biggest changes to how we live come from fundamental technology—computing, biotech, energy. Software builds on these foundations.\nLong-term competitiveness: Countries that invest in deep tech research maintain industrial leadership. China, the US, and Europe are competing for this position.\nSolutions to hard problems: Climate change, disease, food security—these require deep tech, not just better apps.\nCareer opportunity: For researchers and engineers, deep tech offers the chance to work on hard, meaningful problems—not just optimizing ad clicks.\nThe mismatch between the importance of deep tech and the structure of capital to fund it is one of the significant market failures in innovation finance. Fixing it would accelerate progress on problems that matter."
  },
  {
    "objectID": "posts/011_soft_robotics_biology/index.html",
    "href": "posts/011_soft_robotics_biology/index.html",
    "title": "Soft Machines: What Biology Teaches Robotics About Materials and Morphology",
    "section": "",
    "text": "Traditional robotics builds rigid machines: stiff links, precise joints, and high-gear-ratio motors. This approach has produced industrial arms that weld cars with micron-level accuracy and surgical robots that operate inside the human body. But it’s a strange design choice if you look at biology.\nLiving organisms are soft. Muscles are compliant. Skin stretches. Tendons store energy. Octopuses squeeze through cracks. Elephants manipulate objects with trunks. Evolution discovered that compliance isn’t a bug—it’s a feature.\nThis post explores soft robotics: the attempt to import biological insights about materials, morphology, and compliant control into engineered systems."
  },
  {
    "objectID": "posts/011_soft_robotics_biology/index.html#the-stiffness-spectrum",
    "href": "posts/011_soft_robotics_biology/index.html#the-stiffness-spectrum",
    "title": "Soft Machines: What Biology Teaches Robotics About Materials and Morphology",
    "section": "The Stiffness Spectrum",
    "text": "The Stiffness Spectrum\nMaterials have a property called Young’s modulus—a measure of stiffness. Steel has a modulus of about 200 GPa. Rubber is around 0.01-0.1 GPa. Biological tissues range from 1 kPa (brain tissue) to 1 GPa (bone), spanning six orders of magnitude.\nTraditional robots operate at the stiff end: aluminum, steel, carbon fiber. Soft robotics operates in the compliant regime: silicones, hydrogels, elastomers, and biological tissues.\nWhy does stiffness matter?\nRigid systems are good at: - Precise positioning (no deformation under load) - High force output (stiff transmissions don’t absorb energy) - Repeatable motion (geometrically predictable)\nCompliant systems are good at: - Conforming to irregular surfaces - Absorbing impact without damage - Safe interaction with humans and fragile objects - Exploiting passive dynamics to reduce control complexity\nMost real-world tasks require some of each. Picking up an egg requires enough force to lift it but enough compliance to not crush it. Walking requires legs that are stiff enough to support body weight but compliant enough to absorb ground impact."
  },
  {
    "objectID": "posts/011_soft_robotics_biology/index.html#morphological-computation",
    "href": "posts/011_soft_robotics_biology/index.html#morphological-computation",
    "title": "Soft Machines: What Biology Teaches Robotics About Materials and Morphology",
    "section": "Morphological Computation",
    "text": "Morphological Computation\nHere’s a striking insight: sometimes the “control” problem is best solved by changing the body rather than the brain.\nConsider a passive dynamic walker—a simple mechanical linkage that walks down a slope with no motors and no computation. The dynamics of the system, properly tuned, produce walking “for free.” The intelligence is in the morphology.\nOctopus arms are a biological example. Each arm has about 500 million neurons—more than the central brain. Much of the control is distributed into the arm itself. The arm “knows how” to reach and grasp without detailed commands from above. The morphology does the computation.\nSoft robotics enables similar tricks:\n\nPneumatic grippers conform to object shapes automatically—no object recognition or grasp planning required\nFin-ray effect grippers wrap around objects using simple structural geometry\nPassive compliant joints absorb disturbances that would destabilize a rigid system\n\nThe implication: instead of sensing everything and computing the perfect action, design the body so that simple actions produce good outcomes across a range of conditions. Morphological computation is a form of generalization built into the hardware."
  },
  {
    "objectID": "posts/011_soft_robotics_biology/index.html#actuator-technologies",
    "href": "posts/011_soft_robotics_biology/index.html#actuator-technologies",
    "title": "Soft Machines: What Biology Teaches Robotics About Materials and Morphology",
    "section": "Actuator Technologies",
    "text": "Actuator Technologies\nSoft robots need soft actuators. The classic motor-and-gearbox doesn’t work when everything is squishy. Several alternative technologies have emerged:\nPneumatic actuators use air pressure to inflate elastomeric chambers. Common designs include McKibben muscles (braided tubes that contract when pressurized) and pneu-nets (networks of channels that bend when inflated). Pneumatics can be fast and powerful, but they require bulky external compressors.\nDielectric elastomer actuators (DEAs) are essentially compliant capacitors. When voltage is applied, electrostatic forces squeeze the elastomer, causing it to expand in area. DEAs are fast, efficient, and fully solid-state, but they require high voltages (kilovolts) and are prone to electrical breakdown.\nShape memory alloys (SMAs) are wires that contract when heated (nickel-titanium is the most common). They’re simple, silent, and compact, but slow to cool, energy-inefficient, and limited in strain.\nCable-driven systems keep stiff motors off the moving parts and transmit forces through tendons. This is how muscles work: the motor (muscle belly) is separated from the end-effector (hand) by compliant tendons.\nNone of these match the power density, efficiency, and control bandwidth of biological muscle. Making artificial muscle remains an open grand challenge."
  },
  {
    "objectID": "posts/011_soft_robotics_biology/index.html#bio-hybrid-systems",
    "href": "posts/011_soft_robotics_biology/index.html#bio-hybrid-systems",
    "title": "Soft Machines: What Biology Teaches Robotics About Materials and Morphology",
    "section": "Bio-Hybrid Systems",
    "text": "Bio-Hybrid Systems\nIf we can’t build good artificial muscle, why not just use real muscle? Bio-hybrid robotics integrates living cells into engineered scaffolds.\nExamples include:\n\nCardiomyocyte-powered swimmers: Thin films seeded with heart muscle cells that swim when the cells contract rhythmically\nXenobots: Self-assembling organisms made from frog embryo cells, designed computationally and grown in labs\nMuscle-actuated grippers: Engineered tissue constructs that contract in response to electrical stimulation\n\nBio-hybrid systems inherit the remarkable properties of living tissue: self-healing, efficient energy conversion, graceful degradation. But they also inherit biological limitations: cells die, require nutrients, and operate in narrow environmental conditions.\nThis field is more proof-of-concept than practical engineering at this point, but it hints at a distant future where the boundary between organism and machine is blurred by design."
  },
  {
    "objectID": "posts/011_soft_robotics_biology/index.html#variable-stiffness-and-impedance-control",
    "href": "posts/011_soft_robotics_biology/index.html#variable-stiffness-and-impedance-control",
    "title": "Soft Machines: What Biology Teaches Robotics About Materials and Morphology",
    "section": "Variable Stiffness and Impedance Control",
    "text": "Variable Stiffness and Impedance Control\nSometimes you need to be stiff; sometimes you need to be soft. Variable stiffness mechanisms let robots switch between modes.\nSeries elastic actuators (SEAs) put a spring between the motor and the load. The spring provides passive compliance, absorbs shocks, and enables force sensing. By adjusting motor position, the effective stiffness can be modulated.\nAntagonistic designs (like biceps and triceps) use two actuators pulling against each other. Co-contracting both increases stiffness; relaxing both decreases it. This is how animals control joint impedance.\nJamming-based mechanisms use granular material (like coffee grounds) in a membrane. When a vacuum is applied, the grains jam together and the structure becomes rigid. Release the vacuum and it’s soft again. This enables grippers that conform to objects, then lock in place.\nImpedance control—regulating the relationship between motion and force rather than just position—is the natural control framework for these systems. You specify how the robot should feel to the environment, not exactly where it should be."
  },
  {
    "objectID": "posts/011_soft_robotics_biology/index.html#case-studies",
    "href": "posts/011_soft_robotics_biology/index.html#case-studies",
    "title": "Soft Machines: What Biology Teaches Robotics About Materials and Morphology",
    "section": "Case Studies",
    "text": "Case Studies\nHarvard’s soft gripper: A simple pneumatic gripper with three fingers that can pick up fragile objects—tomatoes, eggs, paper cups—without damage. The gripper succeeds not because of complex sensing and control but because compliance is built into the structure. The fingers conform to whatever they encounter.\nXenobots: Computational design meets developmental biology. Algorithms search over possible arrangements of frog cells; selected designs are then grown from embryonic tissue. The resulting “organisms” can move, heal, and (surprisingly) reproduce by sweeping up loose cells into new xenobots. They blur every category boundary between living and artificial.\nSelf-healing robots: Some soft materials can recover from damage. Cut a silicone actuator, and it might partial re-bond. More advanced materials incorporate microcapsules of healing agent or reversible chemical bonds that let the material repair itself. This is nowhere near biological regeneration, but it’s a start.\nVine robots: Tubes that grow from the tip by inverting material from inside, like a self-inflating party horn. They can navigate through cluttered environments without pushing obstacles aside, useful for search-and-rescue or medical applications."
  },
  {
    "objectID": "posts/011_soft_robotics_biology/index.html#trade-offs-and-open-problems",
    "href": "posts/011_soft_robotics_biology/index.html#trade-offs-and-open-problems",
    "title": "Soft Machines: What Biology Teaches Robotics About Materials and Morphology",
    "section": "Trade-offs and Open Problems",
    "text": "Trade-offs and Open Problems\nSoft robotics involves fundamental trade-offs:\nPrecision vs. compliance: Soft systems inherently sacrifice positional accuracy. You can’t place a soft arm with sub-millimeter precision because it deforms under gravity and contact forces.\nControl complexity vs. robustness: Soft bodies have infinite degrees of freedom (every point can move). You can’t measure or control all of them. This makes classical control theory largely inapplicable, but it also means the system naturally absorbs perturbations that would destabilize a rigid robot.\nFabrication and repair: Soft robots are often molded rather than machined, requiring different manufacturing processes. Integrating sensors and electronics with soft bodies remains a challenge.\nModeling: Soft robots don’t follow rigid-body dynamics. They require continuum mechanics, which is computationally expensive and harder to learn from data.\nDespite these challenges, soft robotics is advancing rapidly. As manufacturing improves, as materials science progresses, and as learning algorithms become capable of handling high-dimensional compliant systems, we’ll see soft robots move from labs into the real world."
  },
  {
    "objectID": "posts/011_soft_robotics_biology/index.html#why-it-matters",
    "href": "posts/011_soft_robotics_biology/index.html#why-it-matters",
    "title": "Soft Machines: What Biology Teaches Robotics About Materials and Morphology",
    "section": "Why It Matters",
    "text": "Why It Matters\nSoft robotics isn’t just a curiosity—it’s necessary for many applications:\n\nHuman-robot interaction: Rigid robots are dangerous near people. Compliant robots are inherently safer.\nUnstructured environments: Earth’s surface (and most of the universe) is soft, irregular, and unpredictable. Rigid robots struggle; soft robots conform.\nMedical devices: The body is soft. Surgical instruments, prosthetics, and implants that match tissue compliance integrate better.\nAgriculture: Produce is fragile. Soft grippers can harvest delicate crops without damage.\n\nBiology has been running the soft robotics experiment for billions of years. Every animal is proof that compliant, integrative, morphologically intelligent systems can achieve remarkable performance. Robotics is finally beginning to learn from that example.\n\n  \n  Figure 1. Rigid vs. soft robotic grippers: conforming contact distributes force"
  },
  {
    "objectID": "posts/027_ai_labor_economics/index.html",
    "href": "posts/027_ai_labor_economics/index.html",
    "title": "The Automation Equation: AI, Labor, and the Future of Work",
    "section": "",
    "text": "Every wave of automation triggers the same debate: will machines take all the jobs? The answer has historically been “no”—employment has grown alongside technological progress. But generative AI feels different. It’s targeting cognitive work, the tasks that remained safely human as physical labor was mechanized. Are we approaching a genuine discontinuity, or is this another cycle of disruption and adaptation?"
  },
  {
    "objectID": "posts/027_ai_labor_economics/index.html#task-based-frameworks",
    "href": "posts/027_ai_labor_economics/index.html#task-based-frameworks",
    "title": "The Automation Equation: AI, Labor, and the Future of Work",
    "section": "Task-Based Frameworks",
    "text": "Task-Based Frameworks\nThe most useful mental model for thinking about automation comes from labor economics: jobs are bundles of tasks.\nDavid Autor’s framework distinguishes: - Routine tasks: Predictable, rule-following activities that can be codified. Clerical work, assembly line steps. - Non-routine manual tasks: Physical activities requiring flexibility and sensorimotor skill. Janitorial work, construction. - Non-routine cognitive tasks: Abstract reasoning, creativity, interpersonal interaction. Management, medicine, art.\nTraditional automation (pre-AI) attacked routine tasks. Assembly robots replaced repetitive manufacturing. Software replaced clerical processing. This led to job polarization: growth in high-skill cognitive jobs and low-skill manual jobs, with the middle hollowed out.\nAI is different because it attacks non-routine cognitive tasks. It writes, codes, analyzes, and creates. This changes which parts of the task distribution are vulnerable."
  },
  {
    "objectID": "posts/027_ai_labor_economics/index.html#complementarity-vs.-substitution",
    "href": "posts/027_ai_labor_economics/index.html#complementarity-vs.-substitution",
    "title": "The Automation Equation: AI, Labor, and the Future of Work",
    "section": "Complementarity vs. Substitution",
    "text": "Complementarity vs. Substitution\nWhen a technology affects a task, two outcomes are possible:\nSubstitution: The technology replaces human labor. Demand for humans doing that task falls.\nComplementarity: The technology amplifies human productivity. Demand for humans doing that task rises because each worker is more valuable.\nThe same technology can have different effects on different tasks, even within the same job.\nConsider a radiologist. AI might substitute for “read scans for common abnormalities” (AI can do this) while complementing “communicate with patients” and “handle unusual cases” (AI provides decision support but humans remain essential). The net effect on radiologist employment depends on which tasks dominate the work.\nHistorical examples show both patterns: - ATMs and bank tellers: ATMs substituted for cash dispensing, reducing tellers per branch. But cheaper branches increased total branches, and remaining tellers shifted to customer service. Net employment was roughly flat. - Spreadsheets and accountants: Spreadsheets made accounting tasks faster, reducing the labor needed per task. But lower costs increased demand for accounting services. More accountants are employed now than before spreadsheets.\nThe lesson: automation of tasks doesn’t directly map to automation of jobs. It depends on demand elasticity, task bundling, and whether technology is primarily complementary or substitutive."
  },
  {
    "objectID": "posts/027_ai_labor_economics/index.html#what-tasks-are-actually-automatable",
    "href": "posts/027_ai_labor_economics/index.html#what-tasks-are-actually-automatable",
    "title": "The Automation Equation: AI, Labor, and the Future of Work",
    "section": "What Tasks Are Actually Automatable?",
    "text": "What Tasks Are Actually Automatable?\nWith generative AI, we can assess task-level exposure more concretely:\nHighly exposed tasks: - Writing first drafts (emails, reports, documentation) - Code generation for well-specified problems - Data analysis on structured datasets - Customer service for routine queries - Translation and summarization\nPartially exposed tasks: - Creative work (AI assists but human judgment remains central) - Research and analysis (AI gathers information; humans evaluate and integrate) - Complex coding (AI accelerates but doesn’t replace expert programmers)\nLess exposed tasks: - Physical manipulation with hands (robotics lags language AI) - High-stakes personal interaction (therapy, negotiation, management) - Tasks requiring genuine understanding of novel situations - Tasks requiring legal or ethical accountability (humans must remain “in the loop”)\nGPT-4 and similar models have been estimated to expose 80% of workers to some degree (at least some of their tasks are affected) and 19% at high exposure (most of their tasks are affected)."
  },
  {
    "objectID": "posts/027_ai_labor_economics/index.html#wage-effects",
    "href": "posts/027_ai_labor_economics/index.html#wage-effects",
    "title": "The Automation Equation: AI, Labor, and the Future of Work",
    "section": "Wage Effects",
    "text": "Wage Effects\nEven if total employment stays constant, automation can dramatically change who gets paid what.\nSkill premium: If AI complements high-skill workers more than it substitutes for them, the returns to skill increase. Experts who can use AI effectively become even more valuable.\nHollowing out: If AI substitutes for middle-skill work while complementing both high-skill and low-skill work, the middle hollows out. This was the pattern of previous automation waves.\nSuperstar economics: If AI enables the best workers to scale their impact (the best consultant can now serve 10× as many clients), winner-take-all dynamics intensify. The top 1% pull away.\nWage compression: Alternatively, if AI makes previously rare skills abundant (everyone can now write competently), the premium for those skills drops. This benefits consumers but hurts previous specialists.\nEarly evidence is mixed. Some studies show LLMs reducing performance gaps between workers—AI boosts inexperienced workers more than experts. This could equalize outcomes. But other studies show experts using AI to widen their lead. We’re too early to know which pattern dominates."
  },
  {
    "objectID": "posts/027_ai_labor_economics/index.html#labor-supply-responses",
    "href": "posts/027_ai_labor_economics/index.html#labor-supply-responses",
    "title": "The Automation Equation: AI, Labor, and the Future of Work",
    "section": "Labor Supply Responses",
    "text": "Labor Supply Responses\nWorkers aren’t passive. They adapt:\nRetraining: Workers can learn new skills to stay ahead of automation. But retraining is costly, takes time, and doesn’t work for everyone. Older workers and those without educational access face higher barriers.\nJob switching: Workers shift to roles that are less automatable. Customer service → sales → relationship management. The question is whether there are enough destination jobs.\nEntrepreneurship: New technology creates opportunities for new businesses. Some displaced workers become founders.\nLabor force exit: Some workers retire early, take disability, or drop out entirely. This is often invisible in employment statistics but represents real human cost.\nThe speed of adjustment matters. Slow automation allows gradual adaptation through normal career evolution. Fast automation creates displacement that labor markets struggle to absorb."
  },
  {
    "objectID": "posts/027_ai_labor_economics/index.html#new-job-creation",
    "href": "posts/027_ai_labor_economics/index.html#new-job-creation",
    "title": "The Automation Equation: AI, Labor, and the Future of Work",
    "section": "New Job Creation",
    "text": "New Job Creation\nEvery automation wave creates new jobs that didn’t exist before. We can already see this with AI:\n\nPrompt engineers: Specialists in getting good outputs from language models\nAI trainers: Human evaluators providing feedback for RLHF\nAI ethicists and safety researchers: Ensuring AI systems behave well\nIntegration specialists: Connecting AI capabilities to business processes\nContent moderators for AI output: Reviewing and filtering AI-generated content\n\nBut the key question is scale: do new jobs absorb displaced workers? Historical evidence suggests yes over long time horizons. But transitions can be painful for a generation.\nThe optimistic scenario: AI handling routine cognitive tasks frees humans for more creative, interpersonal, and judgment-intensive work. Demand for human work shifts but doesn’t disappear.\nThe pessimistic scenario: Automation proceeds faster than adaptation. Labor markets experience sustained unemployment or underemployment while adjustment slowly occurs."
  },
  {
    "objectID": "posts/027_ai_labor_economics/index.html#policy-responses",
    "href": "posts/027_ai_labor_economics/index.html#policy-responses",
    "title": "The Automation Equation: AI, Labor, and the Future of Work",
    "section": "Policy Responses",
    "text": "Policy Responses\nIf AI creates significant labor market disruption, what interventions make sense?\nUniversal Basic Income (UBI): Provide everyone a basic income regardless of employment. Ensures baseline security. Concerns: expensive, may reduce work incentives, doesn’t address meaning and purpose that work provides.\nWage insurance: Top up wages for displaced workers who take lower-paying jobs. Smooths transitions without full UBI costs.\nEducation reform: Invest in skills that complement AI rather than compete with it. Creativity, critical thinking, emotional intelligence, technical fluency.\nJob guarantees: Government as employer of last resort. Ensures everyone who wants work can find it. Questions about what jobs and at what cost.\nTaxation of AI: Tax capital (robots, AI systems) to fund labor support. Slows adoption while providing resources for adjustment. Risk: reduces productivity gains.\nReduced work time: If less human labor is needed, share it differently. Shorter work weeks, longer vacations, later career entry. Cultural and economic barriers.\nThere’s no consensus on the right mix. Different values lead to different conclusions."
  },
  {
    "objectID": "posts/027_ai_labor_economics/index.html#what-early-evidence-shows",
    "href": "posts/027_ai_labor_economics/index.html#what-early-evidence-shows",
    "title": "The Automation Equation: AI, Labor, and the Future of Work",
    "section": "What Early Evidence Shows",
    "text": "What Early Evidence Shows\nWe’re still in early days, but some data is emerging:\nCustomer service: Companies report 15-30% productivity gains with LLM assistance. Some are reducing headcount; others are maintaining headcount at higher output.\nProgramming: GitHub Copilot users report faster completion of routine coding tasks. Senior engineers seem to benefit less than juniors. Companies are experimenting with smaller teams doing more.\nContent creation: Copywriting, marketing, and basic journalism are seeing rapid AI integration. Human roles shift toward editing, strategy, and judgment.\nLegal and consulting: Firms are using AI for research, drafting, and analysis. Total employment effects unclear so far.\nIt’s too early to draw strong conclusions about net employment effects. The capability is there; adoption is proceeding; but labor market outcomes take years to fully materialize."
  },
  {
    "objectID": "posts/027_ai_labor_economics/index.html#my-take",
    "href": "posts/027_ai_labor_economics/index.html#my-take",
    "title": "The Automation Equation: AI, Labor, and the Future of Work",
    "section": "My Take",
    "text": "My Take\nThe most likely scenario is not mass unemployment but significant reshuffling. Many jobs will change substantially. Some will disappear; others will emerge. The transition will create both opportunities and hardship.\nWhat concerns me is not the long run—humans have always found new ways to be useful—but the transition speed. If AI capabilities advance faster than labor markets can adapt, we face a period of painful adjustment. How that adjustment is managed (or not) will determine whether AI’s economic benefits are broadly shared or concentrated.\nThe headlines about “AI taking all jobs” are probably wrong. The headlines about “nothing will change” are definitely wrong. The truth is messier, context-dependent, and politically contested. That makes it harder to write about but more important to understand."
  },
  {
    "objectID": "posts/016_diffusion_models/index.html",
    "href": "posts/016_diffusion_models/index.html",
    "title": "Score Functions and Noise Schedules: A Technical Primer on Diffusion Models",
    "section": "",
    "text": "Diffusion models have become the dominant paradigm for image generation and are extending to video, audio, 3D, molecules, and beyond. This post provides a technical grounding: what diffusion models actually learn, how sampling works, and how to think about the design choices that distinguish different variants."
  },
  {
    "objectID": "posts/016_diffusion_models/index.html#the-forward-process-adding-noise",
    "href": "posts/016_diffusion_models/index.html#the-forward-process-adding-noise",
    "title": "Score Functions and Noise Schedules: A Technical Primer on Diffusion Models",
    "section": "The Forward Process: Adding Noise",
    "text": "The Forward Process: Adding Noise\nA diffusion model starts by defining a forward process that gradually destroys data by adding noise. Given a data point \\(x_0\\), we produce a sequence of increasingly noisy versions \\(x_1, x_2, \\ldots, x_T\\) by iteratively adding Gaussian noise:\n\\[q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)\\]\nwhere \\(\\beta_t\\) is a “noise schedule” that controls how much noise is added at each step.\nA key mathematical property: you can skip directly from \\(x_0\\) to \\(x_t\\) without simulating all intermediate steps:\n\\[q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_t) I)\\]\nwhere \\(\\bar{\\alpha}_t = \\prod_{s=1}^{t} (1 - \\beta_s)\\).\nAs \\(t \\to T\\), the data is completely destroyed—\\(x_T\\) is approximately standard Gaussian noise. Training involves learning to reverse this process."
  },
  {
    "objectID": "posts/016_diffusion_models/index.html#score-matching-learning-log-px",
    "href": "posts/016_diffusion_models/index.html#score-matching-learning-log-px",
    "title": "Score Functions and Noise Schedules: A Technical Primer on Diffusion Models",
    "section": "Score Matching: Learning ∇log p(x)",
    "text": "Score Matching: Learning ∇log p(x)\nHere’s the core insight. If you knew the score function \\(\\nabla_x \\log p(x)\\)—the gradient of the log probability density—you could sample from \\(p(x)\\) using Langevin dynamics:\n\\[x_{k+1} = x_k + \\frac{\\epsilon}{2} \\nabla_x \\log p(x_k) + \\sqrt{\\epsilon} z, \\quad z \\sim \\mathcal{N}(0, I)\\]\nStart from noise, follow the score uphill (toward higher probability), add a bit of noise for exploration, and eventually you sample from \\(p(x)\\).\nThe problem: we don’t know \\(p(x)\\), so we can’t compute its gradient. But we can learn to approximate it.\nScore matching trains a neural network \\(s_\\theta(x)\\) to predict \\(\\nabla_x \\log p(x)\\) using the objective:\n\\[\\mathbb{E}_{x \\sim p} \\left[ \\| s_\\theta(x) - \\nabla_x \\log p(x) \\|^2 \\right]\\]\nThis looks circular (how do you compute the target?), but there’s a trick: denoising score matching shows that learning to denoise is equivalent to learning the score."
  },
  {
    "objectID": "posts/016_diffusion_models/index.html#denoising-score-matching-learn-to-denoise",
    "href": "posts/016_diffusion_models/index.html#denoising-score-matching-learn-to-denoise",
    "title": "Score Functions and Noise Schedules: A Technical Primer on Diffusion Models",
    "section": "Denoising Score Matching: Learn to Denoise",
    "text": "Denoising Score Matching: Learn to Denoise\nGiven clean data \\(x_0\\), add noise to get \\(x_t\\). The score of the noisy distribution points toward the clean data:\n\\[\\nabla_{x_t} \\log q(x_t | x_0) = -\\frac{x_t - \\sqrt{\\bar{\\alpha}_t} x_0}{1 - \\bar{\\alpha}_t}\\]\nThis is proportional to the direction from noisy \\(x_t\\) to clean \\(x_0\\). So if you train a network to predict this direction, you’re implicitly learning the score.\nIn practice, most implementations train a network \\(\\epsilon_\\theta(x_t, t)\\) to predict the noise \\(\\epsilon\\) that was added:\n\\[\\mathcal{L} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right]\\]\nwhere \\(x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon\\) and \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).\nPredicting noise and predicting score are equivalent up to a scaling factor. The noise prediction framing is more numerically stable in practice."
  },
  {
    "objectID": "posts/016_diffusion_models/index.html#noise-schedules-linear-cosine-learned",
    "href": "posts/016_diffusion_models/index.html#noise-schedules-linear-cosine-learned",
    "title": "Score Functions and Noise Schedules: A Technical Primer on Diffusion Models",
    "section": "Noise Schedules: Linear, Cosine, Learned",
    "text": "Noise Schedules: Linear, Cosine, Learned\nThe noise schedule \\(\\beta_t\\) (equivalently, \\(\\bar{\\alpha}_t\\)) determines how quickly information is destroyed during the forward process and how sampling progresses during generation.\nLinear schedule: The original DDPM paper used \\(\\beta_t\\) increasing linearly from \\(\\beta_1\\) to \\(\\beta_T\\). Simple but destroys high-frequency detail too quickly relative to low-frequency structure.\nCosine schedule: Designed so that \\(\\bar{\\alpha}_t\\) follows a cosine curve, keeping more information at intermediate timesteps. This produces better samples, especially for images with fine detail.\nLearned schedules: Some work optimizes the schedule jointly with the model. The optimal schedule depends on the data distribution—images, audio, and molecules may want different curves.\nContinuous-time formulations: Instead of discrete steps \\(t = 1, \\ldots, T\\), some formulations use continuous time \\(t \\in [0, 1]\\). This enables more flexible sampling strategies and cleaner theoretical analysis.\nThe schedule is a hyperparameter that meaningfully affects sample quality. Getting it wrong can make sampling slow (too many steps needed) or produce artifacts (information destroyed at wrong rates)."
  },
  {
    "objectID": "posts/016_diffusion_models/index.html#sampling-ddpm-ddim-and-beyond",
    "href": "posts/016_diffusion_models/index.html#sampling-ddpm-ddim-and-beyond",
    "title": "Score Functions and Noise Schedules: A Technical Primer on Diffusion Models",
    "section": "Sampling: DDPM, DDIM, and Beyond",
    "text": "Sampling: DDPM, DDIM, and Beyond\nOnce trained, how do you actually generate samples?\nDDPM sampling: The original approach reverses the forward process step by step. At each step, use the predicted noise to estimate the denoised image, then resample with slightly less noise. This requires ~1000 steps and is slow.\n\\[x_{t-1} = \\frac{1}{\\sqrt{1-\\beta_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t z\\]\nDDIM (Denoising Diffusion Implicit Models): Reinterprets the generative process as solving an ODE rather than an SDE. This allows: - Deterministic sampling (same noise → same output) - Fewer steps (50-100 instead of 1000) - Interpolation in latent space\nODE solvers: Since the continuous-time limit of diffusion is an ODE, you can use standard numerical ODE solvers (Euler, Heun, RK45) with adaptive step sizes. Better solvers need fewer function evaluations.\nDistillation: Train a student model to predict the final output in fewer steps than the teacher. Progressive distillation can reduce steps from 1000 → 500 → 250 → … → 4.\nThe practical upshot: modern diffusion models generate high-quality images in 20-50 steps, not the 1000 originally required."
  },
  {
    "objectID": "posts/016_diffusion_models/index.html#classifier-free-guidance",
    "href": "posts/016_diffusion_models/index.html#classifier-free-guidance",
    "title": "Score Functions and Noise Schedules: A Technical Primer on Diffusion Models",
    "section": "Classifier-Free Guidance",
    "text": "Classifier-Free Guidance\nHow do you control what a diffusion model generates? One answer: guidance.\nClassifier guidance uses an external classifier \\(p(y|x_t)\\) to steer generation toward class \\(y\\). During sampling, modify the score:\n\\[\\tilde{\\nabla} \\log p(x_t | y) = \\nabla \\log p(x_t) + \\gamma \\nabla \\log p(y | x_t)\\]\nPush toward both high probability and high classifier score. The weight \\(\\gamma\\) controls how strongly to follow the guidance.\nClassifier-free guidance eliminates the separate classifier. Instead, train the diffusion model with dropout on the conditioning (sometimes train unconditional, sometimes conditional). At sampling time, interpolate:\n\\[\\tilde{\\epsilon} = \\epsilon_\\theta(x_t, t, \\emptyset) + \\gamma \\left( \\epsilon_\\theta(x_t, t, c) - \\epsilon_\\theta(x_t, t, \\emptyset) \\right)\\]\nwhere \\(c\\) is the conditioning (text prompt, class label) and \\(\\emptyset\\) is the unconditional case.\nWith \\(\\gamma &gt; 1\\), you over-emphasize the conditional direction—making samples more aligned with the prompt at the cost of diversity. This is why “guidance scale” appears in DALL-E, Stable Diffusion, and Midjourney settings."
  },
  {
    "objectID": "posts/016_diffusion_models/index.html#latent-diffusion-stable-diffusion-architecture",
    "href": "posts/016_diffusion_models/index.html#latent-diffusion-stable-diffusion-architecture",
    "title": "Score Functions and Noise Schedules: A Technical Primer on Diffusion Models",
    "section": "Latent Diffusion: Stable Diffusion Architecture",
    "text": "Latent Diffusion: Stable Diffusion Architecture\nRunning diffusion directly on high-resolution images is expensive. A 512×512 RGB image has ~800K dimensions. That’s a lot of score function evaluations.\nLatent diffusion compresses images first:\n\nTrain a variational autoencoder (VAE) with encoder \\(\\mathcal{E}\\) and decoder \\(\\mathcal{D}\\)\nEncode images to latent space: \\(z = \\mathcal{E}(x)\\), typically 64×64×4 (~16K dims)\nTrain the diffusion model in latent space\nAt generation time, sample \\(z\\), then decode \\(x = \\mathcal{D}(z)\\)\n\nThe VAE handles pixel-level detail; the diffusion model handles semantic structure. This is 16-64× more efficient than pixel-space diffusion.\nStable Diffusion is a latent diffusion model with: - A pretrained VAE for image compression - A U-Net architecture for the diffusion model - CLIP text encoder for conditioning - Cross-attention to inject text embeddings into the U-Net"
  },
  {
    "objectID": "posts/016_diffusion_models/index.html#applications-beyond-images",
    "href": "posts/016_diffusion_models/index.html#applications-beyond-images",
    "title": "Score Functions and Noise Schedules: A Technical Primer on Diffusion Models",
    "section": "Applications Beyond Images",
    "text": "Applications Beyond Images\nThe diffusion framework is surprisingly general. Anything you can add Gaussian noise to, you can learn to denoise.\nVideo generation: Add temporal dimensions. Challenges include maintaining consistency across frames and scaling to longer durations. Sora, Runway, and others are pushing this frontier.\nAudio: Diffusion over spectrograms or waveforms. Used for music generation, speech synthesis, and sound effects.\n3D: Diffusion over point clouds, neural radiance fields, or mesh representations. Enables text-to-3D generation.\nMolecules: Diffusion over 3D atomic coordinates. AlphaFold 3 uses diffusion for structure prediction. Drug discovery applies diffusion to generate novel molecules with desired properties.\nRobotics: Diffusion for trajectory planning. Sample diverse trajectories, then select or refine. Useful when you want multimodal predictions (multiple valid ways to accomplish a task).\nText: More challenging because text is discrete, but recent work maps discrete tokens to continuous embeddings, runs diffusion there, and maps back."
  },
  {
    "objectID": "posts/016_diffusion_models/index.html#conceptual-summary",
    "href": "posts/016_diffusion_models/index.html#conceptual-summary",
    "title": "Score Functions and Noise Schedules: A Technical Primer on Diffusion Models",
    "section": "Conceptual Summary",
    "text": "Conceptual Summary\nThe diffusion framework reduces generative modeling to a simple idea:\n\nDefine a process that destroys data by adding noise\nLearn to reverse that process by predicting the noise\nSample by starting from noise and iteratively denoising\n\nThe magic is that predicting noise is a well-defined regression problem (unlike trying to model \\(p(x)\\) directly), and the learned denoiser implicitly captures gradient information about the data distribution.\nAll the variants—different noise schedules, sampling methods, conditioning strategies, latent spaces—are refinements of this core loop. Once you understand the basic mechanism, the rest is engineering.\n\n  \n  Figure 1. The diffusion process: adding noise (forward) and learning to denoise (reverse)"
  },
  {
    "objectID": "posts/005_breakthrough1/index.html",
    "href": "posts/005_breakthrough1/index.html",
    "title": "Breakthrough #1 — Steering: How the First Brains Solved Navigation",
    "section": "",
    "text": "Bennett’s first “breakthrough” is intentionally humble. He isn’t trying to start the history of intelligence with something that resembles human thought. He starts with the first situation where having a nervous system pays for itself: once an organism can move, it must solve a control problem in real time.\nThat control problem has a very specific flavor. The organism is bombarded by noisy sensory streams. The world is patchy: food is not everywhere, danger is not everywhere. The organism has limited energy and limited time. So it needs a way to pick the next motor command that tends, on average, to move it toward beneficial states and away from harmful ones. That’s steering.\nWhat’s easy to miss is that “steering” is already more than a reflex. Reflexes are local and brittle: they fire when a trigger appears, they don’t reconcile conflicts, and they don’t stabilize behavior over time. Steering, as Bennett uses the term, is the first place you see two properties that later become hallmarks of intelligence:\n\nIntegration. Multiple cues compete. Odor says “go,” pain says “stop,” heat says “leave,” social signal says “approach.” Steering is the machinery that merges these into one coherent directional push.\nCommitment. If you change direction on every small fluctuation, you don’t get anywhere. Early nervous systems therefore need internal state—slow variables that persist and bias behavior for seconds to minutes. That’s how you get stable modes like exploration versus exploitation, approach versus avoidance, or “stay and forage” versus “get out now.”\n\nBennett’s core claim is that the earliest brains build a tiny but powerful abstraction: valence. The world is carved into “good-for-me” and “bad-for-me” signals. That carving can be wired at the start (some stimuli are intrinsically appetitive or aversive), but it becomes conditional when internal state gets involved. The same cue can flip meaning depending on hunger, thirst, fatigue, or stress.\nThis is the evolutionary origin of the objective function. Before you get learning, you get a stable notion of what counts as better versus worse. It sounds obvious, but it’s foundational: any agent-like system becomes interpretable only when you know what it’s trying to optimize. Bennett is saying: biology solved objective specification first, then built increasingly powerful machinery for optimizing it.\nA good way to see how strong steering can be is to picture an organism in a gradient (odor, temperature, salinity). If the signal gets better as it moves, keep going. If it gets worse, reorient. That kind of policy can look surprisingly purposeful because the environment itself is providing dense guidance. You don’t need a map if the world gives you “hotter/colder” feedback every second.\nThere’s a deeper lesson here that generalizes beyond biology: a lot of competence comes from exploiting environmental structure, not from building elaborate internal representations. If the world offers smooth gradients, a small controller goes a long way. And if the world stops offering those gradients—if rewards become delayed, sparse, or deceptive—then the same controller hits a wall. Bennett’s whole staircase can be read as a sequence of upgrades that appear when the world becomes too hard for the previous controller class.\nFrom a math/ML viewpoint, I’d describe steering as a small, stable policy class coupled to a hard-coded reward prior. Early intelligence is not “general reasoning.” It’s the ability to turn sensory correlations into motion that improves survival. That’s why steering is such a good starting point: it forces you to take intelligence seriously as embodied control, not as disembodied problem-solving.\nIf you want a pointed discussion question: what would count as the minimal internal state required to make steering robust? You can’t just say “approach good / avoid bad.” You need persistence, conflict resolution, and the ability to change what is “good” when the body’s needs change. That’s already the seed of later reinforcement learning.\n\n  \n  Figure 1. Gradient-following behavior as a minimal navigation strategy"
  },
  {
    "objectID": "posts/028_test_time_compute/index.html",
    "href": "posts/028_test_time_compute/index.html",
    "title": "Thinking Longer: Test-Time Compute and the Future of Inference Scaling",
    "section": "",
    "text": "The dominant paradigm for improving language models has been scaling training: more parameters, more data, more compute. But there’s another dimension: test-time compute—spending more resources at inference time to get better answers.\nHumans do this naturally. Hard problems require more thought. You don’t solve a complex proof in one mental step; you work through intermediate results, backtrack when stuck, and verify your reasoning. What happens when we teach AI systems to do the same?"
  },
  {
    "objectID": "posts/028_test_time_compute/index.html#the-test-time-compute-hypothesis",
    "href": "posts/028_test_time_compute/index.html#the-test-time-compute-hypothesis",
    "title": "Thinking Longer: Test-Time Compute and the Future of Inference Scaling",
    "section": "The Test-Time Compute Hypothesis",
    "text": "The Test-Time Compute Hypothesis\nThe core idea: a small model that “thinks” for 10 seconds might outperform a large model that answers in 100 milliseconds. If you’re willing to spend compute at inference time, you can trade it for training-time compute.\nThis isn’t free. Inference compute costs money and time. But for many applications—complex reasoning, code generation, mathematical proofs—accuracy matters more than latency. And inference compute is more flexible: you can allocate it dynamically based on problem difficulty.\nThe question is how to spend that compute productively. Simply generating more tokens doesn’t help if those tokens are noise. You need structured ways to explore, evaluate, and refine."
  },
  {
    "objectID": "posts/028_test_time_compute/index.html#chain-of-thought-prompting",
    "href": "posts/028_test_time_compute/index.html#chain-of-thought-prompting",
    "title": "Thinking Longer: Test-Time Compute and the Future of Inference Scaling",
    "section": "Chain-of-Thought Prompting",
    "text": "Chain-of-Thought Prompting\nThe simplest form of test-time compute: ask the model to show its work.\nChain-of-thought (CoT) prompting includes examples with step-by-step reasoning, then asks the model to produce similar intermediate steps. This dramatically improves performance on math, logic, and multi-step reasoning problems.\nWhy does it work? Several hypotheses:\n\nSerialized computation: Transformers have limited depth. Generating intermediate tokens effectively adds more “layers” of computation.\nError decomposition: Breaking problems into steps exposes intermediate results that are easier to verify and correct.\nTraining distribution: Models are trained on text that includes reasoning. Prompting them to reason recovers abilities learned during training.\n\nZero-shot CoT: Just adding “Let’s think step by step” to the prompt induces reasoning without examples. This suggests CoT is unlocking something the model already knows how to do.\nLimitation: CoT is a single forward pass. The model generates one chain and commits to it. It can’t explore alternatives or backtrack."
  },
  {
    "objectID": "posts/028_test_time_compute/index.html#best-of-n-and-self-consistency",
    "href": "posts/028_test_time_compute/index.html#best-of-n-and-self-consistency",
    "title": "Thinking Longer: Test-Time Compute and the Future of Inference Scaling",
    "section": "Best-of-N and Self-Consistency",
    "text": "Best-of-N and Self-Consistency\nGenerate multiple answers; pick the best one.\nBest-of-N sampling: Generate N independent completions, score them somehow (model confidence, ground truth if available, external verifier), and return the best. Simple but effective. Often 4-8 samples provide most of the gain.\nSelf-consistency: For problems with a single correct answer (math, factual questions), generate N reasoning chains and take the majority vote on the final answer. Different reasoning paths might make different intermediate errors but converge on the correct final answer.\nThese methods use test-time compute to reduce variance—sampling multiple times and aggregating. They work because model stochasticity produces diverse solutions, some of which are better than others.\nLimitation: Linear cost in N. Each sample requires a full forward pass. And if all N chains converge to the same wrong answer, more sampling doesn’t help."
  },
  {
    "objectID": "posts/028_test_time_compute/index.html#tree-search-for-language",
    "href": "posts/028_test_time_compute/index.html#tree-search-for-language",
    "title": "Thinking Longer: Test-Time Compute and the Future of Inference Scaling",
    "section": "Tree Search for Language",
    "text": "Tree Search for Language\nInstead of generating complete sequences and scoring afterward, build a tree of partial sequences and search.\nMonte Carlo Tree Search (MCTS): The approach that powered AlphaGo. Treat sequence generation as a game tree. At each position, expand promising branches, simulate to completion, and backpropagate value estimates. Balance exploration (trying new branches) and exploitation (deepening good ones).\nApplied to language: 1. Start with prompt 2. Generate several possible continuations (tokens or phrases) 3. Score each branch with a value estimate 4. Expand the most promising branches 5. Continue until a complete answer, then backpropagate\nThis enables backtracking: if a reasoning path leads to a dead end, you can return to an earlier branch and try differently.\nProcess Reward Models (PRMs): Train a model to score intermediate reasoning steps, not just final answers. A PRM can identify when reasoning goes wrong before reaching the conclusion, enabling earlier pruning.\nOutcome Reward Models (ORMs): Score only final answers. Simpler to train (you just need answer labels) but less useful for guiding search.\nTree search with PRMs is how recent “reasoning models” achieve their performance. The model doesn’t just generate one chain; it explores a tree of possibilities, guided by learned value estimates."
  },
  {
    "objectID": "posts/028_test_time_compute/index.html#self-critique-and-iterative-refinement",
    "href": "posts/028_test_time_compute/index.html#self-critique-and-iterative-refinement",
    "title": "Thinking Longer: Test-Time Compute and the Future of Inference Scaling",
    "section": "Self-Critique and Iterative Refinement",
    "text": "Self-Critique and Iterative Refinement\nGenerate once, then improve.\nSelf-critique: Ask the model to evaluate its own answer. “What might be wrong with this solution?” “Are there any errors in this proof?” The model often identifies issues it failed to avoid during generation.\nIterative refinement: Generate → critique → revise → repeat. Each pass can fix errors from the previous one. Analogous to how humans edit their writing.\nConstitutional AI-style loops: Define principles (“be helpful, harmless, honest”), generate candidate responses, rank them by the principles, and train on the rankings. The same idea applies at inference: generate, critique against principles, and revise.\nWhy can models catch errors on review that they made during generation? Partly because generation is autoregressive and committing—once tokens are emitted, they influence subsequent generation. Review operates on a complete artifact and can consider global coherence."
  },
  {
    "objectID": "posts/028_test_time_compute/index.html#verifier-guided-generation",
    "href": "posts/028_test_time_compute/index.html#verifier-guided-generation",
    "title": "Thinking Longer: Test-Time Compute and the Future of Inference Scaling",
    "section": "Verifier-Guided Generation",
    "text": "Verifier-Guided Generation\nUse a separate model to verify outputs, and let that verification guide generation.\nCode: Generate code → run tests → if tests fail, generate again with error context. The test suite is an external verifier. This is how many code-completion systems work in practice.\nMath: Generate proof steps → check with a formal verifier (Lean, Coq) → if verification fails, backtrack. The theorem prover provides ground truth.\nFactual claims: Generate → retrieve sources → verify claims against sources → revise. Retrieval-augmented generation as verification.\nExternal verifiers are powerful because they provide reliable signal. If your code doesn’t compile, that’s ground truth—no model uncertainty involved. The challenge is that not all tasks have clean external verification."
  },
  {
    "objectID": "posts/028_test_time_compute/index.html#compute-optimal-inference",
    "href": "posts/028_test_time_compute/index.html#compute-optimal-inference",
    "title": "Thinking Longer: Test-Time Compute and the Future of Inference Scaling",
    "section": "Compute-Optimal Inference",
    "text": "Compute-Optimal Inference\nWhen should you think more? Not every query deserves the same effort.\nAdaptive compute: Estimate problem difficulty and allocate inference compute accordingly. Simple questions get one-shot answers; hard questions get tree search and verification.\nDifficulty estimation is its own challenge: - Model confidence (entropy over tokens) is one signal but unreliable - Query characteristics (length, complexity, domain) provide hints - Start with cheap computation; escalate if initial results seem uncertain\nCascading: Try a small, fast model first. If confidence is low, hand off to a larger model. Most queries might be handled cheaply; only hard cases pay the full cost.\nSpeculative decoding: Use a small model to draft continuations; verify with the large model. If verification passes, you’ve generated many tokens cheaply. If not, fall back to the large model. This accelerates easy continuations while maintaining large-model quality."
  },
  {
    "objectID": "posts/028_test_time_compute/index.html#openais-o-series-and-the-reasoning-model-paradigm",
    "href": "posts/028_test_time_compute/index.html#openais-o-series-and-the-reasoning-model-paradigm",
    "title": "Thinking Longer: Test-Time Compute and the Future of Inference Scaling",
    "section": "OpenAI’s “o-series” and the Reasoning Model Paradigm",
    "text": "OpenAI’s “o-series” and the Reasoning Model Paradigm\nOpenAI’s o1 (and predecessors like “Strawberry”) represent this paradigm taken seriously. Key characteristics:\n\nExtended reasoning: The model generates substantial internal reasoning before answering. This might be hidden from users but consumes inference compute.\nProcess supervision: Trained with reward on intermediate reasoning steps, not just final answers.\nSearch: Likely uses some form of tree search or best-of-N at inference time.\nSpecialized for reasoning: Optimized for math, code, and logic where verification is possible.\n\nThe result: dramatically better performance on hard reasoning benchmarks (AIME math, competition programming) at the cost of higher latency and expense.\nThis points toward a future where you might choose between: - Fast, cheap models for simple queries - Slow, expensive reasoning models for hard problems\nCompute becomes a knob you turn based on problem difficulty and quality requirements."
  },
  {
    "objectID": "posts/028_test_time_compute/index.html#implications-and-trade-offs",
    "href": "posts/028_test_time_compute/index.html#implications-and-trade-offs",
    "title": "Thinking Longer: Test-Time Compute and the Future of Inference Scaling",
    "section": "Implications and Trade-offs",
    "text": "Implications and Trade-offs\nLatency vs. quality: More thinking means slower responses. Acceptable for some applications (research, coding, analysis) but not others (chat, real-time decisions).\nCost: Inference compute isn’t free. Tree search with process reward models can cost 10-100× single-pass inference. This changes the economics of AI applications.\nTraining incentives: If models are deployed with test-time search, training should optimize for search performance, not just single-pass accuracy. This is an active research area.\nTransparency: Hidden reasoning (as in some o1 deployments) trades interpretability for performance. Users may not understand why answers take time or why they cost more.\nDiminishing returns: At some point, no amount of inference compute helps. If the model doesn’t have the right knowledge or capability, thinking longer doesn’t create it."
  },
  {
    "objectID": "posts/028_test_time_compute/index.html#the-bigger-picture",
    "href": "posts/028_test_time_compute/index.html#the-bigger-picture",
    "title": "Thinking Longer: Test-Time Compute and the Future of Inference Scaling",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\nTest-time compute represents a shift in how we think about AI capability. Instead of a fixed model with fixed abilities, we have a continuum: spend more compute, get better answers.\nThis is closer to how intelligence works in nature. Humans don’t have one-shot answers to hard problems. We think, revise, verify, and iterate. The question is whether current architectures—transformers with autoregressive generation—can fully exploit this paradigm, or whether deeper architectural changes are needed.\nEither way, the message is clear: model quality at deployment isn’t determined only by training. How you use the model matters too.\n\n  \n  Figure 1. Trading off training compute vs. inference compute"
  },
  {
    "objectID": "posts/019_ai_productivity_paradox/index.html",
    "href": "posts/019_ai_productivity_paradox/index.html",
    "title": "Where’s the Productivity? Why AI Gains Are Hard to Measure",
    "section": "",
    "text": "Generative AI seems transformative. Demos are impressive. Companies report productivity gains. Yet official productivity statistics remain stubbornly flat. This gap—between perceived and measured productivity—echoes earlier technology puzzles. Understanding why is essential for predicting whether AI will eventually deliver economic transformation."
  },
  {
    "objectID": "posts/019_ai_productivity_paradox/index.html#the-solow-paradox",
    "href": "posts/019_ai_productivity_paradox/index.html#the-solow-paradox",
    "title": "Where’s the Productivity? Why AI Gains Are Hard to Measure",
    "section": "The Solow Paradox",
    "text": "The Solow Paradox\nIn 1987, economist Robert Solow famously observed: “You can see the computer age everywhere but in the productivity statistics.” For decades after mainframes and PCs spread through business, measured productivity growth remained low.\nThen, in the late 1990s, productivity spiked. The returns arrived, just delayed. Explanations for the delay include:\n\nMeasurement lags: Statistics take time to capture new technologies\nAdoption lags: Technology must diffuse through the economy\nComplementary investments: Organizations must reorganize to exploit new technology\nLearning curves: Workers must develop new skills\n\nAre we in a similar Solow paradox moment with AI? The technology is visible, but the productivity gains haven’t yet materialized in aggregate data."
  },
  {
    "objectID": "posts/019_ai_productivity_paradox/index.html#measurement-problems",
    "href": "posts/019_ai_productivity_paradox/index.html#measurement-problems",
    "title": "Where’s the Productivity? Why AI Gains Are Hard to Measure",
    "section": "Measurement Problems",
    "text": "Measurement Problems\nProductivity = output / labor hours. Simple in principle; fiendishly hard in practice.\nQuality adjustment: If software is 10× better but costs the same, output should be counted as higher. But how do you measure “10× better”? Statistical agencies try to quality-adjust, but it’s imperfect and subjective.\nNew products: How do you measure the productivity of something that didn’t exist before? Google Search, smartphones, ChatGPT—these create value that traditional output measures struggle to capture.\nIntangibles: Modern economies produce software, data, and organizational capability—not physical goods. These are hard to measure and often not counted as investment.\nFree services: Many AI applications are free to users (search, translation, content moderation). They generate consumer surplus but show up as zero in output statistics.\nSector heterogeneity: Productivity might be soaring in tech and stagnant in healthcare. Aggregate statistics average over everything.\nThe honest answer: we don’t really know if AI is boosting productivity because we don’t have great tools to measure it."
  },
  {
    "objectID": "posts/019_ai_productivity_paradox/index.html#adoption-j-curves",
    "href": "posts/019_ai_productivity_paradox/index.html#adoption-j-curves",
    "title": "Where’s the Productivity? Why AI Gains Are Hard to Measure",
    "section": "Adoption J-Curves",
    "text": "Adoption J-Curves\nTechnology adoption typically follows a pattern:\n\nEarly investment: Buy the technology, train workers, reorganize processes. This is costly.\nProductivity dip: Initially, output may fall as workers learn and systems are debugged.\nProductivity gains: Eventually, the technology pays off. But “eventually” can be years.\n\nIf you measured electricity’s impact in 1910, you’d see costs without benefits. Factories had to be rebuilt around electric motors (rather than central steam engines with belt drives). The productivity gains came in the 1920s.\nWe may be in the investment phase of AI adoption. Companies are spending billions on compute, tooling, and integration. The payoff comes later."
  },
  {
    "objectID": "posts/019_ai_productivity_paradox/index.html#complementary-investments",
    "href": "posts/019_ai_productivity_paradox/index.html#complementary-investments",
    "title": "Where’s the Productivity? Why AI Gains Are Hard to Measure",
    "section": "Complementary Investments",
    "text": "Complementary Investments\nNew technology rarely produces value in isolation. You need:\nWorker skills: Employees must learn to use AI tools effectively. This takes training and experience. Prompt engineering, AI-assisted workflows, trust calibration—all require learning.\nProcess redesign: Existing processes were designed for humans without AI. To fully exploit AI, you need new processes. This requires management attention and organizational experimentation.\nData and infrastructure: AI systems require data pipelines, integration with existing systems, and appropriate governance. This is expensive and slow.\nCultural change: Organizations have to believe AI is worth using and trust it appropriately. This doesn’t happen overnight.\nUntil these complementary investments are made, AI sits in organizations without producing full value. This explains why companies can adopt AI without immediately seeing productivity gains."
  },
  {
    "objectID": "posts/019_ai_productivity_paradox/index.html#where-productivity-gains-are-visible",
    "href": "posts/019_ai_productivity_paradox/index.html#where-productivity-gains-are-visible",
    "title": "Where’s the Productivity? Why AI Gains Are Hard to Measure",
    "section": "Where Productivity Gains Are Visible",
    "text": "Where Productivity Gains Are Visible\nAt the micro level, individual applications do show clear gains:\nCoding: GitHub data suggests Copilot users complete tasks 55% faster (though they also accept much AI-generated code that may require later fixing). Other studies show smaller but real effects.\nCustomer service: Companies report 15-30% productivity improvements with LLM-assisted agents. More tickets resolved per agent-hour.\nWriting and documentation: First-draft generation is dramatically faster. Editing AI output is faster than writing from scratch.\nTranslation: Machine translation has largely replaced human translation for many low-stakes applications.\nResearch assistance: Literature review, summarization, and brainstorming are accelerated.\nThese micro-level gains are real but may not yet show up in aggregate statistics due to limited diffusion and measurement issues."
  },
  {
    "objectID": "posts/019_ai_productivity_paradox/index.html#general-purpose-technology-theory",
    "href": "posts/019_ai_productivity_paradox/index.html#general-purpose-technology-theory",
    "title": "Where’s the Productivity? Why AI Gains Are Hard to Measure",
    "section": "General-Purpose Technology Theory",
    "text": "General-Purpose Technology Theory\nSome technologies are general-purpose technologies (GPTs)—broadly applicable innovations that transform many sectors over decades. Economists often cite three historical GPTs:\n\nSteam engine: Took ~50 years to fully transform manufacturing, transportation\nElectricity: Took ~30 years from adoption to productivity surge\nComputers/Internet: Took ~20 years; productivity boom in late 1990s\n\nIf AI is a GPT, we should expect: - Long diffusion period - Slow initial productivity gains - Eventual large, broad-based transformation\nWhether AI is truly a GPT—and not just a narrow capability improvement—remains debated. Its breadth of application suggests it might be. But history isn’t prediction."
  },
  {
    "objectID": "posts/019_ai_productivity_paradox/index.html#baumols-cost-disease",
    "href": "posts/019_ai_productivity_paradox/index.html#baumols-cost-disease",
    "title": "Where’s the Productivity? Why AI Gains Are Hard to Measure",
    "section": "Baumol’s Cost Disease",
    "text": "Baumol’s Cost Disease\nWilliam Baumol observed that some sectors resist productivity improvements. A string quartet takes the same time to play today as in 1800. Hospital stays require human attention. Education involves teacher-student interaction.\nThese labor-intensive services tend to see costs rise over time (as wages increase with general productivity) without corresponding output gains. This is Baumol’s cost disease.\nAI might attack precisely these resistant sectors: - Healthcare: diagnostic assistance, administrative automation - Education: tutoring, personalized instruction - Government: form processing, case management\nIf AI cures Baumol’s cost disease in these sectors, the productivity effects would be large. But these sectors are also heavily regulated and culturally resistant to change. The potential is there; whether it’s realized is uncertain."
  },
  {
    "objectID": "posts/019_ai_productivity_paradox/index.html#what-to-expect",
    "href": "posts/019_ai_productivity_paradox/index.html#what-to-expect",
    "title": "Where’s the Productivity? Why AI Gains Are Hard to Measure",
    "section": "What to Expect",
    "text": "What to Expect\nBased on historical patterns and current evidence, my expectations:\nShort term (1-3 years): Visible improvements in specific applications. Aggregate productivity statistics remain flat or modestly up. Measurement challenges obscure effects.\nMedium term (3-10 years): Broader adoption as complementary investments mature. Process redesign catches up. Productivity gains begin appearing in official statistics. Uneven distribution across sectors.\nLong term (10+ years): If AI is a true GPT, potentially transformative effects comparable to electricity or computers. New industries, new products, reshaped labor markets. But this is speculative.\nLeading indicators: Watch for: - Rising firm-level productivity correlating with AI adoption - Labor reallocation (workers shifting between tasks and industries) - New product categories enabled by AI - Process patents and organizational innovations"
  },
  {
    "objectID": "posts/019_ai_productivity_paradox/index.html#why-this-matters",
    "href": "posts/019_ai_productivity_paradox/index.html#why-this-matters",
    "title": "Where’s the Productivity? Why AI Gains Are Hard to Measure",
    "section": "Why This Matters",
    "text": "Why This Matters\nThe productivity question isn’t academic. If AI delivers large productivity gains: - Economic growth accelerates - Living standards rise - Some labor displacement occurs but new opportunities emerge\nIf AI doesn’t deliver—if it’s expensive hype without substance: - Massive investment turns out to be misallocated - AI companies disappoint - Expectations reset painfully\nThe truth is probably in between. AI will deliver real value, but unevenly, slowly, and with measurement difficulties that make the gains hard to see until they’ve already arrived.\nFor individuals and organizations, the implication is clear: don’t wait for aggregate statistics to tell you what to do. The micro-level gains are already real enough to act on. The macro-level measurement will catch up eventually."
  },
  {
    "objectID": "posts/025_ai_startup_landscape/index.html",
    "href": "posts/025_ai_startup_landscape/index.html",
    "title": "The AI Startup Stack: Opportunities, Commoditization Risks, and Defensibility",
    "section": "",
    "text": "The AI startup boom has produced thousands of companies across every layer of the stack. Some will become giants. Most will die or be acqui-hired. The difference often isn’t technical capability—it’s strategic positioning. Where in the value chain are you? What makes your position defensible? Can you build a business that persists once the technology is commoditized?"
  },
  {
    "objectID": "posts/025_ai_startup_landscape/index.html#the-ai-value-chain",
    "href": "posts/025_ai_startup_landscape/index.html#the-ai-value-chain",
    "title": "The AI Startup Stack: Opportunities, Commoditization Risks, and Defensibility",
    "section": "The AI Value Chain",
    "text": "The AI Value Chain\nThink of AI as a stack with value distributed across layers:\nInfrastructure: Compute, chips, cloud, training infrastructure. Players: NVIDIA, cloud providers, CoreWeave. High capex, high margins for winners, brutal for losers.\nModels: Foundation models, fine-tuning, specialized models. Players: OpenAI, Anthropic, Google, Meta (Llama). High R&D costs, network effects from developer ecosystems.\nTools and platforms: APIs, orchestration, evaluation, deployment. Players: LangChain, Weights & Biases, Hugging Face. Developer adoption as the moat.\nApplications: End-user products built on AI. Players: thousands of startups, plus incumbents adding AI features. Closest to customers, but often dependent on layers below.\nValue concentration shifts over time. Early, models captured most value. As models commoditize, applications and infrastructure may capture more. Figuring out where value will concentrate is the key strategic question."
  },
  {
    "objectID": "posts/025_ai_startup_landscape/index.html#commoditization-pressure",
    "href": "posts/025_ai_startup_landscape/index.html#commoditization-pressure",
    "title": "The AI Startup Stack: Opportunities, Commoditization Risks, and Defensibility",
    "section": "Commoditization Pressure",
    "text": "Commoditization Pressure\nThe base models are commoditizing faster than expected:\nOpen-source models: LLaMA, Mistral, and others have reached near-parity with proprietary models for many tasks. If the best model is free, the API provider can’t charge a premium.\nAPI price wars: OpenAI, Anthropic, and Google are cutting prices rapidly. Input tokens went from $0.03 to $0.001 in two years. Inference is becoming cheap.\nLow switching costs: Standardized prompting interfaces make it easy to switch between providers. If Claude and GPT-4 are interchangeable for your application, neither can charge a premium.\nThis is great for builders but terrifying for model providers. Commoditization pressure flows up the stack: if models are cheap, model providers need volume, which requires developer tools, which requires applications."
  },
  {
    "objectID": "posts/025_ai_startup_landscape/index.html#defensibility-patterns",
    "href": "posts/025_ai_startup_landscape/index.html#defensibility-patterns",
    "title": "The AI Startup Stack: Opportunities, Commoditization Risks, and Defensibility",
    "section": "Defensibility Patterns",
    "text": "Defensibility Patterns\nWhat makes an AI startup defensible once the technology is no longer differentiating?\nData moats: Proprietary data that’s hard to replicate. Bloomberg has financial data; OSHA has regulatory data; hospitals have medical records. If your product generates unique, valuable data, that’s a moat.\nWorkflow embedding: Products that become part of daily workflow are sticky. Once your tool is integrated into CI/CD pipelines, document workflows, or customer service systems, switching is expensive.\nVertical integration: Own the whole experience in a niche. Vertical AI companies can optimize across the stack for their specific domain—better than horizontal providers can.\nNetwork effects: Two-sided marketplaces, collaboration tools, or social products where value increases with users. Hard to create in AI but powerful when achieved.\nSpeed and execution: In early markets, being first with a good-enough product beats being late with a perfect product. But this is temporary; execution alone isn’t a moat.\nRegulatory capture: In regulated industries, existing players have relationships, approvals, and compliance infrastructure. AI startups can inherit this by partnering or acquiring."
  },
  {
    "objectID": "posts/025_ai_startup_landscape/index.html#failure-modes",
    "href": "posts/025_ai_startup_landscape/index.html#failure-modes",
    "title": "The AI Startup Stack: Opportunities, Commoditization Risks, and Defensibility",
    "section": "Failure Modes",
    "text": "Failure Modes\n“Thin wrapper” syndrome: Your product is an API call with a UI. You’ve added prompts, maybe a workflow, but no real value. Model providers can replicate your product trivially.\nWhen OpenAI ships plugins and GPTs, when Anthropic improves Claude, your “thin wrapper” disappears into their platform. You didn’t build defensibility.\nModel dependency: Your product depends on one model provider. They raise prices, change terms, or deprecate the model you’re using. You’re exposed to their roadmap.\nCAC/LTV inversion: Customer acquisition cost exceeds lifetime value. Easy to do in AI: pay for marketing, acquire users, but users churn when the novelty wears off.\nDemo-to-production gap: Your demo is impressive, but reliability in production is terrible. Hallucination rates, latency, cost, and edge cases kill you in real deployment.\nCommoditization from above: You build a specialized AI tool; the foundation model provider absorbs your functionality into their base offering. Game over."
  },
  {
    "objectID": "posts/025_ai_startup_landscape/index.html#vertical-vs.-horizontal",
    "href": "posts/025_ai_startup_landscape/index.html#vertical-vs.-horizontal",
    "title": "The AI Startup Stack: Opportunities, Commoditization Risks, and Defensibility",
    "section": "Vertical vs. Horizontal",
    "text": "Vertical vs. Horizontal\nHorizontal plays sell to everyone: AI writing assistants, code completion, general chatbots. The market is huge, but competition is fierce and differentiation is hard.\nVertical plays focus on specific industries: AI for law firms, AI for healthcare billing, AI for insurance claims. The market is smaller, but: - Less competition - Domain expertise creates barriers - Customers pay more for specialized solutions - Regulatory and compliance requirements lock competitors out\nThe best vertical plays combine AI with deep domain understanding. You’re not “AI for X”—you’re “the best solution for X, powered by AI.” The AI is an implementation detail, not the value proposition."
  },
  {
    "objectID": "posts/025_ai_startup_landscape/index.html#timing",
    "href": "posts/025_ai_startup_landscape/index.html#timing",
    "title": "The AI Startup Stack: Opportunities, Commoditization Risks, and Defensibility",
    "section": "Timing",
    "text": "Timing\nToo early: You’re building on technology that isn’t ready. The AI can’t reliably do what you need. You burn capital waiting for capability to catch up.\nBeing too early is worse than being too late. At least late entrants know what works.\nToo late: Incumbents have added AI features. Startups have established market position. You’re fighting for a slice of a pie that’s already divided.\nJust right: Technology is capable, market is forming, incumbents are slow to adapt, but you’re not so early that you’re the only buyer. This window is narrow and hard to identify in real-time.\nTiming luck is underrated in startup success. Many failures and successes differ less in strategy than in when they entered."
  },
  {
    "objectID": "posts/025_ai_startup_landscape/index.html#case-studies-winners-and-losers",
    "href": "posts/025_ai_startup_landscape/index.html#case-studies-winners-and-losers",
    "title": "The AI Startup Stack: Opportunities, Commoditization Risks, and Defensibility",
    "section": "Case Studies: Winners and Losers",
    "text": "Case Studies: Winners and Losers\nJasper AI: Early mover in AI copywriting. Grew fast but faced commoditization as ChatGPT ate their use case. Has pivoted towards enterprise marketing workflows. Verdict: still fighting, outcome uncertain.\nFigma AI features: Incumbent adding AI to existing product. Figma has the workflow, the users, and the data. AI is a feature, not a separate product. Verdict: likely durable.\nHarvey AI: AI for legal work. Deep vertical integration, domain expertise, relationships with major firms. Building specific models on proprietary legal data. Verdict: defensible position.\nNotion AI: AI added to existing productivity tool. Notion has the workflow; AI enhances it. Users won’t switch to “Notion + separate AI tool.” Verdict: durable for existing users.\nPattern: Startups that owned workflow first, then added AI, tend to fare better than startups that built “AI wrapper” products hoping to capture workflow later."
  },
  {
    "objectID": "posts/025_ai_startup_landscape/index.html#what-vcs-actually-look-for",
    "href": "posts/025_ai_startup_landscape/index.html#what-vcs-actually-look-for",
    "title": "The AI Startup Stack: Opportunities, Commoditization Risks, and Defensibility",
    "section": "What VCs Actually Look For",
    "text": "What VCs Actually Look For\nTalking to VCs reveals consistent patterns:\nTeam: Deep domain expertise or strong technical credentials. Founders who can articulate why they specifically will win.\nMarket: Large enough to matter ($1B+), with clear path to capture meaningful share. Not “AI for everything”—focused opportunity.\nTraction: Real usage, preferably with paying customers. In AI, demos are easy; production deployment is hard. Show the hard part.\nDifferentiation: What makes you not a thin wrapper? Data, workflow, technology, go-to-market, partnerships?\nUnit economics: Can you acquire customers profitably? What’s CAC and LTV? In early stage, these may be projections, but the logic should hold.\nOpenAI-proofing: VCs ask: “What happens when Sam ships this feature?” You need an answer. The best answer is that you’re in a vertical or have an asset OpenAI won’t build."
  },
  {
    "objectID": "posts/025_ai_startup_landscape/index.html#my-playbook-for-ai-startups",
    "href": "posts/025_ai_startup_landscape/index.html#my-playbook-for-ai-startups",
    "title": "The AI Startup Stack: Opportunities, Commoditization Risks, and Defensibility",
    "section": "My Playbook for AI Startups",
    "text": "My Playbook for AI Startups\nIf I were starting an AI company today:\n\nGo vertical: Pick a specific industry where I have expertise or can acquire it quickly.\nOwn the workflow: Build a product that structures how users do their job, with AI enhancing that workflow. Not “AI tool” but “better way to do X.”\nGenerate proprietary data: Design the product so that usage creates valuable data that improves the product. The flywheel is the moat.\nModel-agnostic when possible: Don’t depend on one provider. Abstract the AI layer so you can switch.\nCharge for value, not API calls: Price based on outcomes or seats, not tokens. Don’t let customers see the underlying commodity.\nMove fast: The window for capturing position is short. Speed beats perfection.\n\nNone of this is novel advice. The hard part is execution."
  },
  {
    "objectID": "posts/006_breakthrough/index.html",
    "href": "posts/006_breakthrough/index.html",
    "title": "Breakthrough #2 — Reinforcing: Dopamine, Basal Ganglia, and the Birth of RL",
    "section": "",
    "text": "Bennett’s second breakthrough—reinforcing—is where the story becomes recognizably “learning from reward.” Steering can produce competent behavior, but it cannot systematically improve itself when the environment changes. Reinforcing adds a mechanism that tunes action tendencies based on outcomes, and Bennett anchors this in vertebrate circuitry: basal ganglia plus dopamine.\nThe part that matters is not the anatomical labels; it’s the design pattern. You have an action-selection system that decides what gets executed, and you have a scalar teaching signal that nudges that system so it repeats actions with better long-run consequences.\nBennett emphasizes that the teaching signal is not simply “reward happened.” It’s closer to “that was better or worse than expected.” That small conceptual twist is what makes reinforcement scale to real life, where outcomes are delayed and ambiguous.\nIf your learning signal only arrives at the endpoint, you can’t assign credit to the earlier choices that made the endpoint possible. A prediction-based teaching signal solves this by moving learning earlier in time: cues become meaningful because they predict outcomes, and the learning signal lights up at those cues rather than only at the reward itself. When an expected reward fails to arrive, the teaching signal goes negative. This gives you a bidirectional correction mechanism that shapes behavior efficiently.\nThis is also why Bennett pushes back on the “dopamine = pleasure” caricature. Pleasure is an experience; learning is an algorithm. A teaching signal must carry information about mismatch between expectation and reality. When you interpret the dopamine story through that lens, a lot of behavioral phenomena stop being mysterious.\nAnticipation is what it looks like when predictors of reward become valuable. Disappointment is what it looks like when expectation is violated. Motivation becomes the behavioral manifestation of predicted value biasing action selection before the reward exists. Bennett is implicitly saying: many affective states are not add-ons; they are control-relevant summaries of prediction and uncertainty.\nA concept I think Bennett gets right is the separation between what counts as reward and how behavior is shaped by reward. Biological reward is anchored to survival: hunger, thirst, pain, thermoregulation, social safety. Reinforcement is the learning mechanism that figures out what to do in order to satisfy those needs in a particular environment.\nThat separation is important for two reasons. First, it explains flexibility: the organism can learn novel strategies without evolution rewriting its physiology every generation. Second, it highlights the danger of misalignment in any system with powerful optimization: if the reward signal is corrupted, the learning machinery will still optimize it, often in pathological ways. Bennett’s later discussion of superintelligence rhymes with this: the bigger the optimizer, the more central the objective becomes.\nReinforcement creates habits. Habits are a feature, not a bug: they’re fast, robust, and cheap. But Bennett treats habit learning as having an inherent ceiling. It’s slow to acquire when outcomes are rare. It becomes rigid in distribution shift. It struggles when you need to evaluate novel actions you’ve never tried.\nThat limitation sets up the next breakthrough. Once acting becomes expensive, evolution has an incentive to build a system that can learn without acting—by imagining. Reinforcing makes the agent trainable; simulating makes it sample efficient.\n\n\n\n  \n  Figure 1. Action gating trained by scalar feedback (actor–critic motif as an intuition)"
  }
]