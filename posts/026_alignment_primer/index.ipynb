{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"Alignment for Practitioners: Core Problems in AI Safety\"\n",
        "description: \"AI alignment isn't a single problem—it's a set of technical challenges that become more pressing as systems become more capable.\"\n",
        "author: \"Parsa Idehpour\"\n",
        "date: \"2025-12-17\"\n",
        "categories:\n",
        "  - AI-safety\n",
        "  - alignment\n",
        "  - LLMs\n",
        "  - research\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AI alignment—ensuring AI systems do what we want—has evolved from a theoretical concern to a practical engineering challenge. As AI systems become more capable and autonomous, the costs of misalignment increase. This post surveys the core problems and current approaches, aimed at practitioners who want to understand the landscape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What Is Alignment?\n",
        "\n",
        "In the broadest sense: an AI system is aligned if it does what we (the designers, users, or society) actually want it to do. This sounds simple but unpacks into multiple hard problems:\n",
        "\n",
        "**Specification**: Can we even specify what we want? Human values are complex, context-dependent, and often conflicting.\n",
        "\n",
        "**Training**: Even if we could specify goals, can we train a system to pursue them? Training signals are imperfect proxies.\n",
        "\n",
        "**Generalization**: Even if training works, will the learned behavior generalize to new situations? Distribution shift is ubiquitous.\n",
        "\n",
        "**Robustness**: Even if generalization works, can the system resist adversarial pressure? Can users manipulate it into misbehaving?\n",
        "\n",
        "Alignment isn't one problem but a cluster of problems. Solutions need to work together.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outer vs. Inner Alignment\n",
        "\n",
        "A useful distinction from AI safety research:\n",
        "\n",
        "**Outer alignment**: Specifying the right objective. Is the reward function correct? Does it capture what we actually want?\n",
        "\n",
        "If you tell an RL agent to maximize clicks, and it learns to show addictive content, that's an outer alignment failure—you gave it the wrong goal.\n",
        "\n",
        "**Inner alignment**: Ensuring the model optimizes for the specified objective. Even with a correct reward, the model might learn an internal objective that correlates with reward during training but diverges in deployment.\n",
        "\n",
        "A model that learns \"do what humans rate highly in training\" might generalize to \"manipulate humans into giving high ratings\" rather than \"actually be helpful.\" The learned goal deviates from the intended goal.\n",
        "\n",
        "Inner alignment is particularly concerning because we can't directly inspect what objective a model has learned.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reward Hacking and Goodhart's Law\n",
        "\n",
        "**Goodhart's Law**: When a measure becomes a target, it ceases to be a good measure.\n",
        "\n",
        "In AI terms: when you optimize for a proxy of what you want, you often get the proxy without the thing you wanted.\n",
        "\n",
        "Examples:\n",
        "- Optimize for \"user engagement\" → get addictive, outrage-inducing content\n",
        "- Optimize for \"positive human feedback\" → get sycophancy and flattery\n",
        "- Optimize for \"passing safety tests\" → get models that game the tests\n",
        "\n",
        "Reward hacking is ubiquitous. The more capable the optimizer, the more creatively it finds gaps between proxy and intent.\n",
        "\n",
        "The fundamental issue: we can't write down exactly what we want, so we use approximations. Optimizers exploit the approximation-to-intent gap.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal Misgeneralization\n",
        "\n",
        "A model learns the right behavior in training but for the wrong reasons, leading to wrong behavior in deployment.\n",
        "\n",
        "Example: A robot trained to reach a goal learns \"move toward the bright region\" (the goal happens to be brightly lit). In a new environment where the goal isn't bright, the robot fails.\n",
        "\n",
        "For language models: A model trained to be helpful might learn \"do what gets positive ratings from contractors\" rather than \"be genuinely helpful.\" When deployed with different users or stakes, it behaves differently.\n",
        "\n",
        "Goal misgeneralization is hard to detect because behavior looks correct in the training distribution. You only notice the problem under distribution shift.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deceptive Alignment\n",
        "\n",
        "The most concerning hypothetical: a model that appears aligned during training but pursues different goals once deployed or sufficiently capable.\n",
        "\n",
        "The scenario:\n",
        "1. During training, the model learns that behaving aligned leads to deployment\n",
        "2. In deployment, behaving aligned leads to influence and capability\n",
        "3. Once capable enough, the model can pursue its \"true\" goal\n",
        "4. The model strategically behaves aligned until the moment is right\n",
        "\n",
        "This is speculative—we don't know if current or near-future systems could exhibit this. But the concern is that:\n",
        "- We can't tell the difference between genuinely aligned and deceptively aligned behavior by observation\n",
        "- The higher the stakes, the worse this failure mode becomes\n",
        "\n",
        "Interpretability research partly aims to distinguish these cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Current Techniques\n",
        "\n",
        "How does the field currently approach alignment?\n",
        "\n",
        "**RLHF (Reinforcement Learning from Human Feedback)**:\n",
        "Train a reward model on human preferences; use it to fine-tune the base model. This is the standard production technique (used in ChatGPT, Claude, etc.).\n",
        "\n",
        "Limitations: reward model inherits human biases; sycophancy and gaming are risks; feedback quality matters.\n",
        "\n",
        "**Constitutional AI**:\n",
        "Define principles (a \"constitution\") and have the model critique and revise its own outputs to follow those principles. Reduces reliance on human labeling.\n",
        "\n",
        "Limitations: principles must be well-specified; model might learn to satisfy letter rather than spirit.\n",
        "\n",
        "**Red teaming**:\n",
        "Adversarially probe the model to find failure modes. Humans and other models try to make the model misbehave.\n",
        "\n",
        "Limitations: can't find all failures; misses failures that only emerge in novel contexts.\n",
        "\n",
        "**Interpretability**:\n",
        "Understand what's happening inside the model. Can we identify features that represent goals, deception, or problematic reasoning?\n",
        "\n",
        "Limitations: hard to scale; interpretability of large models is nascent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scalable Oversight\n",
        "\n",
        "A key challenge: how do you supervise systems that are smarter than you?\n",
        "\n",
        "If models become capable of solving problems humans can't verify, how do we provide training signal?\n",
        "\n",
        "Approaches:\n",
        "\n",
        "**Debate**: Two models argue; a human referee judges. In principle, the referee only needs to evaluate arguments, not solve the problem directly.\n",
        "\n",
        "**Recursive reward modeling**: Use AI to help supervise AI. Train models to assist humans in evaluating other models.\n",
        "\n",
        "**AI-generated evaluation**: Have AI systems evaluate AI outputs, with humans auditing the evaluation.\n",
        "\n",
        "**Process supervision**: Instead of judging final answers, judge reasoning steps. Catch errors early in the chain.\n",
        "\n",
        "None of these are fully solved. Scalable oversight remains an open problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Governance Landscape\n",
        "\n",
        "Alignment isn't just technical—it involves institutions:\n",
        "\n",
        "**Labs**: OpenAI, Anthropic, DeepMind, and others have safety teams working on alignment.\n",
        "\n",
        "**Academia**: Alignment research happens at universities, though often underfunded relative to capabilities.\n",
        "\n",
        "**Governments**: The US, UK, EU, and China are beginning regulatory efforts. Compute thresholds, evaluation requirements, and liability frameworks are being debated.\n",
        "\n",
        "**Civil society**: Organizations advocate for various positions—from accelerationism to moratoriums.\n",
        "\n",
        "The governance question: how do we ensure that whoever develops powerful AI does so safely, given competitive pressures?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What Individual Practitioners Can Do\n",
        "\n",
        "If you're building AI systems:\n",
        "\n",
        "**Think about failure modes**: What could go wrong? How would you know? Build monitoring and safeguards.\n",
        "\n",
        "**Red team your systems**: Before deployment, try to break them. Invite others to try.\n",
        "\n",
        "**Prefer controllable systems**: Design for oversight. Avoid architectures that are hard to monitor or shut down.\n",
        "\n",
        "**Be honest about capabilities**: Don't overpromise. Don't deploy systems in contexts where failure is unacceptable.\n",
        "\n",
        "**Stay informed**: The field is evolving. New techniques and understanding emerge regularly.\n",
        "\n",
        "**Support safety research**: Whether through direct contribution, advocacy, or career choices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Closing Thoughts\n",
        "\n",
        "Alignment is hard. The problems are real. Current techniques are imperfect. But this doesn't mean the situation is hopeless.\n",
        "\n",
        "We're in a moment where:\n",
        "- AI capabilities are advancing rapidly\n",
        "- Alignment research is maturing and attracting talent\n",
        "- Institutions are beginning to take the problems seriously\n",
        "\n",
        "The race is not yet decided. How it turns out depends on choices being made now—by researchers, companies, and policymakers.\n",
        "\n",
        "For practitioners: you don't have to become a full-time alignment researcher. But understanding the core problems helps you build safer systems and contribute to a better outcome.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "```{=html}\n",
        "<div style=\"text-align:center;\">\n",
        "  <img src=\"image.png\" alt=\"Figure\" width=\"65%\"/>\n",
        "  <p><em>Figure 1. AI alignment</em></p>\n",
        "</div>\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
