{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"CUDA Inference Kernels and Serving Stacks\"\n",
        "description: \"Key kernels, memory layouts, and serving choices (with vLLM notes)\"\n",
        "author: \"Parsa Idehpour\"\n",
        "date: \"2025-09-14\"\n",
        "categories:\n",
        "  - systems\n",
        "  - inference\n",
        "  - cuda\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "What makes LLM inference fast on GPUs? This post summarizes the core CUDA kernels, memory layouts, and scheduling tricks that dominate latency and throughput.\n",
        "\n",
        "- Attention with KV cache (paged KV, continuous batch, block-sparse)\n",
        "- GEMM-heavy layers (QKV projections, MLP) and tensor cores\n",
        "- Quantization (weight-only, KV cache) and dequant overhead\n",
        "- Serving stacks (vLLM, TensorRT-LLM, Triton) and scheduling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "```{=html}\n",
        "<div style=\"text-align:center;\">\n",
        "  <img src=\"vllm-logo-text-light.png\" alt=\"vLLM\" width=\"35%\"/>\n",
        "  <p><em>Figure 1. vLLM logo </em></p>\n",
        "</div>\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel hotspots\n",
        "\n",
        "- Attention: QK^T, softmax, and AV; fused kernels reduce memory traffic.\n",
        "- KV cache: paging and swizzling to keep contiguous access; block tables.\n",
        "- GEMM: QKV, output projection, MLP GEMMs dominate FLOPs; use tensor cores.\n",
        "- Quant: weight-only (W8A16) vs. activation quant; KV cache quant (e.g., 8-bit) saves VRAM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple throughput estimator\n",
        "from typing import Optional\n",
        "\n",
        "def estimate_throughput(tokens_per_request: int,\n",
        "                        requests_per_second: float,\n",
        "                        decode_tokens_per_s: float,\n",
        "                        prefill_tokens_per_s: Optional[float] = None,\n",
        "                        prefill_tokens: int = 0) -> float:\n",
        "    \"\"\"\n",
        "    Crude throughput estimate in tokens/sec.\n",
        "    - tokens_per_request: average generated tokens per request\n",
        "    - requests_per_second: steady-state request rate\n",
        "    - decode_tokens_per_s: steady-state decode speed (per token) across batch\n",
        "    - prefill_tokens_per_s: optional prefill speed; if None, ignored\n",
        "    - prefill_tokens: avg prompt tokens per request\n",
        "    \"\"\"\n",
        "    decode_toks = tokens_per_request * requests_per_second\n",
        "    total = decode_toks\n",
        "    if prefill_tokens_per_s is not None and prefill_tokens > 0:\n",
        "        total += (prefill_tokens * requests_per_second) * (prefill_tokens_per_s / decode_tokens_per_s)\n",
        "    return total\n",
        "\n",
        "print(estimate_throughput(tokens_per_request=200,\n",
        "                          requests_per_second=5.0,\n",
        "                          decode_tokens_per_s=200000.0,\n",
        "                          prefill_tokens_per_s=1200000.0,\n",
        "                          prefill_tokens=1000))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
