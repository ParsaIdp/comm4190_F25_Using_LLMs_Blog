{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"The GPU Economy: Understanding AI Infrastructure as a Strategic Asset\"\n",
        "description: \"AI capability is gated by compute, and compute supply chains are geopolitically constrained. Understanding infrastructure economics is essential.\"\n",
        "author: \"Parsa Idehpour\"\n",
        "date: \"2025-12-17\"\n",
        "categories:\n",
        "  - economics\n",
        "  - infrastructure\n",
        "  - compute\n",
        "  - geopolitics\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Behind every AI capability is a stack of silicon. GPUs, TPUs, and custom AI accelerators are the physical substrate that makes training and inference possible. Understanding the economics of this infrastructure\u2014who builds it, who controls supply, what it costs\u2014is essential for understanding AI's trajectory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Chip Supply Chain\n",
        "\n",
        "The production of AI chips involves one of the most complex supply chains humans have ever built:\n",
        "\n",
        "**Design**: Companies like NVIDIA, AMD, and Google design chip architectures without owning fabrication facilities (\"fabless\"). NVIDIA designs; someone else manufactures. This requires massive R&D investment and years of development per generation.\n",
        "\n",
        "**Fabrication**: Taiwan Semiconductor Manufacturing Company (TSMC) produces ~90% of the world's most advanced chips. Samsung is a distant second. Intel is trying to catch up. This concentration creates extreme supply chain risk.\n",
        "\n",
        "**Equipment**: The machines that make chips are even more concentrated. ASML (Netherlands) is the sole supplier of EUV lithography machines\u2014the tools that print features at the 7nm scale and below. Without ASML, no one makes cutting-edge chips.\n",
        "\n",
        "**Packaging and assembly**: Advanced packaging (stacking dies, chiplet architectures) happens at TSMC and specialized facilities. This is increasingly the integration point for AI systems.\n",
        "\n",
        "**End systems**: Cloud providers (AWS, Google Cloud, Azure) and AI labs (OpenAI, Anthropic, DeepMind) assemble chips into datacenters and provide services.\n",
        "\n",
        "The result: a small number of chokepoints control global AI compute capacity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Scarcity\n",
        "\n",
        "In 2023-2024, NVIDIA's H100 GPUs became famously difficult to obtain. Waiting lists stretched months. Prices spiked. Why?\n",
        "\n",
        "**Demand surge**: ChatGPT demonstrated that LLMs were ready for prime time. Everyone wanted to train their own. Demand for training compute spiked.\n",
        "\n",
        "**Supply constraints**: Fabrication capacity is limited. TSMC can only produce so many wafers. NVIDIA prioritized its most profitable customers.\n",
        "\n",
        "**Hoarding**: Companies bought more than they immediately needed to secure future supply. This amplified scarcity.\n",
        "\n",
        "**Lead times**: Building fab capacity takes 3-5 years. Supply can't respond quickly to demand spikes.\n",
        "\n",
        "The scarcity was real, not artificial. NVIDIA's market cap reflects genuine pricing power when supply is constrained.\n",
        "\n",
        "Implications for AI development:\n",
        "- **Rich get richer**: Well-funded labs train larger models. Compute-poor researchers fall behind.\n",
        "- **Cloud concentration**: Access to compute flows through a few large cloud providers.\n",
        "- **Innovation bottlenecks**: Ideas that require large-scale experiments wait in line for resources.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Cost Scaling\n",
        "\n",
        "How much does it cost to train a frontier model?\n",
        "\n",
        "**Rough estimates**:\n",
        "- GPT-4-class models: $50-100 million in compute\n",
        "- Gemini Ultra: likely similar or higher\n",
        "- Claude 3.5: undisclosed but comparable\n",
        "\n",
        "These costs are dominated by GPU-hours. A training run might use 10,000+ H100s for months.\n",
        "\n",
        "**Chinchilla scaling laws**: For a given compute budget, there's an optimal split between model size and training tokens. Earlier we overinvested in parameters; now we train smaller models longer. This affects the compute profile but not the total cost.\n",
        "\n",
        "**FLOP budgets**: Training compute is often measured in floating-point operations. GPT-3 was ~3.6 \u00d7 10\u00b2\u00b3 FLOPs. GPT-4 is estimated at 10\u00b2\u2075 FLOPs or higher. Each generation is 10-100\u00d7 more expensive.\n",
        "\n",
        "At some point, training costs become prohibitive for most organizations. Only well-funded labs can afford to train frontier models from scratch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference Economics\n",
        "\n",
        "Training happens once; inference happens every time someone uses the model. As models deploy to millions of users, inference costs dominate.\n",
        "\n",
        "**Cost per token**: Depends on model size, hardware, and optimization. GPT-4 costs ~$0.03/1K input tokens, $0.06/1K output tokens (API pricing, which includes margin). Open-source models on self-hosted hardware can be 10-100\u00d7 cheaper.\n",
        "\n",
        "**Throughput vs. latency**: Batching multiple requests together is efficient (high throughput) but adds latency. Real-time chat needs low latency and sacrifices efficiency.\n",
        "\n",
        "**Quantization and optimization**: Reducing precision (FP16 \u2192 INT8 \u2192 INT4) shrinks model size and speeds inference. Distillation creates smaller, faster models. These techniques trade some quality for major efficiency gains.\n",
        "\n",
        "**Inference chips**: NVIDIA GPUs are great at training but not necessarily optimal for inference. Specialized inference chips (NVIDIA T4, custom ASICs, AWS Inferentia) optimize for cost-per-query.\n",
        "\n",
        "The inference cost structure matters for business models. Products that use expensive models on long contexts need to charge accordingly or find efficiency gains.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cloud vs. On-Prem\n",
        "\n",
        "Where does AI compute run?\n",
        "\n",
        "**Hyperscalers (AWS, Azure, GCP)**: Most training and inference runs on cloud infrastructure. Benefits: scale on demand, no capex, managed services. Costs: higher unit prices, vendor lock-in, data egress fees.\n",
        "\n",
        "**Specialized AI clouds (Lambda Labs, CoreWeave, Together)**: Focus on GPU availability and AI-specific services. Often more competitive pricing for pure compute.\n",
        "\n",
        "**On-prem and self-hosted**: Large enterprises and research institutions increasingly build their own GPU clusters. Benefits: control, customization, data sovereignty. Costs: capex, operations, utilization risk.\n",
        "\n",
        "**Edge deployment**: Running small models on devices (phones, laptops, robots). Latency-sensitive or offline applications. Severely constrained by device thermal and power limits.\n",
        "\n",
        "The trend is bifurcation: massive training runs on hyperscale clouds; inference increasingly distributed to specialized providers and edge devices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Energy Constraints\n",
        "\n",
        "AI datacenters use enormous amounts of power, and the problem is getting worse:\n",
        "\n",
        "**Power density**: A rack of H100 GPUs uses 10-20kW. A large AI cluster might use hundreds of megawatts\u2014the output of a small power plant.\n",
        "\n",
        "**Cooling**: Most datacenter energy goes to cooling, not compute. Liquid cooling is increasingly necessary for high-density GPU racks.\n",
        "\n",
        "**Location matters**: Cheap power and cool climates attract AI infrastructure. Nordic countries, the Pacific Northwest, and locations near hydropower are popular.\n",
        "\n",
        "**Carbon implications**: Unless powered by renewables, AI training has significant carbon footprint. Large training runs can emit as much CO2 as hundreds of flights.\n",
        "\n",
        "Power availability is becoming a constraint on datacenter construction. In some regions, there simply isn't enough grid capacity for proposed AI facilities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Geopolitics\n",
        "\n",
        "The concentration of chip supply chains creates geopolitical vulnerabilities:\n",
        "\n",
        "**Taiwan risk**: TSMC's fabrication facilities are in Taiwan. A Chinese invasion or blockade would devastate global chip supply. The US, Japan, and Europe are investing heavily to diversify, but new fabs take years.\n",
        "\n",
        "**Export controls**: The US restricts export of advanced chips and chip-making equipment to China. This limits China's ability to train frontier models and slows their AI development\u2014at least for now.\n",
        "\n",
        "**CHIPS Act**: The US is investing $52B to rebuild domestic semiconductor manufacturing. Intel, TSMC, and Samsung are building fabs on US soil. But these take 3-5 years to complete.\n",
        "\n",
        "**China's response**: China is investing heavily in domestic alternatives. Huawei's Ascend chips, SMIC's fabrication capabilities. They're behind but working to close the gap.\n",
        "\n",
        "The AI compute supply chain is one of the most strategically significant industrial bases in the world. Governments treat it accordingly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Future Compute\n",
        "\n",
        "Several trends may change the compute landscape:\n",
        "\n",
        "**Algorithmic efficiency**: Moore's Law has slowed, but algorithmic improvements continue. Better architectures, training methods, and representations can achieve the same capability with less compute. This is \"software eating hardware.\"\n",
        "\n",
        "**Sparsity**: Most neural network weights are unnecessary for any given input. Sparse models activate only relevant subsets, dramatically reducing compute. Mixture-of-experts is one form of this.\n",
        "\n",
        "**Alternative hardware**: Photonic computing, neuromorphic chips, and analog computing offer potential for more efficient inference. Still nascent but under active development.\n",
        "\n",
        "**Quantum computing**: Theoretical advantages for some problems, but no near-term impact on the main workloads (transformers, diffusion). Quantum machine learning is mostly hype so far.\n",
        "\n",
        "The most likely scenario: continued GPU/TPU improvement + algorithmic efficiency gains, with specialized accelerators for specific applications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implications\n",
        "\n",
        "Understanding AI infrastructure economics matters for:\n",
        "\n",
        "**Researchers**: Access to compute shapes what's possible to study. Know the economics of your cloud provider.\n",
        "\n",
        "**Startups**: Compute costs are a major expense. Efficiency is a competitive advantage. Don't assume you can just throw money at the problem.\n",
        "\n",
        "**Investors**: The \"picks and shovels\" of AI (NVIDIA, TSMC, datacenter REITs) have been fantastic investments. The question is whether this persists.\n",
        "\n",
        "**Policymakers**: Compute supply chains are strategic. Export controls, subsidies, and supply chain resilience are legitimate policy concerns.\n",
        "\n",
        "**Everyone**: The physical infrastructure underlying AI determines who can build powerful systems. It's not just about software.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}