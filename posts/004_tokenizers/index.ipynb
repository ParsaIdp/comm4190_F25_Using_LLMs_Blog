{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"Tokenization Exploration\"\n",
        "description: \"I explored tokenization with LLMs and evaluated prompting effectiveness\"\n",
        "author: \"Parsa Idehpour\"\n",
        "date: \"2025-11-03\"\n",
        "categories:\n",
        "  - prompting\n",
        "  - evaluation\n",
        "  - LLMs\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Context and assignment\n",
        "\n",
        "I explored tokenization with LLMs (GPT‑5 and Sonnet 4.5) and provide feedback on a peer’s prompting strategy. I analyze effectiveness, cross‑model checks, improvements, and why outcomes were expected/unexpected based on class concepts.\n",
        "\n",
        "- Evidence base: two chats exploring Discrete Fourier Transform (DFT), tokenizer design, and \"better than BPE\" ideas.\n",
        "- Deliverable: A direct URL to my feedback on the issues page (added below when posted).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What the interactions showed (summary)\n",
        "\n",
        "- DFT knowledge: The model stated standard DFT/IDFT formulas, properties, FFT relation, and practical notes — expected for a strong general model.\n",
        "- Tokenizer idea critique: It argued a plain DFT isn’t a practical tokenizer for text (losslessness, semantics, variable length), but suggested Fourier inside models and feature augmentations — aligned with class emphasis on data‑model match.\n",
        "- Novel tokenizer directions: Proposed learned/VQ tokenizers, morphology‑aware hybrids, adaptive tokenizers, information‑theoretic/MDL objectives, and graph/semantic clustering — consistent with \"optimize for predictability, not just compression\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Did they prompt different models and check multiple outputs?\n",
        "\n",
        "- Yes — two separate model families were engaged (GPT‑5 and Sonnet 4.5). This is good practice for triangulation.\n",
        "- Multiple outputs/checks: The threads iterate across related questions (DFT → tokenizer feasibility → alternatives), eliciting deeper reasoning rather than one‑shot answers. This aligns with our class guidance to sample more than once and compare.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
