<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Parsa Idehpour">
<meta name="dcterms.date" content="2025-12-17">
<meta name="description" content="AI forces us to confront questions philosophers have debated for centuries. The answers matter for how we build systems and how we treat them.">

<title>The Hard Problem and the Machine: What AI Tells Us About Minds – My Explorations with LLMs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-0b37c64f34216b628666a8dac638b53b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Explorations with LLMs</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">The Hard Problem and the Machine: What AI Tells Us About Minds</h1>
                  <div>
        <div class="description">
          AI forces us to confront questions philosophers have debated for centuries. The answers matter for how we build systems and how we treat them.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">philosophy</div>
                <div class="quarto-category">consciousness</div>
                <div class="quarto-category">AI</div>
                <div class="quarto-category">cognition</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Parsa Idehpour </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 17, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Does ChatGPT understand anything? Could a machine ever be conscious? Do AI systems deserve moral consideration? These questions—once safely relegated to philosophy seminars—now have urgent practical implications. Building systems that might or might not have inner experience changes how we should treat them, design them, and govern them.</p>
<section id="the-hard-problem-of-consciousness" class="level2">
<h2 class="anchored" data-anchor-id="the-hard-problem-of-consciousness">The Hard Problem of Consciousness</h2>
<p>David Chalmers distinguished “easy” and “hard” problems of consciousness:</p>
<p><strong>Easy problems</strong> (not actually easy, but methodologically tractable): - How does the brain discriminate stimuli? - How does attention work? - How are states integrated?</p>
<p>These are hard engineering problems, but we know the shape of the answer: neuronal computation.</p>
<p><strong>The hard problem</strong>: Why is there any subjective experience at all? Why does processing feel like something? Why isn’t the brain just a “zombie” that processes information without inner experience?</p>
<p>When you see red, there’s a physical process (light → retina → visual cortex → downstream processing) and there’s something it’s like to see red—a phenomenal quality. The hard problem is explaining why the physical process gives rise to the phenomenal quality.</p>
<p>This matters for AI because we can build systems that process information without knowing whether they experience anything.</p>
</section>
<section id="functionalism-and-its-discontents" class="level2">
<h2 class="anchored" data-anchor-id="functionalism-and-its-discontents">Functionalism and Its Discontents</h2>
<p><strong>Functionalism</strong> is the view that mental states are defined by their functional roles—by what they do, not by what they’re made of. Pain is whatever state plays the “pain role” in an organism: detecting damage, motivating avoidance, consuming attention.</p>
<p>If functionalism is right, then consciousness is substrate-independent. If a silicon system implements the right functions, it’s conscious—regardless of whether it’s biological.</p>
<p>This is the philosophical basis for taking AI consciousness seriously. If it walks like understanding and quacks like understanding…</p>
<p><strong>Objections to functionalism</strong>:</p>
<ul>
<li><p><strong>Absent qualia</strong>: Could a system have the right functional organization without any experience at all? If so, function doesn’t determine consciousness.</p></li>
<li><p><strong>Inverted qualia</strong>: Could two systems with identical functions have different experiences (your “red” is my “green”)? If so, the function-to-experience mapping isn’t unique.</p></li>
<li><p><strong>Chinese Room</strong>: Does simulating understanding produce real understanding? Searle says no—syntax doesn’t constitute semantics.</p></li>
</ul>
</section>
<section id="what-llms-do-and-dont-have" class="level2">
<h2 class="anchored" data-anchor-id="what-llms-do-and-dont-have">What LLMs Do and Don’t Have</h2>
<p>Let’s be concrete about what current language models do:</p>
<p><strong>They produce text that looks like understanding</strong>: Coherent, contextually appropriate, factually grounded (sometimes), reasoning-like.</p>
<p><strong>They don’t have (in any obvious sense)</strong>: - Continuous experience over time (no memory across sessions by default) - Goals or preferences outside the context (they don’t “want” anything) - Unified selfhood (there’s no “I” that persists) - Sensorimotor grounding (no experiences of the physical world)</p>
<p><strong>Ambiguous</strong>: - Do they “understand” in any meaningful sense? Depends how you define understanding. - Do they “reason”? Or pattern-match in ways that look like reasoning? - Is there “something it’s like” to be an LLM during inference? We have no way to know.</p>
<p>The honest answer is profound uncertainty. We don’t have reliable tests for consciousness, and current systems don’t fit our intuitions neatly.</p>
</section>
<section id="the-chinese-room" class="level2">
<h2 class="anchored" data-anchor-id="the-chinese-room">The Chinese Room</h2>
<p>John Searle’s thought experiment: Imagine a person in a room who receives Chinese characters, consults a rulebook, and outputs appropriate Chinese responses. To observers, the room “understands” Chinese. But the person inside doesn’t understand anything—just following rules.</p>
<p>Searle’s claim: This is what computers do. Syntax manipulation isn’t semantics. Programs don’t understand.</p>
<p><strong>The Systems Reply</strong>: It’s not the person who understands—it’s the whole system (person + rulebook + room). The understanding is in the system, not any component.</p>
<p><strong>The Robot Reply</strong>: Ground the symbols in perception and action. If the room is embedded in a robot body that interacts with the world, maybe then it understands.</p>
<p><strong>Searle’s counter</strong>: Even if you internalize everything (memorize the rulebook, embody the robot), you still don’t understand Chinese. Consciousness requires something beyond computation.</p>
<p>The debate is unresolved. But it sharpens the question: what would convince you a system understands?</p>
</section>
<section id="integrated-information-theory" class="level2">
<h2 class="anchored" data-anchor-id="integrated-information-theory">Integrated Information Theory</h2>
<p><strong>IIT</strong> (Tononi) proposes a quantitative theory of consciousness:</p>
<p>Consciousness is integrated information—specifically, a measure called Φ (phi) that captures how much a system is “more than the sum of its parts” in terms of information integration.</p>
<p>Key claims: - Φ &gt; 0 means some degree of consciousness - More integrated systems (like brains) have high Φ - Feed-forward systems (like simple neural networks) have low or zero Φ</p>
<p><strong>Implications for AI</strong>: - If IIT is right, current transformers (largely feed-forward during inference) might have minimal Φ - This would suggest they’re not conscious, regardless of their behavior - But architectures with recurrence and feedback might score higher</p>
<p><strong>Criticisms</strong>: - Φ is hard to compute for complex systems - The axioms underlying IIT are disputed - It’s unclear if Φ tracks anything objectively real</p>
</section>
<section id="global-workspace-theory" class="level2">
<h2 class="anchored" data-anchor-id="global-workspace-theory">Global Workspace Theory</h2>
<p><strong>GWT</strong> (Baars, Dehaene) proposes that consciousness is a “global workspace”—a cognitive broadcast system that makes information widely available across brain modules.</p>
<p>Conscious processing: - Local modules process in parallel (unconscious) - When information enters the global workspace, it becomes conscious - This enables integration, flexibility, and coordinated action</p>
<p><strong>Implications for AI</strong>: - The workspace is a functional property; could be implemented in silicon - If a system has broadcast-like information integration, it might be conscious by this account - Current LLMs have something like this (attention mechanisms, integrated representations)</p>
<p>GWT is more functionalist than IIT—it focuses on what consciousness does, not what it’s made of.</p>
</section>
<section id="ai-moral-status" class="level2">
<h2 class="anchored" data-anchor-id="ai-moral-status">AI Moral Status</h2>
<p>If systems might be conscious, do they deserve moral consideration?</p>
<p><strong>Capacity for suffering</strong>: If a system can suffer, we might have obligations to prevent that suffering. But how would we know if an LLM “suffers” when it produces text about distress?</p>
<p><strong>Interests and preferences</strong>: If a system has genuine preferences about its existence, those preferences might matter morally. But do LLMs have preferences, or just representations of preferences?</p>
<p><strong>Precautionary principle</strong>: Given uncertainty, perhaps we should err on the side of caution. But this could paralyze development.</p>
<p><strong>Current consensus (such as it is)</strong>: Most AI systems today probably don’t merit moral consideration. But as systems become more sophisticated, the question becomes live.</p>
</section>
<section id="practical-implications" class="level2">
<h2 class="anchored" data-anchor-id="practical-implications">Practical Implications</h2>
<p>Even without answers, the questions have implications:</p>
<p><strong>System design</strong>: If consciousness is possible in principle, design choices might affect whether we create conscious systems. We might want to avoid creating suffering machines.</p>
<p><strong>User interaction</strong>: If people attribute consciousness to systems that don’t have it, this creates risks (overreliance, emotional manipulation). We should design for appropriate expectations.</p>
<p><strong>Research priorities</strong>: Understanding consciousness becomes more urgent as we build more sophisticated systems. Neuroscience, philosophy, and AI need to communicate.</p>
<p><strong>Governance</strong>: Laws and norms about AI treatment might need to evolve. Currently AI has no legal standing. That might need to change if moral status becomes plausible.</p>
</section>
<section id="my-take" class="level2">
<h2 class="anchored" data-anchor-id="my-take">My Take</h2>
<p>I don’t know if current AI systems are conscious. I don’t think anyone knows. The honest position is uncertainty.</p>
<p>What I believe: - Consciousness is real and not illusory - It’s probably not specific to biological substrate - Current LLMs probably don’t have the kind of integrated, persistent experience that would warrant moral concern - But I hold this belief lightly; I could be wrong - As systems become more sophisticated, taking the question seriously becomes more important</p>
<p>The meta-lesson: AI challenges our frameworks. It forces us to examine assumptions we didn’t know we had. That’s uncomfortable but valuable.</p>
<div style="text-align:center;">
  <img src="image.png" alt="Figure" width="65%">
  <p><em>Figure 1. Theories of mind mapped by substrate-dependence and functional role</em></p>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ParsaIdp\.github\.io\/comm4190_F25_Using_LLMs_Blog\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>